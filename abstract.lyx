#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
In classification tasks of biological data, there are usually fewer labeled
 than unlabeled samples because labeling samples is costly or time-consuming.
 In addition, labeled data sets can be re-used in different contexts as
 additional unlabeled data sets.
 For example, when searching the Gene Expression Omnibus (GEO) repository
 for microarray data sets of drug sensitivity and resistance experiments,
 the largest one has 2,522 samples, but the median has only 12 samples.
\end_layout

\begin_layout Standard
In machine learning in general, utilizing unlabeled data in classification
 tasks is called semi-supervised learning.
 Artificial neural networks can be used to pre-train on unlabeled data before
 fine-tuning via back-propagation with labeled data.
 Such artificial neural networks enabling deep learning have gained attention
 since around 2010, since when they have been among the best-performing
 algorithms in visual object recognition.
\end_layout

\begin_layout Standard
We measured accuracies in the task of classifying tissue taken from breast
 cancer patients at reductive surgery as chemotherapy-resistant or -sensitive.
 Different data sets were constructed by subsampling from GEO data set GSE25055
 and GSE25065.
 Using these data sets, we compared classification accuracy of the neural
 networks autoencoder, Restricted Boltzmann Machine, Deep Belief Network
 (DBN) and support vector machine (SVM), and Transductive SVM (TSVM).
 Training was done both in supervised and semi-supervised mode.
 For the neural networks, we tried several different network architectures.
 
\end_layout

\begin_layout Standard
Smoothing the validation set accuracies obtained during training iterations
 to alleviate low sample numbers helped in model selection of the best classifie
r.
 We also investigated the effect of different normalization procedures on
 the classification accuracy.
 The data were normalized with either RMA or MAS5, followed by either no
 batch-effect correction or Combat batch-effect correction.
 Only MAS5 profited from added Combat batch-effect correction, but normalization
 with RMA alone yielded the best classification accuracy.
\end_layout

\begin_layout Standard
We were particularly interested whether classification accuracies improve
 when adding unlabeled samples in semi-supervised learning.
 Overall, neural networks and support vector machines performed similar.
 We found a slight improvement of classification accuracy when the number
 of unlabeled samples presented to DBN and TSVM was increased to the maximal
 number of samples in our data sets.
 However, this effect was only observed when the learning algorithms were
 presented the expression values of all 22,283 genes, not just the 500 most
 variable genes.
\end_layout

\end_body
\end_document

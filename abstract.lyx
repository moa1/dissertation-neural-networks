#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
In classification tasks of biological data, there are usually fewer labeled
 than unlabeled samples because labeling samples is costly or time-consuming.
 In addition, labeled data sets can be re-used in different contexts as
 additional unlabeled data sets.
 For example, when searching the Gene Expression Omnibus (GEO) repository
 for microarray data of drug sensitivity and resistance experiments, the
 largest one has 2,522 samples, but the median has only 12 samples.
\end_layout

\begin_layout Standard
In machine learning in general, utilizing unlabeled data in classification
 tasks is called semi-supervised learning.
 Artificial neural networks can be used to pre-train on unlabeled data before
 fine-tuning via back-propagation with labeled data.
 Such artificial neural networks enabling deep learning have gained attention
 since around 2010, since when they have been among the best-performing
 algorithms in visual object recognition.
\end_layout

\begin_layout Standard
Here we compared the three aforementioned artificial neural networks and
 support vector machines, both in supervised and semi-supervised mode.
 We measured accuracies in the task of classifying tissue taken from breast
 cancer patients at reductive surgery as chemotherapy-resistant or -sensitive.
\end_layout

\begin_layout Standard
Different data sets were constructed by subsampling from GEO data set GSE25055
 and GSE25065.
 Using these data sets, we compared classification accuracy of the relatively
 new neural networks Autoencoder, Restricted Boltzmann Machine, Deep Belief
 Network (DBN) and the established support vector machine (SVM), and Transductiv
e SVM (TSVM).
 
\end_layout

\begin_layout Standard
Transcriptomic data and visual images are both similar in that they have
 highly correlated dimensions, and dissimilar because the number of available
 unlabeled samples is much higher for images than expression data.
 We found that smoothing the validation set accuracies obtained during training
 iterations helped in model selection of the best classifier.
\end_layout

\begin_layout Standard
We also investigated the effect of different normalization procedures on
 the classification accuracy.
 Before classifying, the data were normalized with either RMA or MAS5, followed
 by either no batch-effect correction or Combat batch-effect correction.
 Only MAS5 profited from added Combat batch-effect correction, but normalization
 with RMA alone yielded the best classification accuracy.
\end_layout

\begin_layout Standard
In particular, we were interested whether classification accuracies improve
 when adding unlabeled samples in semi-supervised learning.
 Overall, neural networks and support vector machines performed similar.
 We found a slight improvement of classification accuracy when the number
 of unlabeled samples presented to DBN and TSVM was increased to the maximal
 number of samples in our data sets.
 However, this effect was only observed when the learning algorithms were
 presented the expression values of all genes, not just a subset of the
 500 most variable genes.
\end_layout

\end_body
\end_document

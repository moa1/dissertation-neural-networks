#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
% Added by lyx2lyx
\end_preamble
\use_default_options true
\begin_modules
fix-cm
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 2
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Ich glaube ich sollte die model-beschreibungen (directed/undirected
 graphical, RBM, DBN) immer gleich strukturieren: 1.
 structure (i.e.
 die nodes und weights erklären) 2.
 How to generate data from the model 3.
 How to train a model so that it fits data.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: check whether to use 
\begin_inset Quotes eld
\end_inset

node
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

neuron
\begin_inset Quotes erd
\end_inset

 in all current usages.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here we will introduce the methods used to train deep neural networks, as
 applied in the results part.
\end_layout

\begin_layout Section
Notation
\end_layout

\begin_layout Description
Random
\begin_inset space ~
\end_inset

variable,
\begin_inset space ~
\end_inset

Node Random variables and nodes are written upper-case.
 For example: 
\begin_inset Formula $X$
\end_inset

 or 
\begin_inset Formula $N_{4}$
\end_inset

.
\end_layout

\begin_layout Description
Value,
\begin_inset space ~
\end_inset

Scalar
\begin_inset space ~
\end_inset

Variable The value of a random variable and a scalar variable are written
 lower-case.
 For example, the value of random variable 
\begin_inset Formula $X$
\end_inset

 is written 
\begin_inset Formula $x$
\end_inset

, and 
\begin_inset Formula $i$
\end_inset

 is a scalar.
\end_layout

\begin_layout Description
Vector,
\begin_inset space ~
\end_inset

Set Vectors or sets are written in bold font.
 For example, the vector 
\begin_inset Formula $\mathbf{X}$
\end_inset

 represents e.g.
 the random variables 
\begin_inset Formula $\{X_{1},X_{2},X_{3}\}$
\end_inset

.
 And the vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 stands for e.g.
 the value 
\begin_inset Formula $\{x_{1},x_{2},x_{3}\}$
\end_inset

 of the variable 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
\end_layout

\begin_layout Section
Machine Learning
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe TODO (but I already have a short section on these in the introduction):
 Definition of supervised, unsupervised, and semi-supervised learning.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Generative and Discriminative Models
\end_layout

\begin_layout Standard
An often-cited quote by 
\begin_inset CommandInset citation
LatexCommand cite
key "Vapnik1998"

\end_inset

 is: 
\begin_inset Quotes eld
\end_inset

If you possess a restricted amount of information for solving some problem,
 try to solve the problem directly and never solve a more general problem
 as an intermediate step.
 It is possible that the available information is sufficient for a direct
 solution but is insufficient for solving a more general intermediate problem.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
A generative model
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
generative model
\end_layout

\end_inset

 is such a more general problem: its aim is to model the input data set
 such that hypothetical samples can be generated from the model which might
 as well be found in the original input data set.
 A discriminative model
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
discriminative model
\end_layout

\end_inset

 on the other hand receives the input samples and models the output from
 these inputs.
 Usually the outputs have lower dimension.
\end_layout

\begin_layout Standard
Restricted Boltzmann Machines and Deep Belief Networks are both generative
 models, while a neural network supervisedly trained with back-propagation
 is a discriminative model.
 In a discriminative model, the parameters (weights and biases) specify
 the class label of a training sample.
 In a generative model, the parameters need to encode the whole sample.
 The number of bits required to specify the class label is much smaller
 than the number of bits required to specify a whole training sample 
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
guideTR.pdf: 
\begin_inset Quotes eld
\end_inset

When learning generative models of high-dimensional data, however, it is
 the number of bits that it takes to specify a data vector that determines
 how much constraint each training case imposes on the parameters of the
 model.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Another advantage of a generative model is that one can draw samples from
 its distribution (
\begin_inset Quotes eld
\end_inset

generate samples
\begin_inset Quotes erd
\end_inset

) to easily find out what the model has learned.
 In a discriminative model, this can be substantially harder.
 Consider for example a classifier network trained with back-propagation
 that decides whether an image shows a red ball (output: true or false).
 The decision function of the network is a complicated function of all input
 pixels.
 Therefore it can be hard to determine the property an unseen image must
 have so that the network would classify it as containing a red ball.
 Due to their greater generality, generative models have the disadvantage
 that they are slower than discriminative models.
 As 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 note, however, the class of too computationally intensive models is being
 eroded by Moore's Law.
\end_layout

\begin_layout Standard
We will discuss the established Support Vector Machines, Graphical Models,
 several artificial neural networks, Restricted Boltzmann Machines, and
 Deep Belief Networks.
\end_layout

\begin_layout Subsection
Support Vector Machines
\end_layout

\begin_layout Standard
Here we review supervised and transductive Support Vector Machines
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Support Vector Machine
\end_layout

\end_inset

 (SVMs
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
SVM
\end_layout

\end_inset

 and TSVMs
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
TSVM
\end_layout

\end_inset

).
\end_layout

\begin_layout Subsubsection
Supervised Support Vector Machines
\end_layout

\begin_layout Standard
A linear SVM
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
supervised SVM
\end_layout

\end_inset

 separates data points into two distinct classes using the hyperplane
\begin_inset Formula 
\[
\mathbf{w}\cdot\mathbf{x}+b=0,
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{w}\in\mathbb{R}^{n}$
\end_inset

 is the vector perpendicular to the plane, 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

 is a point on the plane, and 
\series bold

\begin_inset Formula $b\in\mathbb{R}$
\end_inset


\series default
 is the distance to the origin (in units of length 
\begin_inset Formula $-\left\Vert \mathbf{w}\right\Vert $
\end_inset

) 
\begin_inset CommandInset citation
LatexCommand cite
key "StatnikovGuyon2011"

\end_inset

.
 The hyperplane is defined in the 
\begin_inset Formula $n$
\end_inset

-dimensional space in which the samples lie, each sample having 
\begin_inset Formula $n$
\end_inset

 features.
 For a specific hyperplane defined by 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 and a sample 
\begin_inset Formula $\mathbf{x}$
\end_inset

, one can calculate the distance 
\begin_inset Formula $d$
\end_inset

 between 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and the hyperplane using
\begin_inset Formula 
\[
d=\mathbf{w}\cdot\mathbf{x}+b.
\]

\end_inset

In particular, the sign of 
\begin_inset Formula $d\in\mathbb{R}$
\end_inset

 is called the class of 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Training a Hard-margin SVM
\end_layout

\begin_layout Standard
When training a SVM by providing binary class labels 
\begin_inset Formula $y_{i}\in\{-1,1\}$
\end_inset

 for all training samples 
\begin_inset Formula $\mathbf{x_{i}}$
\end_inset

, the hyperplane is constructed such that it separates the two classes and
 that it has the largest possible distance (
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
margin
\end_layout

\end_inset


\emph on
margin
\emph default
) to border-line samples (
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
support vectors
\end_layout

\end_inset


\emph on
support vectors
\emph default
).
 Learning a hard-margin SVM means finding a 
\begin_inset Formula $\mathbf{w}$
\end_inset

 so that its length is minimal
\begin_inset Formula 
\begin{equation}
\mbox{minimize }\frac{1}{2}\left\Vert \mathbf{w}\right\Vert ^{2}\label{eq:hard-margin-SVM-min-w}
\end{equation}

\end_inset

subject to the condition that all samples are classified correctly by 
\begin_inset Formula $\mathbf{w}$
\end_inset

, 
\begin_inset Formula $b$
\end_inset

.
 Hence, the constraints 
\begin_inset Formula 
\begin{equation}
y_{i}(\mathbf{w}\cdot\mathbf{x_{i}}+b)-1\geq0,\label{eq:hard-margin-SVM-constraints}
\end{equation}

\end_inset

(where 
\begin_inset Formula $y_{i}$
\end_inset

 is the true class of sample 
\begin_inset Formula $i$
\end_inset

) must be fulfilled for all samples 
\begin_inset Formula $i$
\end_inset

.
 The two equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hard-margin-SVM-min-w"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hard-margin-SVM-constraints"

\end_inset

 are called the 
\begin_inset Quotes eld
\end_inset

primal formulation of linear SVMs
\begin_inset Quotes erd
\end_inset

.
 This formulation can be solved by 
\emph on
convex quadratic programming
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
quadratic programming
\end_layout

\end_inset

 with 
\begin_inset Formula $n$
\end_inset

 variables, where 
\begin_inset Formula $n$
\end_inset

 is the number of features.
 Using Lagrange multipliers 
\begin_inset CommandInset citation
LatexCommand cite
key "StatnikovGuyon2011"

\end_inset

, the primal formulation can be rewritten into the equivalent 
\begin_inset Quotes eld
\end_inset

dual formulation
\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\begin{eqnarray}
\mbox{minimize } &  & \sum_{i}^{N}\alpha_{i}-\frac{1}{2}\sum_{i,j}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}\mathbf{x_{i}}\mathbf{x_{j}}\label{eq:hard-margin-SVM-dual-formulation-min-w}\\
\mbox{subject to } &  & \alpha_{i}\geq0\mbox{ and }\sum_{i}^{N}\alpha_{i}y_{i}=0,\nonumber 
\end{eqnarray}

\end_inset

where the 
\begin_inset Formula $\alpha_{i}$
\end_inset

s are the 
\begin_inset Formula $N$
\end_inset

 variables to be solved by quadratic programming, and 
\begin_inset Formula $N$
\end_inset

 is the number of samples.
 After computing the solution for the dual formulation, the 
\begin_inset Formula $\mathbf{w}$
\end_inset

-vector is given in terms of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

: 
\begin_inset Formula $\mathbf{w}=\sum_{i}^{N}\alpha_{i}y_{i}\mathbf{x_{i}}$
\end_inset

 and the distance from the origin is 
\begin_inset Formula $b=y_{i}-\mathbf{w}\mathbf{x_{i}}$
\end_inset

, for any sample 
\begin_inset Formula $i$
\end_inset

 which has 
\begin_inset Formula $\alpha_{i}\neq0$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "BurbidgeBuxton2001"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
eq.
 13 on page 7
\end_layout

\end_inset

.
 The classifier is then 
\begin_inset Formula $f(\mathbf{x})=sgn(\sum_{i}^{N}\alpha_{i}y_{i}\mathbf{x_{i}}\mathbf{x}+b)$
\end_inset

.
\end_layout

\begin_layout Standard
If there is no solution, i.e.
 a separating hyperplane does not exist, one can do two things:
\end_layout

\begin_layout Itemize
make the margin a soft margin, i.e.
 allow some training samples to be misclassified.
\end_layout

\begin_layout Itemize
implicitly map samples into a higher dimensional space where a separating
 hyperplane exists.
 This implicit mapping is called the 
\emph on
kernel trick
\emph default
.
\end_layout

\begin_layout Standard
Both strategies will be described in the following.
\end_layout

\begin_layout Paragraph
Training a Soft-margin SVM
\end_layout

\begin_layout Standard
A soft-margin SVM is learned by introducing 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
slack variables
\end_layout

\end_inset


\emph on
slack variables 
\emph default

\begin_inset Formula $\xi_{i}\geq0$
\end_inset

 in the primal formulation:
\begin_inset Formula 
\begin{eqnarray*}
\mbox{minimize } &  & \frac{1}{2}\left\Vert \mathbf{w}\right\Vert +C\sum_{i}^{N}\xi_{i}\\
\mbox{subject to } &  & y_{i}(\mathbf{w}\cdot\mathbf{x_{i}}+b)\geq1-\xi_{i}\mbox{ for }i=1,\dots,N
\end{eqnarray*}

\end_inset

and in the dual formulation:
\begin_inset Formula 
\begin{eqnarray}
\mbox{minimize } &  & \sum_{i}^{N}\alpha_{i}-\frac{1}{2}\sum_{i,j}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}\mathbf{x_{i}}\mathbf{x_{j}}\label{eq:soft-margin-SVM-dual-formulation-min-w}\\
\mbox{subject to } &  & 0\leq\alpha_{i}\leq C\mbox{ and }\sum_{i}^{N}\alpha_{i}y_{i}=0\mbox{ for }i=1,\dots,N,\nonumber 
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $C>0$
\end_inset

 is a meta-parameter that controls trading off a small margin size 
\begin_inset Formula $\left\Vert \mathbf{w}\right\Vert $
\end_inset

 for allowing misclassifications of training samples.
 Many SVM implementations have a default of 1.
\end_layout

\begin_layout Paragraph
Kernel Trick
\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
kernel trick
\end_layout

\end_inset

The samples can be mapped into a higher-dimensional space where a separating
 hyperplane exists or has a larger margin.
 Mapping vectors 
\begin_inset Formula $\mathbf{x}$
\end_inset

 into a higher-dimensional space 
\begin_inset Formula $\Phi(\mathbf{x})$
\end_inset

 explicitly requires computing all dimensions, which is time-consuming or
 impossible for infinite-dimensional spaces.
\end_layout

\begin_layout Standard
However, in the dual formulations in equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hard-margin-SVM-dual-formulation-min-w"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:soft-margin-SVM-dual-formulation-min-w"

\end_inset

 above, the sample vectors 
\begin_inset Formula $\mathbf{x_{i}}$
\end_inset

 only occur together in a scalar product with another sample 
\begin_inset Formula $\mathbf{x_{j}}$
\end_inset

.
 The mapping can be done implicitly by not computing 
\begin_inset Formula $\Phi(\mathbf{x_{i}})$
\end_inset

 and 
\begin_inset Formula $\Phi(\mathbf{x_{j}})$
\end_inset

, but by defining a 
\emph on
kernel 
\emph default
function 
\begin_inset Formula $K$
\end_inset

 that computes the scalar product 
\begin_inset Formula $\Phi(\mathbf{x_{i}})\cdot\Phi(\mathbf{x_{j}})$
\end_inset

 directly: 
\begin_inset Formula 
\[
K(\mathbf{x_{i}},\mathbf{x_{j}}):\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}.
\]

\end_inset

Calculating 
\begin_inset Formula $K$
\end_inset

 is much cheaper than computing 
\begin_inset Formula $\Phi(\mathbf{x_{i}})$
\end_inset

, 
\begin_inset Formula $\Phi(\mathbf{x_{j}})$
\end_inset

.
 Not every function can be a kernel, it has to satisfy the Mercer conditions:
 for all square-integrable functions 
\begin_inset Formula $g(x)$
\end_inset

 the integral 
\begin_inset Formula 
\[
\int\int K(\mathbf{x_{i}},\mathbf{x_{j}})g(\mathbf{x_{i}})g(\mathbf{x_{j}})d\mathbf{x_{i}}d\mathbf{x_{j}}\geq0
\]

\end_inset

 must be non-negative.
 Otherwise, the quadratic programming problem may not have a solution 
\begin_inset CommandInset citation
LatexCommand cite
key "StatnikovGuyon2011"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Transductive Support Vector Machines
\begin_inset CommandInset label
LatexCommand label
name "sub:Transductive-Support-Vector-Machines"

\end_inset


\end_layout

\begin_layout Standard
A Transductive SVM 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
transductive SVM
\end_layout

\end_inset

 (TSVM) 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
TSVM
\end_layout

\end_inset

 is a semi-supervised version of a SVM 
\begin_inset CommandInset citation
LatexCommand cite
key "Joachims1999a"

\end_inset

.
 In addition to 
\begin_inset Formula $N$
\end_inset

 training samples 
\begin_inset Formula $\mathbf{x_{i}},$
\end_inset

 and their class labels 
\begin_inset Formula $y_{i}$
\end_inset

, we now know 
\begin_inset Formula $N^{*}$
\end_inset

 test samples 
\begin_inset Formula $\mathbf{x_{j}^{*}}$
\end_inset

 without labels.
 The objective of training a TSVM is to find class labels 
\begin_inset Formula $y_{j}^{*}$
\end_inset

 for the test samples, such that a separating hyperplane between the positive
 and negative test and training samples has minimal length
\begin_inset Formula 
\begin{eqnarray*}
\mbox{minimize } &  & \frac{1}{2}\left\Vert \mathbf{w}\right\Vert +C\sum_{i}^{N}\xi_{i}+C^{*}\sum_{j}^{N^{*}}\xi_{j}^{*}\\
\mbox{subject to } &  & y_{i}\mathbf{w}\cdot\mathbf{x_{i}}+b\geq1-\xi_{i}\mbox{ for }i=1,\ldots,N\\
 &  & y_{j}^{*}\mathbf{w}\cdot\mathbf{x_{j}^{*}}+b\geq1-\xi_{j}^{*}\mbox{ for }j=1,\ldots,N^{*}\\
 &  & \xi_{i}>0\mbox{ for }i=1,\ldots,N\\
 &  & \xi_{j}^{*}>0\mbox{ for }j=1,\ldots,N^{*},
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $C$
\end_inset

 allows trading off margin size for misclassification errors of training
 samples (as for the supervised SVM), and 
\begin_inset Formula $C^{*}$
\end_inset

 controls the influence of test samples.
 If 
\begin_inset Formula $C^{*}$
\end_inset

 is zero, the formulation above is equivalent to the inductive case.
\end_layout

\begin_layout Standard
Choosing class labels 
\begin_inset Formula $y_{j}^{*}$
\end_inset

 for the test samples 
\begin_inset Formula $\mathbf{x_{j}^{*}}$
\end_inset

 must be done before solving the quadratic programming problem, otherwise
 it is not convex anymore (see 
\begin_inset CommandInset citation
LatexCommand cite
key "CollobertBottou2006"

\end_inset

).
 
\begin_inset CommandInset citation
LatexCommand cite
key "Joachims1999a"

\end_inset

 does this by starting with an inductive SVM, i.e.
 setting 
\begin_inset Formula $C^{*}$
\end_inset

 to zero, and classifying the test samples 
\begin_inset Formula $\mathbf{x_{j}^{*}}$
\end_inset

 to obtain class labels 
\begin_inset Formula $y_{j}^{*}$
\end_inset

.
 Then he proceeds by incrementing 
\begin_inset Formula $C^{*}$
\end_inset

 while swapping two test samples' class labels if the objective function
 decreases.
 When 
\begin_inset Formula $C^{*}$
\end_inset

 has reached a user-defined threshold, training stops and the current test
 sample labels 
\begin_inset Formula $y_{j}^{*}$
\end_inset

, and the separating hyperplane defined by the parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset


\series bold
 
\series default
are returned.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Joachims1999a"

\end_inset

 notes that it is the co-occurrence of features that the transductive SVM
 exploits to transduce labels from training samples to test samples.
 For example, if a cluster of features always has a certain pattern in a
 group of samples containing mostly positively labelled training samples,
 then test samples showing that same feature pattern will likely also be
 positively labelled.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:transductive-feature-co-occurrence"

\end_inset

 is adapted from 
\begin_inset CommandInset citation
LatexCommand cite
key "Joachims1999a"

\end_inset

.
 It shows samples and features.
 Suppose sample A and F are given as training samples, with A labeled 
\begin_inset Quotes eld
\end_inset

positive
\begin_inset Quotes erd
\end_inset

 and F labeled 
\begin_inset Quotes eld
\end_inset

negative
\begin_inset Quotes erd
\end_inset

.
 Samples B-E are given as test samples and we have to label them.
 Transductive learning can use the co-occurrence of features 1-3, and the
 co-occurrence of features 4-6 to label samples B and C 
\begin_inset Quotes eld
\end_inset

positive
\begin_inset Quotes erd
\end_inset

 and samples D and E 
\begin_inset Quotes eld
\end_inset

negative
\begin_inset Quotes erd
\end_inset

.
 Although feature 3 does not occur in sample A, sample C belongs to the
 
\begin_inset Quotes eld
\end_inset

positive
\begin_inset Quotes erd
\end_inset

 class because sample B links samples A and C, by having features 1 and
 3 present simultaneously.
 Feature 7 has the same value in all samples and cannot contribute to the
 labeling.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/TSVM-co-occurrence-pattern.dia
	width 35col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:transductive-feature-co-occurrence"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Example of the co-occurrence of features that transductive learning can
 exploit.
\end_layout

\end_inset

Example of co-occurrence of features (columns, 1-7) that transductive learning
 can exploit to label samples (rows, A-F).
 (See text.)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Graphical Models
\end_layout

\begin_layout Subsection
Graphs
\end_layout

\begin_layout Standard
In the following we will define some graph nomenclature.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: I think I should write nodes in upper-case in the whole document.
 This is logical since I interchangeably refer to both nodes and random
 variables as the same entity, and random variables are upper-case.
 So write 
\begin_inset Formula $\mathbf{E}\ni e=(N_{1},N_{2})$
\end_inset

 below.
\end_layout

\begin_layout Plain Layout
TODO: I also should write edges upper-case since they are sets with two
 elements, and sets are written upper-case.
 Do this in the whole document.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A 
\emph on
graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
graph
\end_layout

\end_inset

 is a tuple 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 of nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

 and edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

.
 An 
\emph on
edge
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
edge
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{E}\ni E=(N_{1},N_{2})$
\end_inset

 consists of a pair of nodes 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

.
 Two nodes 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

 connected by an edge are called 
\emph on
neighbors
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
neighbor
\end_layout

\end_inset

.
 A 
\emph on
complete graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
complete graph
\end_layout

\end_inset

 is a graph with an edge 
\begin_inset Formula $E=(N_{1},N_{2})$
\end_inset

 for every distinct pair of nodes 
\begin_inset Formula $\mathbf{N}\ni N_{1}\neq N_{2}\in\mathbf{N}$
\end_inset

.
\end_layout

\begin_layout Standard
An edge can be 
\emph on
directed
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
directed edge
\end_layout

\end_inset

 or 
\emph on
undirected
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
undirected edge
\end_layout

\end_inset

, which means that the edge 
\begin_inset Formula $(N_{1},N_{2})$
\end_inset

 is either distinct from the edge 
\begin_inset Formula $(N_{2},N_{1})$
\end_inset

 or they are the same.
 If all edges of a graph are directed, then the graph is called a 
\emph on
directed graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
directed graph
\end_layout

\end_inset

; if all edges are undirected, then the graph is called an 
\emph on
undirected graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
undirected graph
\end_layout

\end_inset

.
 In a directed edge 
\begin_inset Formula $E=(P,C)$
\end_inset

, also written as 
\begin_inset Formula $P\rightarrow C$
\end_inset

, 
\begin_inset Formula $P$
\end_inset

 is called the 
\emph on
parent
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
parent
\end_layout

\end_inset

 and 
\begin_inset Formula $C$
\end_inset

 the 
\emph on
child
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
child
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
A 
\emph on
path
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
path
\end_layout

\end_inset

 is an ordered list of edges 
\begin_inset Formula $\mathbf{P}=[E_{1},E_{2},\dots,E_{n}]$
\end_inset

, so that the child of the previous edge is the parent of the next edge:
 If 
\begin_inset Formula $E_{i}=(P_{i},C_{i})$
\end_inset

 and 
\begin_inset Formula $E_{i+1}=(P_{i+1},C_{i+1})$
\end_inset

, then 
\begin_inset Formula $C_{i}=P_{i+1}$
\end_inset

.
 In the path 
\begin_inset Formula $\mathbf{P}$
\end_inset

, 
\begin_inset Formula $P_{p}$
\end_inset

 is called 
\emph on
ancestor
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
ancestor
\end_layout

\end_inset

 of 
\begin_inset Formula $C_{c}$
\end_inset

 if 
\begin_inset Formula $p\leq c$
\end_inset

, and 
\begin_inset Formula $C_{c}$
\end_inset

 is called  
\emph on
descendant
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
descendant
\end_layout

\end_inset

 of 
\begin_inset Formula $P_{p}$
\end_inset

 if 
\begin_inset Formula $p\leq c$
\end_inset

.
 If the child 
\begin_inset Formula $C_{j}$
\end_inset

 of any edge 
\begin_inset Formula $E_{j}$
\end_inset

 in 
\begin_inset Formula $\mathbf{P}$
\end_inset

 is equal to the parent 
\begin_inset Formula $P_{i}$
\end_inset

 of the same or a previous edge (i.e.
 
\begin_inset Formula $i\leq j$
\end_inset

), then the sub-path 
\begin_inset Formula $\mathbf{C}=[(P_{i},C_{i}),(P_{i+1},C_{i+1}),\dots,(P_{j},C_{j})]$
\end_inset

 is called a 
\emph on
cycle
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
cycle
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
A 
\emph on
directed acyclic graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
directed acyclic graph
\end_layout

\end_inset

 (DAG
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DAG
\end_layout

\end_inset

) is a directed graph that does not contain directed cycles.
\end_layout

\begin_layout Standard
A 
\emph on
clique
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
clique
\end_layout

\end_inset

 in an undirected graph is a subset of nodes 
\begin_inset Formula $\mathbf{N_{C}}\subset\mathbf{N}$
\end_inset

, such that every pair of nodes in the clique 
\begin_inset Formula $N_{1},N_{2}\in\mathbf{N_{C}}$
\end_inset

 has an edge in the graph: 
\begin_inset Formula $(N_{1},N_{2})\in\mathbf{E}$
\end_inset

.
 A 
\emph on
maximal clique
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
maximal clique
\end_layout

\end_inset

 is a clique where there are no nodes that can be added to it so that the
 resulting set of nodes is still a clique.
\end_layout

\begin_layout Standard
A set of nodes 
\begin_inset Formula $\mathbf{N_{A}}\subset\mathbf{N}$
\end_inset

 is 
\emph on
separated
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
separation of nodes
\end_layout

\end_inset

 from a set of nodes 
\begin_inset Formula $\mathbf{N_{B}}\subset\mathbf{N}$
\end_inset

 by a set of nodes 
\begin_inset Formula $\mathbf{N_{S}}\subset\mathbf{N}$
\end_inset

, if it is impossible to go (along the edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

 of the graph) from a node 
\begin_inset Formula $N_{1}\in\mathbf{N_{A}}$
\end_inset

 to a node 
\begin_inset Formula $N_{2}\in\mathbf{N_{B}}$
\end_inset

 without passing through any of the nodes in 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

.
\end_layout

\begin_layout Subsection
Definition of Graphical Models
\end_layout

\begin_layout Standard
Graphical models encode a factorization of a joint probability distribution
 with the help of a graph.
 Each node of the graph corresponds to a random variable of the joint probabilit
y distribution.
 The (union of the) edges of the graph encode the conditional probability
 distributions.
 Missing edges encode conditional independencies.
 The graph, together with probability functions over the structural elements
 of the graph is equivalent to the joint probability distribution.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-a-graphical-model"

\end_inset

 is an example of a (directed) graphical model.
 The random variables are 
\series bold

\begin_inset Formula $\mathbf{Sun}$
\end_inset


\series default
, 
\begin_inset Formula $\mathbf{Clouds}$
\end_inset

, 
\series bold

\begin_inset Formula $\mathbf{Temperature}$
\end_inset


\series default
, and 
\series bold

\begin_inset Formula $\mathbf{Icecream}$
\end_inset


\series default
.
 Each variable has two possible values, for example 
\begin_inset Formula $\mathbf{Sun}$
\end_inset

 can be either 
\begin_inset Quotes eld
\end_inset

S+
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

S-
\begin_inset Quotes erd
\end_inset

.
 The tables below 
\begin_inset Formula $\mathbf{Sun}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Clouds}$
\end_inset

 are called the priors, and the tables below 
\begin_inset Formula $\mathbf{Temperature}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Icecream}$
\end_inset

 are each conditional probability distributions.
 The graph, together with the priors and the conditional probability distributio
ns, encodes the joint probability distribution.
 For example, the first entry in the joint probability distribution table
 is 
\begin_inset Formula $P(\mathbf{Sun}=\mbox{S+},\mathbf{Clouds}=\mbox{C+},\mathbf{Temperature}=\mbox{T+},\mathbf{Icecream}=\mbox{I+})=$
\end_inset


\begin_inset Formula $P(\mathbf{Sun}=\mbox{S+})*P(\mathbf{Clouds}=\mbox{C+})*P(\mathbf{Temperature}=\mbox{T+}\mid\mathbf{Sun}=\mbox{S+},\mathbf{Clouds}=\mbox{C+})$
\end_inset


\begin_inset Formula $*P(\mathbf{Icecream}=\mbox{I+}\mid\mathbf{Temperature}=\mbox{T+})$
\end_inset


\begin_inset Formula $=0.5*0.5*0.7*0.7=0.1225.$
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/graphical-model-example.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-a-graphical-model"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Example of a graphical model.
\end_layout

\end_inset

Example of a graphical model.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Directed and Undirected Graphical Models
\end_layout

\begin_layout Standard
In the following, we will introduce and discuss directed graphical models
 and undirected graphical models.
 Directed graphical models are also called 
\emph on
Bayesian networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Bayesian network
\end_layout

\end_inset

 or 
\emph on
Belief Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Belief network
\end_layout

\end_inset

, and undirected graphical models are also called 
\emph on
Markov random fields
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov random field
\end_layout

\end_inset

 or 
\emph on
Markov Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov network
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
There is also an unification of Bayesian networks and Markov random fields,
 i.e.
 a graphical model that can have both directed and undirected edges.
 These networks are called 
\emph on
chain graphs
\emph default
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
probably not (since I won't go into chain graphs): reference.
 Referenz zu chain graph ist 
\begin_inset Quotes eld
\end_inset

Chain graph models and their causal interpretations Steffen L.
 Lauritzen Aalborg University, Denmark and Thomas S.
 Richardson
\begin_inset Quotes erd
\end_inset

 oder http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html (
\begin_inset Quotes erd
\end_inset

It is possible to have a model with both directed and undirected arcs, which
 is called a chain graph.
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_inset

, or 
\emph on
partially directed acyclic graphs
\emph default
 and are not discussed here.
\end_layout

\end_inset

.
 We will consider only models with discrete random variable values.
\end_layout

\begin_layout Subsubsection
Undirected Graphical Models
\begin_inset CommandInset label
LatexCommand label
name "par:Hammersley-Clifford-theorem"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Hier muß eine formale Definition von 
\begin_inset Quotes eld
\end_inset

undirected graphical model
\begin_inset Quotes erd
\end_inset

 stehen: d.h.
 Erkläre wie man aus dem graph zusammen mit den potentials (die über die
 Cliquen des Graphs definiert sind) die joint probability distribution berechnen
 kann.
 (das produkt der potentials ist die joint).
\end_layout

\begin_layout Plain Layout
I should at some point write that in undirected graphical models, the probabilit
y is factored into potentials (written 
\begin_inset Formula $\phi$
\end_inset

), where each factor corresponds to a clique in the undirected graph.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We want to encode a joint probability distribution 
\begin_inset Formula $P$
\end_inset

 in an undirected graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

.
 Every random variable corresponds to a node.
 The missing edges encode conditional independencies.
 A complete graph would encode no conditional independencies.
 However, we normally want to get a graph with the least possible edges
 (so that the independencies between random variables in the joint probability
 distribution are all represented in the graph).
 What properties does the joint probability distribution 
\begin_inset Formula $P$
\end_inset

 have to fulfill so that it can be encoded in an undirected graph and what
 does the minimal undirected graph 
\begin_inset Formula $G$
\end_inset

 look like? This is answered by the 
\emph on
Hammersley-Clifford theorem
\emph default
.
 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Hammersley-Clifford theorem
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "HammersleyClifford1971"

\end_inset


\end_layout

\begin_layout Paragraph
Hammersley-Clifford Theorem
\begin_inset CommandInset label
LatexCommand label
name "par:The-Hammersley-Clifford-theorem-of-Undirected-Graphical-Model"

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\mathbf{N}=\{N_{1},\dots,N_{n}\}$
\end_inset

 be a vector of random variables, 
\begin_inset Formula $P(\mathbf{N})$
\end_inset

 be a strictly positive joint probability distribution with 
\begin_inset Formula $P(\mathbf{n})>0$
\end_inset

 for all possible values 
\begin_inset Formula $\mathbf{n}$
\end_inset

 of 
\series bold

\begin_inset Formula $\mathbf{N}$
\end_inset


\series default
, and 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 be an undirected graph with each node corresponding to a random variable
 (i.e.
 
\begin_inset Formula $\mathbf{N}=\{N_{1},\dots,N_{n}\}$
\end_inset

).
 Then the following statements are equivalent:
\end_layout

\begin_layout Itemize
\begin_inset Formula $P(\mathbf{N})$
\end_inset

 factorizes according to the maximal cliques 
\begin_inset Formula $\mathbf{C_{1}},\dots,\mathbf{C_{m}}$
\end_inset

 in 
\begin_inset Formula $G$
\end_inset

, i.e.
 
\begin_inset Formula $P(\mathbf{N})=\frac{1}{Z}\phi_{1}(\mathbf{C_{1}})\cdot\ldots\cdot\phi_{m}(\mathbf{C_{m}})$
\end_inset

, where 
\begin_inset Formula $Z$
\end_inset

 is a scalar such that 
\begin_inset Formula $\sum_{\mathbf{n}}P(\mathbf{N}=\mathbf{n})=1$
\end_inset

, i.e.
 
\begin_inset Formula $Z=\sum_{N_{1},\dots,N_{n}}\phi_{1}(\mathbf{C_{1}})\cdot\ldots\cdot\phi_{3}(\mathbf{C_{3}})$
\end_inset

, and the 
\begin_inset Formula $\phi_{i}(\mathbf{C_{i}})$
\end_inset

 depend only on the states of the random variables in the clique 
\begin_inset Formula $\mathbf{C_{i}}=(N_{i_{1}},\dots,N_{i_{n}})$
\end_inset

 and must be positive for all possible states.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
On page 21 of the Hammersley-Clifford paper, the authors define the name
 
\begin_inset Quotes eld
\end_inset

light-coloured potential function
\begin_inset Quotes erd
\end_inset

.
 There, they also define 
\begin_inset Quotes eld
\end_inset

Gibbsian ensemble
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

 
\begin_inset Formula $P(\mathbf{N})$
\end_inset

 is then called a 
\emph on
Gibbs distribution
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Gibbs distribution
\end_layout

\end_inset

, 
\begin_inset Formula $Z$
\end_inset

 is called the 
\emph on
partition function
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
partition function
\end_layout

\end_inset

, and 
\begin_inset Formula $\phi(\mathbf{C_{i}})$
\end_inset

 are called the 
\emph on
potential functions
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
potential function
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset CommandInset label
LatexCommand label
name "local-Markov-property"

\end_inset

the 
\emph on
local Markov property
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
local Markov property
\end_layout

\end_inset

 holds for the graph 
\begin_inset Formula $G$
\end_inset

 and the joint probability distribution 
\begin_inset Formula $P$
\end_inset

: A node 
\begin_inset Formula $N_{i}$
\end_inset

 is conditionally independent from all non-neighbor nodes 
\begin_inset Formula $\mathbf{N}\backslash\mathbf{N_{neighbor(i)}}$
\end_inset

, given the states of the random variables 
\begin_inset Formula $\mathbf{N_{neighbor(i)}}$
\end_inset

 immediately connected to 
\begin_inset Formula $N$
\end_inset

: 
\begin_inset Formula $P(N_{i}\mid\mathbf{N_{neighbor(i)}})=P(N_{i}\mid\mathbf{N})$
\end_inset

.
\end_layout

\begin_layout Itemize
the 
\emph on
global Markov property
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
global Markov property
\end_layout

\end_inset

 holds for the graph 
\begin_inset Formula $G$
\end_inset

 and the joint probability distribution 
\begin_inset Formula $P$
\end_inset

: Given any disjoint subsets 
\begin_inset Formula $\mathbf{N_{A}},\mathbf{N_{B}},\mathbf{N_{S}}\subset\mathbf{N}$
\end_inset

 where 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

 separates the nodes 
\begin_inset Formula $\mathbf{N_{A}}$
\end_inset

 from the nodes 
\begin_inset Formula $\mathbf{N_{B}}$
\end_inset

, and given the states of the random variables of 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

, the nodes 
\begin_inset Formula $\mathbf{N_{A}}$
\end_inset

 are conditionally independent of the nodes 
\begin_inset Formula $\mathbf{N_{B}}$
\end_inset

: 
\begin_inset Formula $P(\mathbf{N_{A}}\mid\mathbf{N_{S}})=P(\mathbf{N_{A}\mid}\mathbf{N_{S}},\mathbf{N_{B}})$
\end_inset

.
\end_layout

\begin_layout Standard
Hence when we have a strictly positive joint probability distribution 
\begin_inset Formula $P(\mathbf{N})$
\end_inset

, we can determine the corresponding minimal graph 
\begin_inset Formula $G$
\end_inset

 with the following 
\begin_inset Quotes eld
\end_inset

brute-force
\begin_inset Quotes erd
\end_inset

 algorithm by using the local Markov property: Start with the empty graph.
 For a variable 
\begin_inset Formula $N_{i}$
\end_inset

, consider all sets of possible neighbor nodes, i.e.
 the power set 
\begin_inset Formula $\mathbf{P}=\mathbb{\mathfrak{\mathcal{P}}}(\{\mathbf{N}\backslash N_{i}\})$
\end_inset

, and check for each such possible set of neighbors 
\begin_inset Formula $\mathbf{N_{neighbors(i)}}$
\end_inset

, whether all non-neighbors 
\begin_inset Formula $\mathbf{N_{nonneighbors(i)}}$
\end_inset

 are independent of 
\begin_inset Formula $N_{i}$
\end_inset

, given the neighbors
\begin_inset Formula 
\[
N_{i}\bot\mathbf{N_{nonneighbors(i)}}\mid\mathbf{N_{neighbors(i)}},
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: the 
\backslash
bot 
\begin_inset Formula $\bot$
\end_inset

 above should be two vertical lines with one horizontal line _||_, but I
 don't know its latex name
\end_layout

\end_inset

 i.e.
 whether 
\begin_inset Formula $P(N_{i}\mid\mathbf{N_{neighbors(i)}},\mathbf{N_{nonneighbors(i)}})=P(N_{i}\mid\mathbf{N_{neighbors(i)}})$
\end_inset

.
 When a set of neighbors 
\begin_inset Formula $\mathbf{N_{neighbors}}$
\end_inset

 satisfying the conditional independency has been found, we can draw an
 edge 
\begin_inset Formula $E=(N_{i},N_{j})$
\end_inset

 between 
\begin_inset Formula $N_{i}$
\end_inset

 and all neighbors 
\begin_inset Formula $N_{j}\in\mathbf{N_{neighbors(i)}}$
\end_inset

.
 Repeat this for all variables 
\begin_inset Formula $N_{i}\in\mathbf{N}$
\end_inset

.
 Call the resulting set of edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

, and the minimal graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
NOTE: That's the crux of being NP; we have to enumerate all possible separators.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
On the other hand, if we have a graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 consisting of a given set of nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

 and edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

, and local conditional probabilities 
\begin_inset Formula $P(N\mid\mathbf{N}_{\mathbf{Parents}})$
\end_inset

 at each node fulfilling the Markov property, then we can derive from that
 the joint probability distribution over all random variables, or equivalently
 over all nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Example of an Undirected Graphical Model
\begin_inset CommandInset label
LatexCommand label
name "par:Example-of-an-Undirected-Graphical-Model"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/undirected-graphical-model-example.dia
	width 45col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-an-Undirected-Graph"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Example of an Undirected Graph.
\end_layout

\end_inset

Example of an Undirected Graph encoding the following conditional independencies
: the node pairs (N1,N4), (N1,N5), (N2,N4), (N2,N5),(N4,N5) are conditionally
 independent given node N3.
 (But (N1,N2) are not conditionally independent given N3.)
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
probably don't add potential functions, since I cannot show local and global
 Markov property.
 Potentials would be defined from the conditional independencies of the
 graph, and confirming them by computation from the potentials would be
 self-fulfilling, since they were defined from the structure of the graph.
\end_layout

\begin_layout Plain Layout
Der to do Eintrag war mal: In 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Example-of-an-Undirected-Graphical-Model"

\end_inset

 eine Figure mit einem MRF: insert example graphics with an undirected graph
 with node names.
 The figure text should denote the maximal cliques text and illustrate the
 local Markov property for a node 
\begin_inset Formula $N$
\end_inset

 and the global Markov property for two nodes 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

 separated by a set of nodes 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

.
 Maybe also print definitions of the potential functions for the maximal
 cliques (but these are probably too large).
 The graphics could be similar to Figure 2.3 in 
\begin_inset Quotes eld
\end_inset

~/uni/publication/zusammenfassung/graphical model/Koller+al_SRL07.pdf
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For example, if there are the three cliques 
\begin_inset Formula $\mathbf{C_{1}}=\{N_{1},N_{2},N_{3}\}$
\end_inset

, 
\begin_inset Formula $\mathbf{C_{2}}=\{N_{3},N_{4}\}$
\end_inset

, and 
\begin_inset Formula $\mathbf{C_{3}}=\{N_{3},N_{5}\}$
\end_inset

 (see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-an-Undirected-Graph"

\end_inset

), then the joint probability distribution 
\begin_inset Formula $P$
\end_inset

 (also called 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Gibbs distribution
\end_layout

\end_inset

Gibbs distribution) can be written as: 
\begin_inset Formula 
\[
P(N_{1},\dots,N_{5})=\frac{1}{Z}\phi_{1}(\mathbf{C_{1}})\phi_{2}(\mathbf{C_{2}})\phi_{3}(\mathbf{C_{3}}),
\]

\end_inset

where 
\begin_inset Formula $\phi_{1}(\mathbf{C_{1}})=\phi(N_{1},N_{2},N_{3})$
\end_inset

 is the potential function of clique 1 (
\emph on
clique potential
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
clique potential
\end_layout

\end_inset

), and is a function of the 3 random variables 
\begin_inset Formula $N_{1},N_{2},N_{3}$
\end_inset

 in the clique.
 
\begin_inset Formula $Z$
\end_inset

 is the partition function and must normalize the function so that 
\begin_inset Formula $P$
\end_inset

 is a probability: 
\begin_inset Formula 
\[
Z=\sum_{X_{1},\dots,X_{n}}\phi_{1}(\mathbf{C_{1}})\phi_{2}(\mathbf{C_{2}})\phi_{3}(\mathbf{C_{3}}).
\]

\end_inset


\end_layout

\begin_layout Standard
In practice, 
\begin_inset Formula $\phi_{1}(N_{1},N_{2},N_{3})$
\end_inset

 can be represented by a table that holds, for each possible combination
 of states of the three random variables, a positive real number.
 For example, if each of the three random variables has two states, then
 the table (with 
\begin_inset Formula $2^{3}$
\end_inset

 entries) could look like in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Example-of-potential-function"

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n_{3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\phi(N_{1}=n_{1},N_{2}=n_{2},N_{3}=n_{3})$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.124
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.553
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.842
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.258
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Example-of-potential-function"

\end_inset

Example of a potential function 
\begin_inset Formula $\phi$
\end_inset

 represented as a table.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Directed Graphical Models
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/directed-graphical-model-example.dia
	width 30col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-a-Directed-Graph"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Example of a Directed Graph.
\end_layout

\end_inset

Example of a Directed Graph.
 Note that the graph is acyclic, and the nodes are layed out in layers.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Directed graphical models are also called 
\emph on
Bayesian Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Bayesian network
\end_layout

\end_inset

 or 
\emph on
Belief Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Belief network
\end_layout

\end_inset

.
 (See e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "KollerTaskar2007,Neal1992"

\end_inset

.)
\end_layout

\begin_layout Standard
Like undirected graphical models, directed graphical models represent an
 implicit joint probability distribution over all random variables present
 in the model.
 The graph must be directed and acyclic, and each node of the graph is associate
d with a random variable.
 See figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-a-Directed-Graph"

\end_inset

 for an example.
\end_layout

\begin_layout Standard
A directed graphical model is defined by the directed graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

, a prior probability distribution 
\begin_inset Formula $P(N_{noparents})$
\end_inset

 at the nodes that do not have parents 
\begin_inset Formula $\mathbf{N_{noparents}}$
\end_inset

, and conditional probability distributions 
\begin_inset Formula $P(N_{hasparents}\mid\mathbf{N_{parents}})$
\end_inset

 at the nodes that have parents 
\begin_inset Formula $\mathbf{N_{hasparents}}$
\end_inset

.
 In the latter nodes the conditional probability distribution may only be
 conditional on its immediate parents, not on distant ancestors.
 The directed graph encodes the set of conditional independencies between
 the random variables: A random variable 
\begin_inset Formula $X$
\end_inset

 of the graph is conditionally independent of all random variables that
 are not descendants of 
\begin_inset Formula $X$
\end_inset

 (i.e.
 
\begin_inset Formula $\mathbf{N}\backslash\mathbf{N_{descendant(\mathbf{\mathrm{X}})}}$
\end_inset

) given the values of the parent variables of 
\begin_inset Formula $X$
\end_inset

 (this is called the 
\emph on
local Markov property
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
local Markov property
\end_layout

\end_inset

 of directed graphs):
\begin_inset Formula 
\[
X\perp(\mathbf{N}\backslash\mathbf{N_{descendant(\mathrm{X})}})\mid\mathbf{N_{parent(\mathrm{X})}}.
\]

\end_inset

(In general, independencies betweem any two sets of variables conditioned
 on a third can be derived from the structure of the graph using 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
d-separation
\end_layout

\end_inset


\emph on
d-separation
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "Barber2012"

\end_inset

.)
\end_layout

\begin_layout Paragraph
The Joint Probability Distribution Encoded by the Graphical Model
\begin_inset CommandInset label
LatexCommand label
name "par:The-Joint-Encoded-by-Directed-Graphical-Model"

\end_inset


\end_layout

\begin_layout Standard
In a directed graphical model, the random variables 
\begin_inset Formula $\mathbf{N}$
\end_inset

 can be totally ordered such that 
\begin_inset Formula $N_{i}$
\end_inset

 comes before 
\begin_inset Formula $N_{j}$
\end_inset

 if there is a directed path from 
\begin_inset Formula $N_{i}$
\end_inset

 to 
\begin_inset Formula $N_{j}$
\end_inset

 in the graph, and the order is unspecified if there is no directed path
 between 
\begin_inset Formula $N_{i}$
\end_inset

 and 
\begin_inset Formula $N_{j}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
equation 8, page 77 bottom
\end_layout

\end_inset

.
 (Thus, there are graphs which have more than one compatible ordering, for
 example in the graph 
\begin_inset Formula $A\rightarrow C\leftarrow B$
\end_inset

, the ordering can be 
\begin_inset Formula $A,B,C$
\end_inset

 as well as 
\begin_inset Formula $B,A,C$
\end_inset

.) In the following, the ordering is expressed as the subscript 
\begin_inset Formula $i\in\mathbb{N}$
\end_inset

 of the random variable 
\begin_inset Formula $N_{i}$
\end_inset

.
 The node 
\begin_inset Formula $N_{i}$
\end_inset

 has associated with it the conditional probability 
\begin_inset Formula $P(N_{i}=n_{i}\mid N_{j}=n_{j}\forall j<i)$
\end_inset

.
 (
\begin_inset Quotes eld
\end_inset

The probability that the random variable 
\begin_inset Formula $N_{i}$
\end_inset

 is equal to 
\begin_inset Formula $n_{i}$
\end_inset

 given that the random variables 
\series bold

\begin_inset Formula $N_{j}$
\end_inset

 
\series default
are equal to 
\begin_inset Formula $n_{j}$
\end_inset

 where all subscripts 
\begin_inset Formula $j$
\end_inset

 that are smaller than 
\begin_inset Formula $i$
\end_inset

.
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Standard
The joint probability distribution encoded by the graph is
\begin_inset Formula 
\[
P(\mathbf{N})=P(N_{1}=n_{1},N_{2}=n_{2},\dots,N_{n}=n_{n})=\prod_{i=1}^{n}P(N_{i}=n_{i}\mid N_{j}=n_{j}\forall j<i).
\]

\end_inset


\end_layout

\begin_layout Paragraph
Example How to Represent the Conditional Probability Distribution
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe TODO: hier muss ein Beispiel mit einer Tabelle einer joint probability
 distribution stehen.
\end_layout

\end_inset

For example, if the 
\begin_inset Formula $N_{i}$
\end_inset

 can only assume discrete 
\begin_inset Formula $n_{i}$
\end_inset

, the conditional probability distribution can be represented by a table
 with a size exponential in the number of involved nodes.
 When a node has 
\begin_inset Formula $a$
\end_inset

 ancestors, each of which has 
\begin_inset Formula $s$
\end_inset

 possible states, and the node itself also has 
\begin_inset Formula $s$
\end_inset

 possible states, then the table must contain one probability for each of
 the 
\begin_inset Formula $s^{a}\cdot(s-1)$
\end_inset

 possible states.
 (
\begin_inset Formula $s^{a}$
\end_inset

 for the combinations of the values of the ancestors and 
\begin_inset Formula $(s-1)$
\end_inset

 for the values the node itself can assume.)
\end_layout

\begin_layout Subsection
Exact Inference in Graphical Models
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Do 
\begin_inset Formula $\mathbf{K},\mathbf{W},\mathbf{U}$
\end_inset

 have to fulfill some relationship for directed acyclic graphs in the inference
 example below? (For example, does 
\begin_inset Formula $\mathbf{K}$
\end_inset

 have to be a subset of the parents of 
\series bold

\begin_inset Formula $\mathbf{W}$
\end_inset


\series default
?)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
NOTE: Maybe I should explain the algorithm 
\begin_inset Quotes eld
\end_inset

ELIMINATE
\begin_inset Quotes erd
\end_inset

 from 
\begin_inset Quotes eld
\end_inset

~/uni/publication/zusammenfassung/graphical model/inference/jordan.pdf
\begin_inset Quotes erd
\end_inset

 and exercise it for an instance of a directed and undirected graph.
 This would be more systematic than it is now.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Inference in a graphical model is the task of answering a query about the
 joint probability distribution encoded by the graph, or of a part of the
 joint probability distribution.
 For example, one might be interested in the overall probability of the
 configuration of a sub-set of variables.
 However, in the general case this takes exponential time.
\end_layout

\begin_layout Subsubsection
Naive Approach: Marginalizing the Joint 
\begin_inset CommandInset label
LatexCommand label
name "par:Exact-Inference-in-Directed-and-Undirected Graphical Models"

\end_inset


\end_layout

\begin_layout Standard
Since a graphical model is a representation of a joint probability distribution,
 it can answer queries about probabilities of the joint probability distribution
 encoded by the graphical model by first explicitly calculating the joint,
 then marginalizing out non-interesting variables.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: I'm not sure I know how 
\begin_inset Quotes eld
\end_inset

Inference
\begin_inset Quotes erd
\end_inset

 is defined.
 I should look it up in a book, e.g.
 
\begin_inset Quotes eld
\end_inset

Probabilistic Graphical Model, by Koller and Friedman
\begin_inset Quotes erd
\end_inset

.
 But that book is not downloadable.
\end_layout

\begin_layout Plain Layout
There is a paper, however: http://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pd
f
\end_layout

\end_inset


\end_layout

\begin_layout Standard
For example, we might want to infer the probability of a configuration of
 variables when the values of only some of the variables are known.
 We can then partition the variables 
\begin_inset Formula $\mathbf{V}$
\end_inset

 of a graphical model into three disjoint groups:
\end_layout

\begin_layout Enumerate
the known variables 
\begin_inset Formula $\mathbf{K}$
\end_inset

,
\end_layout

\begin_layout Enumerate
the unknown variables 
\begin_inset Formula $\mathbf{W}$
\end_inset

 that we want to know the probability distribution of,
\end_layout

\begin_layout Enumerate
the unknown variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

 that we do not care about.
\end_layout

\begin_layout Standard
Let the known values of 
\begin_inset Formula $\mathbf{K}$
\end_inset

 be written 
\series bold

\begin_inset Formula $\mathbf{k}$
\end_inset


\series default
.
 The unknown values of 
\begin_inset Formula $\mathbf{W}$
\end_inset

 are named 
\series bold

\begin_inset Formula $\mathbf{w}$
\end_inset


\series default
, and the values of 
\begin_inset Formula $\mathbf{U}$
\end_inset

, 
\begin_inset Formula $\mathbf{u}$
\end_inset

.
 
\end_layout

\begin_layout Standard
How to calculate the joint 
\begin_inset Formula $P(\mathbf{W},\mathbf{U},\mathbf{K})$
\end_inset

 encoded by the graphical model was defined in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:The-Hammersley-Clifford-theorem-of-Undirected-Graphical-Model"

\end_inset

 for undirected graphical models and in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:The-Joint-Encoded-by-Directed-Graphical-Model"

\end_inset

 for directed graphical models.
 Let's now turn our attention to marginalizing out the non-interesting variables
 
\begin_inset Formula $\mathbf{U}$
\end_inset

.
\end_layout

\begin_layout Standard
When we want to find the probability of configuration 
\begin_inset Formula $\mathbf{W}=\mathbf{w}$
\end_inset

, given 
\begin_inset Formula $\mathbf{K}=\mathbf{k}$
\end_inset

, we can first write the query in terms of the joint probability distribution.
 We have to condition on 
\begin_inset Formula $\mathbf{K}$
\end_inset

, and marginalize out the unknown variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

 that we do not care about: 
\begin_inset Formula 
\begin{eqnarray}
P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k}) & = & \sum_{\mathbf{U}}P(\mathbf{W}=\mathbf{w},\mathbf{U}=\mathbf{u}|\mathbf{K}=\mathbf{k})\nonumber \\
 & = & \sum_{\mathbf{U}}\frac{P(\mathbf{W}=\mathbf{w},\mathbf{U}=\mathbf{u},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}.\label{eq:Inference in graphical models}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In the above formula there is a sum over all variables 
\series bold

\begin_inset Formula $\mathbf{U}$
\end_inset


\series default
.
 Writing this out, we obtain 
\begin_inset Formula 
\begin{eqnarray}
P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k}) & = & \sum_{\mathbf{U}}\frac{P(\mathbf{W}=\mathbf{w},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\label{eq:Inference in graphical models, written out}\\
 & = & \sum_{U_{1}}\sum_{U_{2}}\cdots\sum_{U_{n}}\frac{P(\mathbf{W}=\mathbf{w},U_{1}=u_{1},U_{2}=u_{2},\dots,U_{n}=u_{n},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}.\nonumber 
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In the general case (if the joint probability cannot be factorized), this
 nested sum needs 
\begin_inset Formula $O(|\mathbf{u}|^{|\mathbf{U}|})=O(|\mathbf{u}|^{n})$
\end_inset

 operations to compute, where 
\begin_inset Formula $|\mathbf{u}|$
\end_inset

 is the number of possible values a variable 
\begin_inset Formula $U_{i}$
\end_inset

 can have (assuming for simplicity that all random variables 
\begin_inset Formula $U_{i}$
\end_inset

 have the same number of possible values 
\begin_inset Formula $|\mathbf{u}|$
\end_inset

) and 
\begin_inset Formula $|\mathbf{U}|$
\end_inset

 is the number of unknown variables 
\begin_inset Formula $U_{i}$
\end_inset

.
 This is because all possible combinations of variable assignments have
 to be considered.
 Thus, for this naive marginalization run-time is exponential in the number
 of variables, and therefore intractable.
\end_layout

\begin_layout Standard
However, we have not yet considered the structure of the graph.
 We can improve run-time in some cases of graphs and for some sets of variables
 
\begin_inset Formula $\mathbf{K}$
\end_inset

, 
\begin_inset Formula $\mathbf{W}$
\end_inset

, 
\begin_inset Formula $\mathbf{U}$
\end_inset

, as shown by the following example.
\end_layout

\begin_layout Subsubsection
Factorization in Undirected Graphical Models
\end_layout

\begin_layout Standard
In the case of an undirected graphical model, we can factorize the joint
 probability into independent sub-joint-probabilities according to the cliques.
 For instance, if the random variables 
\begin_inset Formula $\mathbf{U}=\{U_{1},U_{2},\dots,U_{m}\}$
\end_inset

 are composed of cliques 
\begin_inset Formula $\mathbf{C_{1}},\mathbf{C_{2}},\dots,\mathbf{C_{n}}$
\end_inset

, so that 
\begin_inset Formula $P(\mathbf{U})=\frac{1}{Z}\phi_{1}(\mathbf{C_{1}})\phi_{2}(\mathbf{C_{2}})\dots\phi_{n}(\mathbf{C_{n}})$
\end_inset

, then the above sum can be written, using Hammersley-Clifford, as
\begin_inset Formula 
\begin{eqnarray*}
 & P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k})\\
= & \sum_{U_{1}}\sum_{U_{2}}\cdots\sum_{U_{n}}\frac{P(\mathbf{W}=\mathbf{w},U_{1}=u_{1},U_{2}=u_{2},\dots,U_{n}=u_{n},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\\
= & \sum_{\mathbf{C}_{1}}\cdots\sum_{\mathbf{C}_{n}\backslash\{C_{1},\dots,C_{n-1}\}}\frac{\frac{1}{Z}\mbox{\phi}_{1}(\mathbf{W}=\mathbf{w},\mathbf{C}_{1}=\mathbf{c}_{1},\mathbf{K}=\mathbf{k})\cdot\ldots\cdot\mbox{\phi}_{n}(\mathbf{W}=\mathbf{w},\mathbf{C}_{n}=\mathbf{c}_{n},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\\
= & \frac{\frac{1}{Z}\left(\sum_{\mathbf{C}_{1}}\mbox{\phi}_{1}(\mathbf{W}=\mathbf{w},\mathbf{C}_{1}=\mathbf{c}_{1},\mathbf{K}=\mathbf{k})\cdot\left(\ldots\cdot\left(\sum_{\mathbf{C}_{n}\backslash\{C_{1},\dots,C_{n-1}\}}\mbox{\phi}_{n}(\mathbf{W}=\mathbf{w},\mathbf{C}_{n}=\mathbf{c}_{n},\mathbf{K}=\mathbf{k})\right)\right)\right)}{P(\mathbf{K}=\mathbf{k})}.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The sums in the last line are nested sums that sum over all possible states
 in the corresponding cluster.
 (If we order cliques descendingly by the number of clique members then
 
\begin_inset Formula $\mathbf{C_{1}}$
\end_inset

 is the largest clique.) We still need to sum over the state combinations
 in the largest clique.
 Therefore the run-time is at least 
\begin_inset Formula $O(|\mathbf{u}|^{|\mathbf{C_{m}}|})$
\end_inset

, where 
\begin_inset Formula $\mathbf{C_{m}}$
\end_inset

 is the clique with the largest number of variables in it.
 This is still an exponential run-time.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
An algorithm that systematizes exact inference in graphical model is the
 
\emph on
junction tree
\emph default
 
\emph on
algorithm
\emph default
.
 (It is not discussed here, see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Jordan2004"

\end_inset

.)
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Example of Inference in a Directed Graphical Model
\begin_inset CommandInset label
LatexCommand label
name "sub:Example-of-Exact-Inference-in-a-directed-graphical-model"

\end_inset


\end_layout

\begin_layout Standard
Here we show an example how inference in a specific directed graphical model
 is done.
 In our example, the directed graphical model is composed of several densely
 connected layers, where the nodes within a layer are not connected, and
 they have outgoing directed connections only to nodes in the adjacent layer
 below.
\end_layout

\begin_layout Standard
When the probability distribution of the parent nodes are known, inferring
 the probability distributions of child nodes is easy: just multiply the
 probability of the parents with the conditional probability of the child.
 To keep the example interesting, given the probability distributions of
 the nodes in the bottom layer, we want to infer the probability distributions
 for all the other nodes.
\end_layout

\begin_layout Paragraph
Deep Belief Networks
\end_layout

\begin_layout Standard
For example consider the graph in figure 
\begin_inset CommandInset ref
LatexCommand vref
reference "fig:Example-of-a-Directed-Graph"

\end_inset

.
 This directed acyclic graph has the following directed connections between
 its nodes: 
\begin_inset Formula $G_{1}\rightarrow H_{1}$
\end_inset

, 
\begin_inset Formula $G_{1}\rightarrow H_{2}$
\end_inset

, 
\begin_inset Formula $G_{2}\rightarrow H_{1}$
\end_inset

, 
\begin_inset Formula $G_{2}\rightarrow H_{2}$
\end_inset

, 
\begin_inset Formula $H_{1}\rightarrow V_{1}$
\end_inset

, 
\begin_inset Formula $H_{1}\rightarrow V_{2}$
\end_inset

, 
\begin_inset Formula $H_{2}\rightarrow V_{1}$
\end_inset

, 
\begin_inset Formula $H_{2}\rightarrow V_{2}$
\end_inset

.
 Furthermore, the following conditional probability distributions are given:
 
\begin_inset Formula $P(H_{1}|G_{1},G_{2})$
\end_inset

, 
\begin_inset Formula $P(H_{2}|G_{1},G_{2})$
\end_inset

, 
\begin_inset Formula $P(V_{1}|H_{1},H_{2})$
\end_inset

, 
\begin_inset Formula $P(V_{2}|H_{1},H_{2})$
\end_inset

.
 This layered architecture, where each node in a layer is connected to all
 nodes in adjacent layers, defines a directed graphical model called 
\emph on
Deep Belief Network
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
 (We will discuss them in section  
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Deep-Belief-Network"

\end_inset

.)
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Bayes Theorem Applied to Inference in a Deep Belief Network
\end_layout

\begin_layout Standard
Now assume that 
\begin_inset Formula $P(V_{1})$
\end_inset

, 
\begin_inset Formula $P(V_{2})$
\end_inset

 are given and we want to infer 
\begin_inset Formula $P(G_{1}\mid\mathbf{V})$
\end_inset

, 
\begin_inset Formula $P(G_{2}\mid\mathbf{V})$
\end_inset

, 
\begin_inset Formula $P(H_{1}\mid\mathbf{V})$
\end_inset

, 
\begin_inset Formula $P(H_{2}\mid\mathbf{V})$
\end_inset

.
 Using Bayes' Theorem (
\begin_inset Formula $\mbox{posterior}=\mbox{likelihood}\cdot\mbox{prior}$
\end_inset

) we get
\begin_inset Formula 
\begin{eqnarray}
P(H_{1},H_{2}|V,V_{2}) & = & \frac{P(V_{1},V_{2}|H_{1},H_{2})P(H_{1},H_{2})}{P(V_{1},V_{2})}\nonumber \\
 & = & \frac{P(V_{1},V_{2}|H_{1},H_{2})\left(\sum_{g_{1}}\sum_{g_{2}}P(g_{1})P(g_{2})P(H_{1}|g_{1},g_{2})P(H_{2}|g_{1},g_{2})\right)}{P(V_{1},V_{2})}\nonumber \\
 & = & \frac{1}{P(V_{1},V_{2})}\cdot P(V_{1}|H_{1},H_{2})P(V_{2}|H_{1},H_{2})\nonumber \\
 &  & \cdot\left(\sum_{g_{1}}\sum_{g_{2}}P(g_{1})P(g_{2})P(H_{1}|g_{1},g_{2})P(H_{2}|g_{1},g_{2})\right),\label{eq:deep-network-conditional-probability}
\end{eqnarray}

\end_inset

where 
\begin_inset Formula 
\[
P(V_{1},V_{2})=\sum_{h_{1}}\sum_{h_{2}}P(V_{1}|H_{1}=h_{1},H_{2}=h_{2})P(V_{2}|H_{1}=h_{1},H_{2}=h_{2})P(H_{1}=h_{1},H_{2}=h_{2}).
\]

\end_inset

We can make the last transformation because 
\begin_inset Formula $\ensuremath{V_{1}}$
\end_inset

 and 
\begin_inset Formula $\ensuremath{V_{2}}$
\end_inset

 are independent given 
\begin_inset Formula $\ensuremath{H_{1}},\ensuremath{H_{2}}$
\end_inset

 (local Markov property).
 To determine 
\begin_inset Formula $P(H_{1}|V_{1},V_{2})$
\end_inset

 and 
\begin_inset Formula $P(H_{2}|V_{1},V_{2})$
\end_inset

, we have to marginalize the other variable in 
\begin_inset Formula $\mathbf{H}$
\end_inset

 out:
\begin_inset Formula 
\begin{eqnarray}
P(H_{1}|V_{1},V_{2}) & = & \sum_{h_{2}}P(H_{1},H_{2}=h_{2}|V_{1},V_{2})\nonumber \\
P(H_{2}|V_{1},V_{2}) & = & \sum_{h_{1}}P(H_{1}=h_{1},H_{2}|V_{1},V_{2})\label{eq:deep-network-marginalization}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Since we now have 
\begin_inset Formula $P(H_{1}|\mathbf{V})$
\end_inset

, using 
\begin_inset Formula $P(H_{1})=\sum_{v_{1}}\sum_{v_{2}}P(H_{1},V_{1}=v_{1},V_{2}=v_{2})$
\end_inset

 and 
\begin_inset Formula $P(H_{1}|V_{1},V_{2})=P(H_{1},V_{1},V_{2})/P(V_{1},V_{2})$
\end_inset

, we can determine 
\begin_inset Formula $P(H_{1})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(H_{1}) & = & \sum_{v_{1}}\sum_{v_{2}}P(H_{1}|V_{1}=v_{1},V_{2}=v_{2})P(V_{1}=v_{1},V_{2}=v_{2}).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(H_{2})$
\end_inset

 can be computed similarly.
 Now that we know 
\begin_inset Formula $P(H_{1})$
\end_inset

 and 
\begin_inset Formula $P(H_{2})$
\end_inset

, we can repeat the steps to determine 
\begin_inset Formula $P(G_{1}\mid\mathbf{H})$
\end_inset

 and 
\begin_inset Formula $P(G_{2}\mid\mathbf{H})$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Inference in Deep Belief Networks is Complicated
\begin_inset CommandInset label
LatexCommand label
name "par:Exact-Inference-in-Deep-Belief-Networks-is-Complicated"

\end_inset


\end_layout

\begin_layout Standard
The previous example shows that inference in a directed graphical model
 with densely connected layers is complicated.
 If there are 
\begin_inset Formula $n$
\end_inset

 binary variables in 
\begin_inset Formula $\mathbf{H}$
\end_inset

 and 
\begin_inset Formula $\mathbf{G}$
\end_inset

, then the computation of 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:deep-network-marginalization"

\end_inset

 takes 
\begin_inset Formula $O(2^{n-1}n)$
\end_inset

 due to having to marginalize out all variables in 
\begin_inset Formula $\mathbf{H}$
\end_inset

 except one (the term 
\begin_inset Formula $2^{n-1}$
\end_inset

), and this for all variables (the term 
\begin_inset Formula $n$
\end_inset

).
 In addition, this applies only if 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

 is known already.
 But in the computation of 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

, equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:deep-network-conditional-probability"

\end_inset

, there are sums over the variables 
\begin_inset Formula $g_{1}$
\end_inset

 and 
\begin_inset Formula $g_{2}$
\end_inset

, which take another 
\begin_inset Formula $O(2^{n})$
\end_inset

 in general.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: In Rainers Korrektur steht hier etwas, was ich nicht entziffern kann,
 in etwa 
\begin_inset Quotes eld
\end_inset

comp.
 shps
\begin_inset Quotes erd
\end_inset

 oder 
\begin_inset Quotes eld
\end_inset

comp.
 slrps
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

The phenomenon that leads to this computational problem is called 
\emph on
explaining away
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
explaining away
\end_layout

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: 
\begin_inset Quotes eld
\end_inset

~/uni/publication/zusammenfassung/graphical model/explaining_away/pami93.pdf
\begin_inset Quotes erd
\end_inset

 says that explaining away can not only decrease but also INCREASE belief:
 (in the abstract) 
\begin_inset Quotes eld
\end_inset

The opposite of explaining away also can occur, where the confirmation of
 one cause increases belief in another.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The posterior of 
\begin_inset Formula $H_{1}$
\end_inset

 depends on all conditional probabilities of the model, in this example,
 
\begin_inset Formula $P(\mathbf{V}\mid\mathbf{H})$
\end_inset

 and 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{G})$
\end_inset

.
 For Deep Belief Networks, which are a kind of Directed Graphical Model
 with densely connected layers, the conditional probabilities 
\begin_inset Formula $P(\mathbf{V}\mid\mathbf{H})$
\end_inset

 and 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{G})$
\end_inset

 have parameters called 
\begin_inset Quotes eld
\end_inset

weights
\begin_inset Quotes erd
\end_inset

 associated with them, and inference of the layer immediately above 
\begin_inset Formula $\mathbf{V}$
\end_inset

, namely 
\begin_inset Formula $\mathbf{H}$
\end_inset

, requires knowing all weights in the graph, not just those of 
\begin_inset Formula $P(\mathbf{V}\mid\mathbf{H})$
\end_inset

.
 In addition, explaining away requires us to marginalize out all variables
 in 
\begin_inset Formula $\mathbf{H}$
\end_inset

 except one, and this for all variables in 
\begin_inset Formula $\mathbf{H}$
\end_inset

.
 A further problem is that we have to integrate over all variables in all
 layers above 
\begin_inset Formula $\mathbf{H}$
\end_inset

 if we are interested in 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

.
\end_layout

\begin_layout Standard
These procedures become infeasible in a Belief Network with more than a
 few parents per node.
 This is a problem in learning.
 If we want to learn the parameters of a Deep Belief Network, we have to
 do inference.
 However, we will see
\begin_inset Note Note
status open

\begin_layout Plain Layout
in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:A-Fast-Learning-Algorithm-for-Deep-Belief-Networks"

\end_inset


\end_layout

\end_inset

 that there is a fast approximate learning algorithm.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
probably not TODO since we don't need them for later: 
\end_layout

\begin_layout Subsubsection
General Algorithm
\end_layout

\begin_layout Plain Layout
In the following paragraph we summarize the ELIMINATE algorithm from 
\begin_inset CommandInset citation
LatexCommand cite
key "JordanWeiss2002"

\end_inset

.
 This algorithm works for both directed and undirected graphical models
 and computes the marginal probabilities of all nodes.
\end_layout

\begin_layout Paragraph
The Junction-tree Algorithm
\end_layout

\begin_layout Plain Layout
The run-time can be shortened by marginalizing variables out as early as
 possible.
 This must be done according to the factorization imposed by the graph structure.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Intractability of Exact Inference on General Graphs
\end_layout

\begin_layout Standard
Here we reference a proof by 
\begin_inset CommandInset citation
LatexCommand cite
key "ChandrasekaranHarsha2012"

\end_inset

 that low treewidth of the graph underlying inference in a graphical model
 is the only structural property that enables tractable inference.
\end_layout

\begin_layout Standard
A 
\emph on
triangulated graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
triangulated graph
\end_layout

\end_inset

 is a graph where every loop having at least four nodes contains a 
\emph on
chord
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
chord
\end_layout

\end_inset

, i.e.
 an edge between two non-adjacent nodes in the loop
\begin_inset CommandInset citation
LatexCommand cite
key "Barber2012"

\end_inset

.
\end_layout

\begin_layout Standard
For a triangulated graph, the 
\emph on
treewidth
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
treewidth
\end_layout

\end_inset

 is the number of nodes contained in the largest clique minus one.
 For a graph of any form, the treewidth is the treewidth of the triangulation
 that minimizes the treewidth.
 For directed acyclic graphs, the maximal number of parents of any node
 is the critical number, since it determines the treewidth of the moralized
\begin_inset Foot
status open

\begin_layout Plain Layout
You obtain a moralized graph from a directed acyclic graph by introducing
 edges between all parents of a node, and then replacing directed edges
 by undirected edges.
\end_layout

\end_inset

 graph.
 (This was also shown by 
\begin_inset CommandInset citation
LatexCommand cite
key "KwisthoutVanderGaag2010"

\end_inset

.) 
\begin_inset Note Note
status open

\begin_layout Plain Layout
NOTE: Stimmt das Folgende? Ich dachte, man könnte das aufgrund der Struktur
 von Formel (1) im Paper sagen, aber glaube gerade, dass das nicht reicht:
 
\begin_inset Quotes eld
\end_inset

For directed graphs, factorization according to the edges of the graph serves
 the purpose of cliques in the proof.
 Thus, the cliques are defined on the moral graph of the directed graph.
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A graph is a 
\emph on
minor
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
minor of a graph
\end_layout

\end_inset


\emph on
 
\emph default
of a graph 
\begin_inset Formula $G$
\end_inset

 if it is obtained from 
\begin_inset Formula $G$
\end_inset

 by one or more of the following operations: 1.
 deletion of an edge, 2.
 deletion of a node together with all edges containing that node, and 3.
 contraction of an edge, which means an edge 
\begin_inset Formula $(N_{1},N_{2})$
\end_inset

 and its two nodes 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

 are replaced with a new single node and the new node has edges to all nodes
 that 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

 had edges to.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Es fehlt noch eine Definition von 
\begin_inset Formula $P/Poly$
\end_inset

 und von der grid-minor hypothesis.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f(k)$
\end_inset

 be the largest number such that every graph of treewidth 
\begin_inset Formula $k$
\end_inset

 contains a grid of size 
\begin_inset Formula $f(k)\times f(k)$
\end_inset

 as minor.
 The 
\emph on
grid-minor hypothesis
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
grid-minor hypothesis
\end_layout

\end_inset

 states that 
\begin_inset Formula $f(k)$
\end_inset

 is polynomial in 
\begin_inset Formula $k$
\end_inset

 (see 
\begin_inset CommandInset citation
LatexCommand cite
key "ChandrasekaranHarsha2012"

\end_inset

).
 
\begin_inset CommandInset citation
LatexCommand cite
key "ChekuriChuzhoy2014"

\end_inset

 proved it in 2014.
\begin_inset Note Note
status open

\begin_layout Plain Layout
(in Theorem 1.1 whose proof is essentially the whole rest of their paper
 (
\begin_inset Quotes eld
\end_inset

In order to complete the proof of Theorem 1.1, it now suffices to prove Theorem
 4.1
\begin_inset Quotes erd
\end_inset

))
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Susi hat mir bewiesen, dass Theorem 1.1 in 
\begin_inset CommandInset citation
LatexCommand cite
key "ChekuriChuzhoy2014"

\end_inset

 die 
\emph on
obere 
\emph default
Schranke von Theorem 2.1 in 
\begin_inset CommandInset citation
LatexCommand cite
key "ChandrasekaranHarsha2012"

\end_inset

 updated und diese obere Schranke dann 
\emph on
polynomiell 
\emph default
in 
\begin_inset Formula $k$
\end_inset

 ist.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A decision problem is in the complexity class 
\begin_inset Formula $\mathbf{NP}$
\end_inset

 if it can be decided in time polynomial in the input by a non-deterministic
 Turing machine.
 A decision problem is in the complexity class 
\begin_inset Formula $\mathbf{P/Poly}$
\end_inset

 if it can be decided in time polynomial in the input 
\begin_inset Formula $x$
\end_inset

 by a deterministic Turing machine that receives as input not only 
\begin_inset Formula $x$
\end_inset

, but also an advice string of length at most polynomial in the length of
 
\begin_inset Formula $x$
\end_inset

 that may only depend on the length of 
\begin_inset Formula $x$
\end_inset

, not 
\begin_inset Formula $x$
\end_inset

 itself
\begin_inset Foot
status open

\begin_layout Plain Layout
The advice string allows modeling pre-computation in the computation.
\end_layout

\end_inset

.
 (See for example 
\begin_inset CommandInset citation
LatexCommand cite
key "Sipser1996,Goldreich2008,AroraBarak2009"

\end_inset

.) The problem whether or not 
\begin_inset Formula $\mathbf{NP}\subseteq\mathbf{P/Poly}$
\end_inset

 is unsolved.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
(see Wikipedia articles 
\begin_inset Quotes eld
\end_inset

P/Poly
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Advice (complexity)
\begin_inset Quotes erd
\end_inset

).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "ChandrasekaranHarsha2012"

\end_inset

 showed that under the assumptions that the grid-minor hypothesis is true
 (which it is), and that 
\begin_inset Formula $\mathbf{NP}\nsubseteq\mathbf{P/poly}$
\end_inset

, and given that arbitrary potential functions should be allowed, low treewidth
 is the only structural property of otherwise arbitrary graphs that ensures
 tractable run-time of exact inference on the graphical model belonging
 to the graph
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
the paper says on page 1: 
\begin_inset Quotes eld
\end_inset

Among these problems is inference in graphi- cal models, which, as mentioned
 earlier, can be solved in polynomial-time if the treewidth of the underly-
 ing graphs is bounded.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
I rephrased 
\begin_inset CommandInset citation
LatexCommand cite
key "ChandrasekaranHarsha2012"

\end_inset

's theorem 2.1 to make it more easily comparable to 
\begin_inset CommandInset citation
LatexCommand cite
key "ChekuriChuzhoy2014"

\end_inset

's theorem 1.1: For all graphs 
\begin_inset Formula $G$
\end_inset

 with treewidth 
\begin_inset Formula $k$
\end_inset

 greater than 
\begin_inset Formula $\kappa_{GM}(g)$
\end_inset

, the grid 
\begin_inset Formula $g\times g$
\end_inset

 is a minor of 
\begin_inset Formula $G$
\end_inset

, where 
\begin_inset Formula $\kappa_{GM}(g)$
\end_inset

 is polynomial in 
\begin_inset Formula $g$
\end_inset

.
\end_layout

\end_inset

.
 There exists no inference algorithm with complexity polynomial in the treewidth.
\end_layout

\begin_layout Standard
If the assumption 
\begin_inset Formula $\mathbf{NP}\nsubseteq\mathbf{P/poly}$
\end_inset

 is correct, then the only way to reduce the computational cost of exact
 inference on a general graph with a given number of nodes is to reduce
\begin_inset Note Note
status open

\begin_layout Plain Layout
Page 2, section 
\begin_inset Quotes eld
\end_inset

Main Result
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

 the treewidth or to choose restricted potential functions (for example
 constants) whose products do not require multiplication or can be pre-computed
\begin_inset Note Note
status open

\begin_layout Plain Layout
Page 2, section 
\begin_inset Quotes eld
\end_inset

Main Result
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Quotes eld
\end_inset

there exists a choice of potential functions ...
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
 Therefore, in practice, the joint probability is approximated, for example
 by Gibbs Sampling.
\end_layout

\begin_layout Subsection
Approximate Inference
\end_layout

\begin_layout Standard
Approximate Inference in general graphical models by means of Gibbs Sampling
 was first described by 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

.
 We first have to introduce Markov chains.
\end_layout

\begin_layout Subsubsection
Markov Chains
\end_layout

\begin_layout Standard
For the following section, see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Norris1997,GrinsteadSnell2003"

\end_inset

 as references.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Bild von einer Markov chain.
 Evtl.
 von einer Markov chain, an der ich die verschiedenen Eigenschaften wie
 period, irreducible, etc.
 zeigen kann.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Markov property: Memorylessness
\end_layout

\begin_layout Standard
A 
\emph on
Markov chain
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov chain
\end_layout

\end_inset

 is a sequence of random variables 
\begin_inset Formula $X_{t}$
\end_inset

, where 
\begin_inset Formula $t\in\mathbb{N}_{0}$
\end_inset

 denotes the discrete index of time.
 In a Markov chain, each random variable 
\begin_inset Formula $X_{t}$
\end_inset

 may depend only on the state of the random variable at the immediate previous
 time point 
\begin_inset Formula $t-1$
\end_inset

, i.e.
 
\begin_inset Formula 
\[
P(X_{t}=x_{t}\mid X_{0}=x_{0},\dots,X_{t-1}=x_{t-1})=P(X_{t}=x_{t}\mid X_{t-1}=x_{t-1})
\]

\end_inset

must hold for all 
\begin_inset Formula $t\geq1$
\end_inset

.
 This memorylessness is called the 
\emph on
Markov property
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov property
\end_layout

\end_inset

.
 In a Markov chain, possible states 
\begin_inset Formula $x_{t}$
\end_inset

 at each time point are discrete and from the same set 
\begin_inset Formula $\mathbb{S}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
x_{t} & \in & \mathbb{S}\ \ \ \mbox{for all \ensuremath{t}}.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Paragraph
Time-homogeneous Markov Chain and Transition Matrix
\end_layout

\begin_layout Standard
A 
\emph on
time-homogeneous Markov chain
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
time-homogeneous Markov chain
\end_layout

\end_inset

 is a Markov chain in which the conditional probability 
\begin_inset Formula $P(X_{t}=x_{t}|X_{t-1}=x_{t-1})$
\end_inset

 is the same for all time points 
\begin_inset Formula $t$
\end_inset

, i.e.
 
\begin_inset Formula 
\[
P(X_{t}=x_{t}\mid X_{t-1}=x_{t-1})=P(X_{t-1}=x_{t-1}\mid X_{t-2}=x_{t-2})
\]

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
for all time points 
\begin_inset Formula $t\geq2$
\end_inset

.
 If this is the case, then this conditional probability
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: In the following, should I replace 
\begin_inset Formula $p$
\end_inset

 with 
\begin_inset Formula $P$
\end_inset

, because it is a matrix?
\end_layout

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula 
\[
P(X_{t}=j\mid X_{t-1}=i)\eqqcolon p_{ij}
\]

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 is independent of the current time 
\begin_inset Formula $t$
\end_inset

 and can be written as the 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
two-dimensional
\end_layout

\end_inset

matrix 
\begin_inset Formula $p$
\end_inset

, called the 
\family default
\series default
\shape default
\size default
\emph on
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
transition matrix
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
transition matrix
\end_layout

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
 In the following we will only deal with time-homogeneous Markov chains.
\end_layout

\begin_layout Paragraph
Computing future states from the initial distribution
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
It makes sense to talk about the distribution of states a Markov chain can
 have at a certain time point 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\end_inset

Let 
\begin_inset Formula $d^{(t)}$
\end_inset

 be the distribution of 
\begin_inset Formula $X_{t}$
\end_inset

, also named the 
\emph on
probability vector
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
probability vector
\end_layout

\end_inset

, a row vector of length 
\begin_inset Formula $|\mathbb{S}|$
\end_inset

.
 An entry 
\begin_inset Formula $d_{i}^{(t)}$
\end_inset

 is equal to the probability of 
\begin_inset Formula $X_{t}$
\end_inset

 having state 
\begin_inset Formula $x_{i}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
d_{i}^{(t)} & = & P(X_{t}=x_{i}).
\end{eqnarray*}

\end_inset

This implies 
\begin_inset Formula $\sum_{i}d_{i}^{(t)}=1$
\end_inset

 for all 
\begin_inset Formula $t$
\end_inset

.
 The distribution 
\begin_inset Formula $d^{(t+1)}$
\end_inset

 can be computed from 
\begin_inset Formula $d^{(t)}$
\end_inset

 by matrix multiplication with the transition matrix 
\begin_inset Formula $p$
\end_inset

: 
\begin_inset Formula 
\[
d^{(t+1)}=d^{(t)}p.
\]

\end_inset

Given an initial distribution 
\begin_inset Formula $d^{(0)}$
\end_inset

 and the transition matrix 
\begin_inset Formula $p$
\end_inset

, all 
\begin_inset Formula $d^{(t)}$
\end_inset

 are specified by 
\begin_inset Formula 
\[
d^{(t)}=d^{(0)}p^{t}.
\]

\end_inset


\end_layout

\begin_layout Paragraph
Stationary distribution
\end_layout

\begin_layout Standard
There are time-homogeneous Markov chains whose state distribution stays
 constant once it has assumed a certain state distribution.
 Such state distributions are called 
\emph on
invariant
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
invariant distribution
\end_layout

\end_inset

 or 
\emph on
stationary distribution
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
stationary distribution
\end_layout

\end_inset

.
 A stationary distribution 
\begin_inset Formula $\pi$
\end_inset

 must fulfill the following equation: 
\begin_inset Formula 
\[
\pi p=\pi
\]

\end_inset

If 
\begin_inset Formula $d^{(t)}=\pi$
\end_inset

, then 
\begin_inset Formula $d^{(t+u)}=\pi$
\end_inset

 for all 
\begin_inset Formula $u\geq0$
\end_inset

.
 A Markov chain can have more than one stationary distribution.
\end_layout

\begin_layout Paragraph
Detailed balance/Reversibility
\end_layout

\begin_layout Standard
A Markov chain with transition matrix 
\begin_inset Formula $p$
\end_inset

 satisfies 
\emph on
detailed balance
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
detailed balance
\end_layout

\end_inset


\emph on
 
\emph default
if there exists a probability distribution 
\begin_inset Formula $\pi=(\pi_{1},\pi_{2},\dots,\pi_{n})$
\end_inset

 such that 
\begin_inset Formula 
\[
\pi_{j}p_{ji}=\pi_{i}p_{ij}\ \ \ \mbox{for all \ensuremath{i}, \ensuremath{j}}.
\]

\end_inset

Such a Markov chain is also
\begin_inset Note Note
status open

\begin_layout Plain Layout
Is this true? In 
\begin_inset Quotes eld
\end_inset

neal.pdf
\begin_inset Quotes erd
\end_inset

, it says 
\begin_inset Quotes eld
\end_inset

Often, we will use time reversible homogeneous Markov chains that satisfy
 the 
\series bold
more restrictive 
\series default
condition of detailed balance
\begin_inset Quotes erd
\end_inset

.
 So Markov chains satisfying detailed balance seem to be a subset of the
 Markov chains satisfying time reversibility.
\end_layout

\end_inset

 called a 
\emph on
reversible
\emph default
 Markov chain
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
reversible Markov chain
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Norris1997"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Theorem 1.9.3 shows 
\emph on
detailed balance 
\emph default
and 
\emph on
reversible
\emph default
 to mean the same thing
\end_layout

\end_inset

.
 A Markov chain with the detailed balance property 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: why is detailed balance not enough for a Markov chain to have an unique
 stationary distribution in AltRBM-proof.pdf? why do we need irreducibility
 and aperiodicity?
\end_layout

\begin_layout Plain Layout
irreducibility has to be shown so that we can be sure that the markov chain
 has an unique stationary distribution.
\end_layout

\begin_layout Plain Layout
aperiodicity: 
\begin_inset Quotes eld
\end_inset

AltRBM-proof.pdf
\begin_inset Quotes erd
\end_inset

 says 
\begin_inset Quotes eld
\end_inset

One can show that an irreducible and aperiodic Markov chain on a finite
 state space is guarantied to converge to its stationary distribution (see,
 e.g., [6]).
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

has at least one stationary distribution, where each stationary distribution
 
\begin_inset Formula $\lambda$
\end_inset

 fulfills the detailed balance condition: 
\begin_inset Formula $\lambda_{j}p_{ji}=\lambda_{i}p_{ij}$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $j$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
key "Norris1997"

\end_inset

, Lemma 1.9.2
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
While having detailed balance implies that a Markov chain has a stationary
 distribution, the reverse is not true: there are Markov chains with a stationar
y distribution but not satisfying detailed balance.
 For example, a Markov chain with transition probabilities 
\begin_inset Formula 
\[
(p_{ij})=\left(\begin{array}{ccc}
0 & 2/3 & 1/3\\
1/3 & 0 & 2/3\\
2/3 & 1/3 & 0
\end{array}\right)
\]

\end_inset

does not satisfy detailed balance, but 
\begin_inset Formula $\pi=\left(\begin{array}{ccc}
\frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{array}\right)$
\end_inset

 is a stationary distribution of this Markov chain 
\begin_inset CommandInset citation
LatexCommand cite
key "Norris1997"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Example 1.9.4
\end_layout

\end_inset

.
\end_layout

\begin_layout Paragraph
Fundamental Theorem of Markov Chains
\begin_inset CommandInset label
LatexCommand label
name "par:Fundamental-Theorem-of-Markov-Chains"

\end_inset


\end_layout

\begin_layout Standard
Under what conditions does a Markov chain have an unique stationary distribution
? This is answered by the 
\emph on
Fundamental Theorem of Markov Chains
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
fundamental theorem of Markov chains
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: which was proved by Kolmogorov?
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Behrends2000"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Theorem 7.4 in ~/uni/publication/zusammenfassung/markov chains/markovbuch2008.pdf.
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
Theorem: Let 
\begin_inset Formula $d^{(0)}$
\end_inset

 and 
\begin_inset Formula $e^{(0)}$
\end_inset

 be any probability vectors of a Markov chain with transition probabilities
 
\begin_inset Formula $(p_{ij})$
\end_inset

, where 
\begin_inset Formula $(p_{ij})$
\end_inset

 is irreducible, positive-recurrent and aperiodic.
 Then the Markov chain converges to the unique stationary distribution 
\begin_inset Formula $\pi$
\end_inset

, irrespective of the starting states:
\begin_inset Formula 
\[
\lim_{t\rightarrow\infty}d^{(0)}p^{t}=\lim_{t\rightarrow\infty}e^{(0)}p^{t}=\pi.
\]

\end_inset


\end_layout

\begin_layout Standard
The definitions of irreducibility, positive recurrence, and aperiodicity
 follow.
\end_layout

\begin_layout Paragraph
Irreducibility
\end_layout

\begin_layout Standard
A Markov chain is called 
\emph on
irreducible
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
irreducible
\end_layout

\end_inset

, if it is possible to go from any state 
\begin_inset Formula $i$
\end_inset

 of the Markov chain to any state 
\begin_inset Formula $j$
\end_inset

 (possibly in more than 1 steps).
 Formally, a Markov chain is called irreducible, if its states are all in
 the same (and only) closed subset.
 A subset 
\begin_inset Formula $C$
\end_inset

 of 
\begin_inset Formula $\mathbb{S}$
\end_inset

 is called 
\emph on
closed 
\emph default
if 
\begin_inset Formula $p_{ij}=0$
\end_inset

 whenever 
\begin_inset Formula $i\in C$
\end_inset

 and 
\begin_inset Formula $j\notin C$
\end_inset

.
 (Remember that 
\begin_inset Formula $\mathbb{S}$
\end_inset

 is the set of possible states of the Markov chain.)
\end_layout

\begin_layout Paragraph
Positive Recurrence
\end_layout

\begin_layout Standard
A state 
\begin_inset Formula $i$
\end_inset

 of a Markov chain is 
\emph on
positive recurrent
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
positive recurrent
\end_layout

\end_inset

, if we expect the Markov chain to take a finite number of steps until it
 is in state 
\begin_inset Formula $i$
\end_inset

 again, when it started in state 
\begin_inset Formula $i$
\end_inset

 at time point 
\begin_inset Formula $0$
\end_inset

.
 To define positive recurrence formally, we have to define auxiliary measures
 first.
 The probability that state 
\begin_inset Formula $j$
\end_inset

 is visited at time step 
\begin_inset Formula $k$
\end_inset

 for the first time after the Markov chain had been in state 
\begin_inset Formula $i$
\end_inset

 at time point 
\begin_inset Formula $0$
\end_inset

 is 
\begin_inset Formula 
\[
f_{ij}^{(k)}:=P(X_{1}\neq j,X_{2}\neq j,\dots,X_{k-1}\neq j,X_{k}=j\mid X_{0}=i).
\]

\end_inset

The probability that state 
\begin_inset Formula $j$
\end_inset

 is ever reached from state 
\begin_inset Formula $i$
\end_inset

 is 
\begin_inset Formula 
\[
f_{ij}^{*}:=\sum_{k=1}^{\infty}f_{ij}^{(k)}.
\]

\end_inset

With this, we can define the expected number of steps for the Markov chain
 to reach state 
\begin_inset Formula $j$
\end_inset

, when starting at state 
\begin_inset Formula $i$
\end_inset

:
\begin_inset Formula 
\[
\mu_{ij}:=\sum_{k=1}^{\infty}kf_{ij}^{(k)}.
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
I won't use 
\begin_inset CommandInset citation
LatexCommand cite
key "Behrends2000"

\end_inset

 for positive recurrence, because he defines (on page 46 of markovbuch2008.pdf)
 
\begin_inset Formula $\mu_{ij}:=\sum_{k=1}^{\infty}kf_{ij}^{(k)}+(1-f_{ij}^{*})\infty$
\end_inset

, which is not defined for 
\begin_inset Formula $f_{ij}^{*}=1$
\end_inset

.
 I don't understand his definition.
\end_layout

\end_inset

This number is also called the 
\emph on
mean recurrence time
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
mean recurrence time
\end_layout

\end_inset

.
 With these definitions, we can define a state to be 
\emph on
transient
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
transient
\end_layout

\end_inset

, positive recurrent, or 
\emph on
null recurrent
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
null recurrent
\end_layout

\end_inset

:
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $f_{ii}^{*}<1$
\end_inset

, the state 
\begin_inset Formula $i$
\end_inset

 is called transient.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $f_{ii}^{*}=1$
\end_inset

, the state 
\begin_inset Formula $i$
\end_inset

 is called recurrent.
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $f_{ii}^{*}=1$
\end_inset

 and 
\begin_inset Formula $\mu_{ii}<\infty$
\end_inset

, the state 
\begin_inset Formula $i$
\end_inset

 is called 
\emph on
positive recurrent
\emph default
.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $f_{ii}^{*}=1$
\end_inset

 and 
\begin_inset Formula $\mu_{ii}=\infty$
\end_inset

, the state 
\begin_inset Formula $i$
\end_inset

 is called 
\emph on
null recurrent
\emph default
.
\end_layout

\end_deeper
\begin_layout Standard
It can be proven that when there are finitely many states, there are no
 null recurrent states (see Proposition 7.2.
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Behrends2000"

\end_inset

).
\end_layout

\begin_layout Paragraph
Aperiodicity
\end_layout

\begin_layout Standard
The definition of an 
\emph on
aperiodic
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
aperiodic
\end_layout

\end_inset

 state is shorter.
 The period of a state
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
period of a state
\end_layout

\end_inset

 
\begin_inset Formula $i$
\end_inset

 is defined as the greatest common denominator of the number of time steps
 needed for a Markov chain so that it is possible to be in state 
\begin_inset Formula $i$
\end_inset

 again, after it was in state 
\begin_inset Formula $i$
\end_inset

 before: 
\begin_inset Formula 
\[
period(i)=gcd(\{k\mid k\geq0,(p^{k})_{ii}>0\}).
\]

\end_inset

If 
\begin_inset Formula $period(i)=1$
\end_inset

, state 
\begin_inset Formula $i$
\end_inset

 is called 
\emph on
aperiodic
\emph default
.
 If all states of a Markov chain are aperiodic, the Markov chain is called
 aperiodic.
\end_layout

\begin_layout Paragraph
Markov Chain Monte Carlo (MCMC)
\end_layout

\begin_layout Standard

\emph on
Markov Chain Monte Carlo
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov Chain Monte Carlo algorithm
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
MCMC algorithm
\end_layout

\end_inset

 is an algorithm to sample from a multivariate probability distribution.
 It sets up a Markov chain that converges to the desired stationary distribution
 and iterates it through time until the stationary distribution is approximated
 sufficiently.
\end_layout

\begin_layout Standard
A trick can be useful.
 To efficiently calculate the stationary distribution 
\begin_inset Formula $\pi$
\end_inset

, compute only the transition matrix for time steps that are a power of
 2, i.e.
 
\begin_inset Formula $p^{2^{t}}$
\end_inset

.
 This can be done by starting with 
\begin_inset Formula $p^{1}:=p$
\end_inset

, repeated squaring: 
\begin_inset Formula $p^{2t}:=p^{t}p^{t}$
\end_inset

, and assigning the stationary distribution 
\begin_inset Formula $\pi=d^{(0)}p^{2^{t}}$
\end_inset

 for a large enough 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
MCMC will be described in the next section.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: And also write down how the target distribution is specified.
 At this point I think that this depends on the specific instance of MCMC,
 e.g.
 Gibbs Sampling needs conditional probabilities.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Gibbs Sampling
\end_layout

\begin_layout Standard
Here, we will review Gibbs Sampling to show how 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 used it to approximate inference in directed and undirected graphical models.
\end_layout

\begin_layout Standard

\emph on
Gibbs Sampling
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Gibbs sampling
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "GemanGeman1984"

\end_inset

 is an instance of a Markov chain Monte Carlo (MCMC) algorithm.
 Its goal is to generate samples from a multivariate joint probability distribut
ion, without having to know its closed form.
 The generated samples can then be used to compute an approximation of the
 mean of a distribution, for example.
\end_layout

\begin_layout Standard
To use a Gibbs Sampler, one must construct a Markov chain with its (only)
 stationary distribution equal to the target distribution.
 Each random variable is updated in turn, based on the conditional probabilities.
\end_layout

\begin_layout Paragraph
Gibbs Sampling Requires Closed-form Conditional Probabilities
\end_layout

\begin_layout Standard
Suppose that the multivariate target distribution is 
\begin_inset Formula $P(X_{1},\dots,X_{n})$
\end_inset

.
 Here, for each random variable 
\begin_inset Formula $X_{i}\in\{X_{1},\dots,X_{n}\}$
\end_inset

, the conditional probability of the variable given all other variables
 must be known in closed form, so that it can be evaluated: 
\begin_inset Formula 
\[
P(X_{i}\mid\mathbf{X_{j}},j\in\{1,\dots,n\}\backslash i)=P(X_{i}\mid X_{1},\dots,X_{i-1},X_{i+1},X_{n}).
\]

\end_inset

Another prerequisite is that Gibbs Sampling, which requires sampling from
 the conditional probability distribution and iterating, should be faster
 than sampling from the joint target probability distribution directly.
\begin_inset Note Note
status open

\begin_layout Plain Layout
We already showed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Exact-Inference-in-Deep-Belief-Networks-is-Complicated"

\end_inset

 that exact inference is infeasible in Deep Belief Networks.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
A Markov Chain in Multiple Dimensions
\end_layout

\begin_layout Standard
The variable updates in Gibbs Sampling can be regarded as a Markov chain.
 However, we must first define how the random variables of Gibbs Sampling
 are mapped to the random variable of the Markov chain.
 One possibility is to re-map the random variables and their states to a
 single random variable.
\end_layout

\begin_layout Standard
Above, Markov chains were defined for a single state variable 
\begin_inset Formula $X$
\end_inset

.
 But in Gibbs Sampling there are usually more than one variable, written
 
\begin_inset Formula $X_{i}\in\{X_{1},\dots,X_{n}\}$
\end_inset

 above.
 If there are a finite number of random variables in Gibbs Sampling, and
 the state space of these variables is also finite, say of size 
\begin_inset Formula $m$
\end_inset

, then the (therefore also finite) number of states of these variables can
 be encoded in a single variable with a state space of size 
\begin_inset Formula $m^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
For example, say there are 3 variables, each of which can assume 2 states,
 and we want to encode these 
\begin_inset Formula $2^{3}$
\end_inset

 different possible states in one variable.
 Then the Markov chain has one variable with 
\begin_inset Formula $2*2*2$
\end_inset

 different possible states.
\end_layout

\begin_layout Paragraph
Constructing a Markov Chain from Base Transitions
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
hier schreiben, dass die Markov-Kette aus dem Produkt der einzelnen Variable-Upd
ates (
\begin_inset Quotes eld
\end_inset

base transitions
\begin_inset Quotes erd
\end_inset

) gebildet wird.
 Siehe Neal93, Kapitel 4.1 (Seite 47).
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Neal schreibt auf Seite 44 ganz unten: 
\begin_inset Quotes eld
\end_inset

For example, each 
\begin_inset Formula $B_{k}$
\end_inset

 might change only some subset of the variables making up the state.
\begin_inset Quotes erd
\end_inset

 Also definiert er anscheinend Markov-Ketten mit mehrdimensionalem state
 vector?
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 suggests
\begin_inset Note Note
status open

\begin_layout Plain Layout
on page 45
\end_layout

\end_inset

 constructing a non-homogeneous Markov chain with transition matrix 
\begin_inset Formula $T$
\end_inset

 by applying base transitions in turn, each of which describe the probability
 of a state change of one random variable.
 The base transitions are named 
\begin_inset Formula $B_{k}(x,x')$
\end_inset

, where 
\begin_inset Formula $k\in1,2,\ldots,s$
\end_inset

 is the index of the base transition, 
\begin_inset Formula $x$
\end_inset

 is the starting state of the transition, 
\begin_inset Formula $x'$
\end_inset

 is the target state of the transition.
 
\begin_inset Formula $B_{k}(x,x')$
\end_inset

 is the probability of the transition and must be strictly greater than
 zero for all values of 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $x'$
\end_inset

, to make the Markov chain irreducible.
 At each time-point 
\begin_inset Formula $a*s+k-1$
\end_inset

 with 
\begin_inset Formula $a\in\mathbf{\mathbb{N}}$
\end_inset

, the next single base transition 
\begin_inset Formula $B_{k}(x,x')$
\end_inset

 is then applied:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
T_{as+k-1}(x,x')=B_{k}(x,x').
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 also notes that the required properties for a Markov chain to converge
 are fulfilled: If each of the base transitions 
\begin_inset Formula $B_{k}$
\end_inset

 have a stationary distribution, then the non-homogeneous 
\begin_inset Formula $T$
\end_inset

 also has a stationary distribution.
\begin_inset Note Note
status open

\begin_layout Plain Layout
vielleicht TODO: Hinzufügen, dass Neal auf Seite 45 schreibt: 
\begin_inset Quotes eld
\end_inset

Unlike the case with mixtures, though, even when all the B k satisfy detailed
 balance, T generally does not.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Initialization of the Gibbs Sampler
\end_layout

\begin_layout Standard
A Gibbs Sampler starts by specifying a start value 
\begin_inset Formula $x_{i}^{(0)}$
\end_inset

 for each random variable 
\begin_inset Formula $X_{i}$
\end_inset

.
 Since the Markov chain must be constructed such that it converges to its
 only stationary distribution the choice of start values is not critical,
 but it influences the numbers of iterations needed until the Gibbs Sampler
 returns samples from the target distribution.
 So the start value should be close to the expected value of the distribution.
\end_layout

\begin_layout Paragraph
Iterating
\end_layout

\begin_layout Standard
Then an iterative process is started.
 In each iteration 
\begin_inset Formula $t$
\end_inset

, each random variable 
\begin_inset Formula $X_{i}$
\end_inset

 is updated by sampling a new value 
\begin_inset Formula $x_{i}^{(t)}$
\end_inset

 from the conditional probability distribution 
\begin_inset Formula 
\begin{equation}
P(X_{i}\mid X_{1}=x_{1}^{(t-1)},\dots,X_{i-1}=x_{i-1}^{(t-1)},X_{i+1}=x_{i+1}^{(t-1)},\dots,X_{n}=x_{n}^{(t-1)}).\label{eq:Gibbs sampling: CPD}
\end{equation}

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
 Then all the other variables 
\begin_inset Formula $X_{i}$
\end_inset

 must be updated.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are different alternative ways in which the random variables are updated,
 for example updating the random variables can be done in random order,
 or sequentially, or a whole 
\begin_inset Quotes eld
\end_inset

block
\begin_inset Quotes erd
\end_inset

 of multiple random variables can be sampled from the conditional distribution
 given all the other random variables (e.g.
 
\begin_inset Formula $P(X_{i_{1}},X_{i_{2}},X_{i_{3}}\mid\mathbf{X_{j}=x_{j}^{(t-1)}},j\in\{1,\dots,n\}\backslash\{i_{1},i_{2},i_{3}\})$
\end_inset

).
\end_layout

\begin_layout Standard
If the Markov chain fulfills irreducibility, positive recurrence, and aperiodici
ty, then it is guaranteed to converge to its stationary distribution (Fundamenta
l Theorem of Markov Chains)
\begin_inset Note Note
status open

\begin_layout Plain Layout
(see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Fundamental-Theorem-of-Markov-Chains"

\end_inset

)
\end_layout

\end_inset

.
 However there is currently no known analytic method to determine when the
 Markov chain has reached its stationary distribution, which leads to the
 following two strategies, 
\emph on
burn-in
\emph default
 and 
\emph on
thinning
\emph default
.
\end_layout

\begin_layout Paragraph
Burn-in Period
\end_layout

\begin_layout Standard
The starting value of the random variable might be far from the 
\begin_inset Quotes eld
\end_inset

center
\begin_inset Quotes erd
\end_inset

 of the distribution.
 But after some number of throw-away iterations, the Gibbs Sampler's values
 
\begin_inset Formula $\mathbf{x}=(x_{1},\dots,x_{n})$
\end_inset

 will start coming from the target joint distribution 
\begin_inset Formula $P(X_{1},\dots,X_{n})$
\end_inset

.
 This 
\begin_inset Quotes eld
\end_inset

some number of throw-away iterations
\begin_inset Quotes erd
\end_inset

 is called the 
\emph on
burn-in period
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
burn-in period
\end_layout

\end_inset

 and can be considerable depending on the starting values and the joint
 probability distribution underlying the conditional probability distributions.
 If one knows where the 
\begin_inset Quotes eld
\end_inset

center
\begin_inset Quotes erd
\end_inset

 of the equilibrium distribution is then one should use a value near that
 center as the starting point, however, in many cases such things are not
 known (and may be the goal of Gibbs Sampling in the first place).
\end_layout

\begin_layout Standard
There is no known analytic method to determine when a chain is burned-in.
 Several convergence diagnostics methods have been proposed, see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "CowlesCarlin1996"

\end_inset

 for a review.
\end_layout

\begin_layout Paragraph
Thinning
\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
thinning
\end_layout

\end_inset

Even after the Markov chain is burned in, there is still a problem with
 the returned samples, which prevent them from being used in those applications
 needing 
\emph on
independent 
\emph default
samples.
 The samples of two adjacent time steps 
\begin_inset Formula $\mathbf{x}^{(t)}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}^{(t+1)}$
\end_inset

 are correlated however, because the latter is dependent on the former (by
 definition).
\end_layout

\begin_layout Standard
This can be mitigated by returning only the states of every 
\begin_inset Formula $n$
\end_inset

th iteration, where 
\begin_inset Formula $n$
\end_inset

 is a sufficiently large number.
 This is called 
\begin_inset Quotes eld
\end_inset

thinning
\begin_inset Quotes erd
\end_inset

 of the Gibbs sampler.
\end_layout

\begin_layout Standard
Again, there is currently no straightforward analytic way to determine what
 a sufficiently large 
\begin_inset Formula $n$
\end_inset

 is for the adjacent 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to be regarded independent.
 In practice one resorts to heuristics like autocorrelation.
\end_layout

\begin_layout Subsubsection
Gibbs Sampling in Markov Random Fields and Bayesian Networks 
\begin_inset CommandInset label
LatexCommand label
name "sub:Inference-in-Markov-Random-Fields"

\end_inset


\end_layout

\begin_layout Standard
We can now define the algorithms for approximate inference using Gibbs Sampling
 in Markov Random Fields and Bayesian Networks.
\end_layout

\begin_layout Standard
In Markov Random Fields, we use the local Markov property
\begin_inset Note Note
status open

\begin_layout Plain Layout
defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "local-Markov-property"

\end_inset


\end_layout

\end_inset

: each random variable 
\begin_inset Formula $X_{i}$
\end_inset

 is independent of all other random variables given the states of the neighborin
g random variables.
 Thus, in Markov Random Fields, the conditional probability (see equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Gibbs sampling: CPD"

\end_inset

) is 
\begin_inset Formula 
\[
P(X_{i}\mid X_{1},\dots,X_{i-1},X_{i+1},\dots,X_{n})=P(X_{i}\mid\mathbf{X}_{Neighborhood(X_{i})}).
\]

\end_inset

In Bayesian Networks the conditional probability is 
\begin_inset Formula 
\[
P(X_{i}\mid X_{1},\dots,X_{i-1},X_{i+1},\dots,X_{n})=P(X_{i}\mid\mathbf{X}_{MarkovBlanket(X_{i})}),
\]

\end_inset

where 
\begin_inset Formula $\mathbf{X}_{MarkovBlanket(X_{i})}$
\end_inset

 are the random variables in the Markov Blanket of 
\begin_inset Formula $X_{i}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Gibbs Sampling in Markov Random Fields
\begin_inset CommandInset label
LatexCommand label
name "par:Gibbs-Sampling-in-Markov-Random-Fields"

\end_inset


\end_layout

\begin_layout Standard
Exact inference in Markov Random Fields can be done by conditioning on the
 known random variables and marginalizing out the uninteresting variables
 (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Exact-Inference-in-Directed-and-Undirected Graphical Models"

\end_inset

).
 The exponential runtime of exact inference can be circumvented by approximate
 methods like Gibbs Sampling.
\end_layout

\begin_layout Standard
For example, we might want to infer the probability of a configuration of
 variables when the values of only some of the variables are known.
 We can partition the variables 
\begin_inset Formula $\mathbf{X}:=\mathbf{U}\cup\mathbf{K}\cup\mathbf{W}$
\end_inset

 of a graphical model into three disjoint groups:
\end_layout

\begin_layout Enumerate
the variables 
\begin_inset Formula $\mathbf{K}$
\end_inset

 whose states are known (for example because they were measured)
\end_layout

\begin_layout Enumerate
the unknown variables 
\begin_inset Formula $\mathbf{W}$
\end_inset

 that we want to know the probability distribution of,
\end_layout

\begin_layout Enumerate
the unknown variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

 that we do not care about.
\end_layout

\begin_layout Standard
Gibbs Sampling in Markov Random Fields uses a converging Markov chain to
 sample from the target joint probability distribution.
 A prerequisite is that the conditional probability distributions are known
 in closed form.
\end_layout

\begin_layout Enumerate
Initialize the state 
\begin_inset Formula $x_{i}^{(1)}$
\end_inset

 of the random variable 
\begin_inset Formula $X_{i}\in\mathbf{W}\cup\mathbf{U}$
\end_inset

 with an arbitrary state.
\end_layout

\begin_layout Enumerate
Initialize the state 
\begin_inset Formula $x_{i}^{(1)}$
\end_inset

 of the known random variables 
\begin_inset Formula $X_{i}\in\mathbf{K}$
\end_inset

 with their known state.
\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:For-each-time"

\end_inset

For each time point 
\begin_inset Formula $t\in\{1,2,\dots\}$
\end_inset

 do
\end_layout

\begin_deeper
\begin_layout Enumerate
Keep the known variables fixed (i.e.
 
\begin_inset Formula $x_{i}^{(t+1)}:=x_{i}^{(t)}$
\end_inset

 for all 
\begin_inset Formula $X_{i}\in\mathbf{K}$
\end_inset

).
\end_layout

\begin_layout Enumerate
For each random variable 
\begin_inset Formula $X_{i}\in\mathbf{W}\cup\mathbf{U}$
\end_inset

 do
\end_layout

\begin_deeper
\begin_layout Enumerate
Given the states of all variables 
\begin_inset Formula $\mathbf{X}\backslash X_{i}$
\end_inset

, sample a new 
\begin_inset Formula $X_{i}$
\end_inset

 from its conditional distribution 
\begin_inset Formula $P(X_{i}=x_{i}^{(t+1)}\mid X_{1}=x_{1}^{(t)},\dots,X_{i-1}=x_{i-1}^{(t)},X_{i+1}=x_{i+1}^{(t)},\dots,X_{n}=x_{n}^{(t)})$
\end_inset

.
 The Hammersley-Clifford theorem 
\begin_inset Note Note
status open

\begin_layout Plain Layout
(section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Hammersley-Clifford-theorem"

\end_inset

)
\end_layout

\end_inset

 states that this conditional probability is equal to 
\begin_inset Formula $P(X_{i}=x_{i}^{(t+1)}\mid\mathbf{X}_{Neighborhood(X_{i})})$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Repeat step 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:For-each-time"

\end_inset

 until the Markov chain converges.
\end_layout

\begin_layout Enumerate
Discard the states of the uninteresting random variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Return the states of the interesting random variables 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Gibbs Sampling in Bayesian Networks
\end_layout

\begin_layout Standard
We use the same algorithmic structure as for Markov Random Fields above
\begin_inset Note Note
status open

\begin_layout Plain Layout
paragraph 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Gibbs-Sampling-in-Markov-Random-Fields"

\end_inset


\end_layout

\end_inset

, but sample from a different conditional probability distribution 
\begin_inset Formula $P(X_{i}=x_{i}\mid\mathbf{X_{j}}=\mathbf{x_{j}}:j\neq i)$
\end_inset

 when updating the state of 
\begin_inset Formula $X_{i}$
\end_inset

 in step 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:For-each-time"

\end_inset

(b).
\end_layout

\begin_layout Standard
The conditional probability of 
\begin_inset Formula $X_{i}$
\end_inset

 given all other nodes is equal to its conditional probability given the
 values of the nodes in 
\begin_inset Formula $X_{i}$
\end_inset

's Markov Blanket 
\begin_inset Formula $X_{MarkovBlanket(X_{i})}$
\end_inset


\begin_inset Formula 
\[
P(X_{i}=x_{i}\mid\mathbf{X_{j}}=\mathbf{x_{j}}:j\neq i)=P(X_{i}=x_{i}\mid\mathbf{X}_{MarkovBlanket(X_{i})}=\mathbf{x}_{MarkovBlanket(X_{i})}),
\]

\end_inset

where 
\begin_inset Formula $\mathbf{X}_{MarkovBlanket(X_{i})}$
\end_inset

 is the set of 
\begin_inset Formula $X_{i}$
\end_inset

's parents and children, and its children's parents.
 Formally, and parallel to section 4.1 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

, the conditional probability distribution 
\begin_inset Formula $P(X_{i}=x_{i}\mid\mathbf{X}_{MarkovBlanket(X_{i})})$
\end_inset

 is equal to
\begin_inset Formula 
\begin{eqnarray*}
P(x_{i}\mid\{x_{i}:i\neq k\}) & = & P(x_{i}\mid\mathbf{x}_{MarkovBlanket(X_{i})})\\
 & = & \frac{P(x_{i}\mid\mathbf{x}_{Parent(i)})\prod_{j\in Child(i)}P(x_{j}\mid x_{i},\mathbf{x}_{Parent(j)\backslash i})}{\sum_{\tilde{x}_{i}}P(\tilde{x}_{i}\mid\mathbf{x}_{Parent(i)})\prod_{j\in Child(i)}P(x_{j}\mid\tilde{x}_{i},\mathbf{x}_{Parent(j)\backslash i})}.
\end{eqnarray*}

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
rewrote the algorithm from 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

: 
\begin_inset Quotes eld
\end_inset

We start the Gibbs Sampling procedure by fixing the observed variables to
 their known values, and setting the unobserved variables arbitrarily.
 We then repeatedly visit each unobserved variable in turn, each time randomly
 selecting a new value for the variable from its conditional distribution
 given the current values of the other variables.
 From the joint distribution of equation (2.15), we see that the conditional
 distribution for X k is as follows:
\begin_inset Quotes erd
\end_inset

 Diese Formel wurde hier eingefügt.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Artificial Neural Networks
\end_layout

\begin_layout Standard
Here we will introduce artificial neural networks that have been developed
 as models of biological neural networks since in the middle of the last
 century.
 The artificial neural networks introduced here are related to Deep Belief
 Networks, namely Hopfield networks, Multilayer Perceptrons, and (Restricted)
 Boltzmann Machines.
\end_layout

\begin_layout Paragraph
Distinction Between a Deterministic and Stochastic Network
\end_layout

\begin_layout Standard
In a deterministic network
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
deterministic network
\end_layout

\end_inset

 a node represents a deterministic value.
 In contrast, in a stochastic network
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
stochastic network
\end_layout

\end_inset

 a node represents a probability distribution.
 If there is a set of 
\begin_inset Quotes eld
\end_inset

output
\begin_inset Quotes erd
\end_inset

 nodes, then in a deterministic setting the output can be interpreted as
 a point in a high-dimensional space, while in a stochastic network the
 output is the joint probability distribution over all the random variables
 associated with the output nodes
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Rainer meint hier, ich sollte 
\begin_inset Quotes eld
\end_inset

associated with
\begin_inset Quotes erd
\end_inset

 genauer beschreiben, aber ich weiß nicht, was man da genauer beschreiben
 sollte.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
A stochastic network is more general than its deterministic counterpart,
 since a stochastic network can be converted to a deterministic network,
 but not vice-versa.
 This comes at a higher cost.
 Inference and learning in stochastic networks take longer than in deterministic
 networks.
\end_layout

\begin_layout Paragraph
Distinction Between Feed-forward and Recurrent Networks
\end_layout

\begin_layout Standard
A 
\emph on
feed-forward network
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
feed-forward neural network
\end_layout

\end_inset


\emph on
 
\emph default
is a network defined on a directed acyclic graph.
 In contrast, the connections of a 
\emph on
recurrent network
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
recurrent neural network
\end_layout

\end_inset


\emph on
 
\emph default
may form cycles.
\end_layout

\begin_layout Subsection
Hopfield Networks
\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
A 
\emph on
Hopfield Network
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Hopfield Network
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

 is a deterministic recurrent network with 
\begin_inset Formula $m$
\end_inset

 nodes
\begin_inset Note Note
status open

\begin_layout Plain Layout
As this is not a stochastic network, I don't think it makes sense to define
 random variables 
\begin_inset Formula $\mathbf{N}=(N_{1},\dots,N_{m})$
\end_inset

.
\end_layout

\end_inset

, each having a binary state 
\begin_inset Formula $n_{i}\in\{0,1\}$
\end_inset

 for all nodes 
\begin_inset Formula $i$
\end_inset

.
 Every node is connected with all others but not with itself.
 The connection from node 
\begin_inset Formula $N_{i}$
\end_inset

 to node 
\begin_inset Formula $N_{j}$
\end_inset

 is directed and has a weight 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

.
 There is no connection from node 
\begin_inset Formula $i$
\end_inset

 to node 
\begin_inset Formula $i$
\end_inset

, and therefore 
\begin_inset Formula $w_{ii}=0$
\end_inset

 for all nodes 
\begin_inset Formula $i$
\end_inset

.
 Hence, each node has 
\begin_inset Formula $m-1$
\end_inset

 outgoing connections and 
\begin_inset Formula $m-1$
\end_inset

 incoming connections.
 There is also a real-valued bias 
\begin_inset Formula $b_{i}\in\mathbb{R}$
\end_inset

 for each node 
\begin_inset Formula $i$
\end_inset

 that acts as a weight of a connection from a 
\begin_inset Quotes eld
\end_inset

virtual
\begin_inset Quotes erd
\end_inset

 node that always has state 1.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-a-Hopfield-Network"

\end_inset

 shows an example of the structure of a Hopfield Network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/hopfield-network-example.dia
	width 30col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-a-Hopfield-Network"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Example of a Hopfield Network.
\end_layout

\end_inset

Example of a Hopfield Network.
 The circles are the nodes; the arrows are the (directed) connections.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Associative Memory
\end_layout

\begin_layout Standard
Hopfield networks can be used as 
\emph on
associative 
\emph default
or
\emph on
 content-addressable memory
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
associative memory
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
content-addressable memory
\end_layout

\end_inset

, where the memory is a binary number.
 Bit 
\begin_inset Formula $i$
\end_inset

 of the memory is stored in node 
\begin_inset Formula $i$
\end_inset

 of the network.
 Associative or content-addressable memory means that the network can be
 initialized with a partially distorted memory and the network can recall
 a previously learned memory that is close to the initialized memory.
 Recalling a partially known memory is done by repeatedly updating the network.
\end_layout

\begin_layout Paragraph
Updating Rule
\end_layout

\begin_layout Standard
The network is updated asynchronously: At each time point 
\begin_inset Formula $t$
\end_inset

, a node 
\begin_inset Formula $i$
\end_inset

 is chosen at random out of the 
\begin_inset Formula $m$
\end_inset

 possible nodes and it is updated, while all other nodes remain constant.
 The state 
\begin_inset Formula $n_{i}$
\end_inset

 of node 
\begin_inset Formula $i$
\end_inset

 at time point 
\begin_inset Formula $t$
\end_inset

 is denoted 
\begin_inset Formula $n_{i}^{(t)}$
\end_inset

 and depends on the state of all other nodes at time step 
\begin_inset Formula $t-1$
\end_inset

, the weights 
\begin_inset Formula $w_{ji}$
\end_inset

 from node 
\begin_inset Formula $j$
\end_inset

 to node 
\begin_inset Formula $i$
\end_inset

, and the bias 
\begin_inset Formula $b_{i}$
\end_inset

: 
\begin_inset Formula 
\[
n_{i}^{(t)}=f\left(\sum_{j\neq i}n_{j}^{(t-1)}w_{ji}+b_{i}\right),
\]

\end_inset

where the 
\emph on
activation function
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
activation function
\end_layout

\end_inset

 
\begin_inset Formula $f$
\end_inset

 is a step function that maps nonpositive values to 0, and positive values
 to 1: 
\begin_inset Formula 
\[
f(x)=\begin{cases}
0 & \mbox{for }x\leq0\\
1 & \mbox{for }x>0
\end{cases}.
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
In 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1982"

\end_inset

, 
\begin_inset Formula $f$
\end_inset

 ist defined implicitly in equation [1] (or equation [2] in 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

).
\end_layout

\begin_layout Plain Layout
Note that I have added 
\begin_inset Formula $b_{j}$
\end_inset

 here (to be consistent with the rest of this text), while in 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

 the bias is on the right-hand side in eq.
 2: 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $<U_{i}$
\end_inset


\begin_inset Quotes erd
\end_inset

.
 To make up for this sign change, I also swap signs in the energy (eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Energy of a Hopfield network"

\end_inset

) below.
\end_layout

\end_inset

(In 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

 there is also an external input 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Der Input, der in jedem Zeitschritt auf den Input addiert wird, heißt in
 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

 
\begin_inset Formula $I_{i}$
\end_inset

.
\end_layout

\end_inset

to each node, constant over all times 
\begin_inset Formula $t$
\end_inset

.
 Because the bias also does not depend on 
\begin_inset Formula $t$
\end_inset

, both are combined into 
\begin_inset Formula $b_{i}$
\end_inset

 here.)
\begin_inset Newline newline
\end_inset

As described here, the updating rule is asynchronous (i.e.
 at each time step a node is picked at random and its state is updated,
 which is how Hopfield described it 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
page 1: 
\begin_inset Quotes eld
\end_inset

Each neuron samples its input at random times.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

).
 Updating the network synchronously (i.e.
 all nodes are updated at the same time) is also possible
\begin_inset Note Note
status open

\begin_layout Plain Layout
 as this is a special case of asynchronous updating
\end_layout

\end_inset

.
\end_layout

\begin_layout Paragraph
Energy of a Hopfield Network
\end_layout

\begin_layout Standard
The 
\emph on
energy 
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
energy
\end_layout

\end_inset

 is associated with the state of the network at time point 
\begin_inset Formula $t$
\end_inset

 and is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E^{(t)}=-\frac{1}{2}\sum_{i}\sum_{j\neq i}w_{ij}n_{i}^{(t)}n_{j}^{(t)}-\sum_{i}b_{i}n_{i}^{(t)}.\label{eq:Energy of a Hopfield network}
\end{equation}

\end_inset

Hopfield showed that when applying the updating rule repeatedly, the energy
 converges to a (possibly local) minimum, provided that the weights are
 symmetric (i.e.
 
\begin_inset Formula $w_{ij}=w_{ji}$
\end_inset

) and there are no single-node loops (i.e.
 
\begin_inset Formula $w_{ii}=0$
\end_inset

)
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Aus 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

: 
\begin_inset Quotes eld
\end_inset

There is a simple mathematical condition which guarantees that the state
 space flow algorithm converges on stable states.
 Any symmetric T with zero diagonal elements (i.e., 
\begin_inset Formula $T_{ij}$
\end_inset

 = 
\begin_inset Formula $T_{ji}$
\end_inset

, 
\begin_inset Formula $T_{ii}$
\end_inset

 = 0) will produce such a flow.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
 In more detail, each update of a node either doesn't change the energy
 
\begin_inset Formula $E$
\end_inset

 or decreases it.
 As time progresses 
\begin_inset Formula $E$
\end_inset

 becomes smaller and smaller, i.e.
 
\begin_inset Formula $E^{(t)}\leq E^{(t-1)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Although a Hopfield network is recurrent, 
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Recalling a Training Pattern by the Updating Rule
\end_layout

\begin_layout Standard
Training a Hopfield network is the task of finding weights 
\begin_inset Formula $w_{ij}$
\end_inset

 and biases 
\begin_inset Formula $b_{i}$
\end_inset

, so that training patterns (i.e.
 memories to be learned) have a low energy and all other states have a high
 energy.
 After training, a Hopfield network can be initialized with a distorted
 pattern, in which the states of some nodes are inverted.
 After iteratively updating the network until its state doesn't change anymore,
 the stationary state will be similar to a training pattern.
 In a demonstration of 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1982"

\end_inset

, approximately 85% of the trials ended in training patterns, 5% resulted
 in stable states near training patterns, and 10% ended in stable states
 of no obvious meaning.
\end_layout

\begin_layout Subsection
Multilayer Perceptrons
\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
A Multilayer Perceptron belongs to the class of deterministic feed-forward
 neural networks.
 The neurons are arranged in layers, with the value of nodes in a layer
 only depending on the values of nodes in the layer above.
 Example structures of multilayer perceptrons were given in figure 
\begin_inset CommandInset ref
LatexCommand vref
reference "fig:sigmoid-function"

\end_inset

 and figure 
\begin_inset CommandInset ref
LatexCommand vref
reference "fig:Data-flow-in-back-propagation"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Multilayer Feed-forward Networks as Universal Function Approximators
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HornikWhite1989"

\end_inset

 found that artificial feed-forward neural networks with as few as one hidden
 layer can model any Borel measurable function within a given error, provided
 the following conditions are met:
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Itemize
The activation function must be a 
\begin_inset Quotes eld
\end_inset

squashing
\begin_inset Quotes erd
\end_inset

 function: A squashing function 
\begin_inset Formula $s(x)$
\end_inset

 must be non-decreasing, 
\begin_inset Formula $\lim_{x\rightarrow-\infty}s(x)=0$
\end_inset

 and 
\begin_inset Formula $\lim_{x\rightarrow\infty}s(x)=1$
\end_inset

.
 An example is the sigmoid function 
\begin_inset Formula $\frac{1}{1+exp(-x)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Sufficiently many hidden nodes must be available.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HornikWhite1989"

\end_inset

 also note that 
\begin_inset Quotes eld
\end_inset

This [result] implies that any lack of success in applications must arise
 from inadequate learning, insufficient numbers of hidden units[nodes] or
 the lack of a deterministic relationship between input and target.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsubsection
Training Using Back-propagation
\end_layout

\begin_layout Standard

\emph on
Back-propagation
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
back-propagation
\end_layout

\end_inset

 is the adaptation of weights and biases of the network to make its set
 of actual outputs better fit a set of desired outputs for a given set of
 inputs.
 Technically it is just running the network for a given input in the forward
 pass
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
forward pass
\end_layout

\end_inset

, observing the outputs in the output layer, computing the errors to the
 desired outputs and back-propagating them to adapt the weights and biases
 between all the layers.
 This will make the network output values closer to the desired values next
 time this particular input pattern is given to the network.
 The back-propagation algorithm is a supervised learning step and thus prone
 to overfitting.
 In order to discuss modifications and extensions of the algorithm, we will
 first repeat the most important points of back-propagation as reviewed
 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Back-propagation"

\end_inset

.
\end_layout

\begin_layout Paragraph
Forward and Backward Pass
\end_layout

\begin_layout Standard
In the 
\emph on
forward pass
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
forward pass
\end_layout

\end_inset

, each node's output is computed from the sum of its inputs
\begin_inset Formula 
\begin{eqnarray*}
v_{j} & = & b_{j}+\sum_{i\in\mathbf{c_{j}}}o_{i}w_{ji},
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $b_{j}$
\end_inset

 is the bias, 
\begin_inset Formula $o_{i}$
\end_inset

 is the output of a node in the layer above, and 
\begin_inset Formula $w_{ji}$
\end_inset

 is the weight of the connection from node 
\begin_inset Formula $i$
\end_inset

 to node 
\begin_inset Formula $j$
\end_inset

.
 The input 
\begin_inset Formula $v_{j}$
\end_inset

 is then scaled by the sigmoid function to produce a node's output 
\begin_inset Formula $o_{j}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
o_{j}=\sigma(v_{j})=\frac{1}{1+\exp(-v_{j})}.
\]

\end_inset


\end_layout

\begin_layout Standard
In the 
\emph on
backward pass
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
backward pass
\end_layout

\end_inset

, the training procedure computes the total error 
\begin_inset Formula $E$
\end_inset

 of the network, which is defined as the squared sum of differences between
 actual output 
\begin_inset Formula $o_{k}$
\end_inset

 and desired output 
\begin_inset Formula $y_{k}$
\end_inset


\begin_inset Formula 
\[
E=\frac{1}{2}\sum_{k}(o_{k}-y_{k})^{2},
\]

\end_inset

 where 
\begin_inset Formula $o_{k}$
\end_inset

 is the actual activation of node 
\begin_inset Formula $k$
\end_inset

 in the output layer, and 
\begin_inset Formula $y_{k}$
\end_inset

 is its the desired output.
 The sum-of-squared-differences term 
\begin_inset Formula $\frac{1}{2}\sum_{k}(o_{k}-y_{k})^{2}$
\end_inset

 is called the 
\emph on
error
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
error function
\end_layout

\end_inset

,
\emph on
 loss 
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
loss function
\end_layout

\end_inset

, or 
\emph on
cost
\emph default
 function
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
cost function
\end_layout

\end_inset

.
\end_layout

\begin_layout Paragraph
Error of the Output Layer
\end_layout

\begin_layout Standard
The error is then differentiated with respect to a weight 
\begin_inset Formula $w_{kj}$
\end_inset

 for a connection from node  
\begin_inset Formula $j$
\end_inset

 in the last hidden layer to node 
\begin_inset Formula $k$
\end_inset

 in the output layer 
\begin_inset Formula 
\begin{equation}
\frac{\partial E}{\partial w_{kj}}=\frac{\partial E}{\partial o_{k}}\cdot\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial w_{kj}}=(o_{k}-y_{k})\cdot o_{k}(1-o_{k})\cdot o_{j},\label{eq:backpropagation-error-wrt-weight}
\end{equation}

\end_inset

 and with respect to 
\begin_inset Formula $b_{k}$
\end_inset


\begin_inset Formula 
\[
\frac{\partial E}{\partial b_{k}}=\frac{\partial E}{\partial o_{k}}\cdot\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial b_{k}}=(o_{k}-y_{k})\cdot o_{k}(1-o_{k})\cdot1.
\]

\end_inset


\end_layout

\begin_layout Paragraph
Error of the Other Layers
\end_layout

\begin_layout Standard
The derivative of the error with respect to the weights 
\begin_inset Formula $w_{ji}$
\end_inset

 of the remaining connections from node 
\begin_inset Formula $i$
\end_inset

 in a layer to node 
\begin_inset Formula $j$
\end_inset

 in the layer below is
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E}{\partial w_{ji}} & = & \frac{\partial E}{\partial o_{j}}\cdot\frac{\partial o_{j}}{\partial v_{i}}\cdot\frac{\partial v_{i}}{\partial w_{ji}}\\
 & = & \frac{\partial E}{\partial o_{j}}\cdot o_{j}(1-o_{j})\cdot o_{i},
\end{eqnarray*}

\end_inset

 where
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E}{\partial o_{j}} & = & \sum_{k}\frac{\partial E}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}w_{kj}
\end{eqnarray*}

\end_inset

and we take the value for 
\begin_inset Formula $\frac{\partial E}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}$
\end_inset

 from node 
\begin_inset Formula $k$
\end_inset

, which is in the layer below node 
\begin_inset Formula $j$
\end_inset

.
 Analogously, the derivative with respect to 
\begin_inset Formula $b_{j}$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E}{\partial b_{j}} & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}w_{kj}\cdot o_{j}(1-o_{j})\cdot1.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Paragraph
Updating Rule and Learning Rate
\end_layout

\begin_layout Standard
After computing the derivatives of the error with respect to the parameters
 of the network, we can perform gradient descent and update the parameters
 using the learning rate
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
learning rate
\end_layout

\end_inset

 
\begin_inset Formula $\epsilon$
\end_inset

, a small positive number:
\begin_inset Formula 
\begin{eqnarray}
\Delta w & = & -\epsilon\frac{\partial E}{\partial w}\label{eq:backpropagation-deltas}\\
\Delta b & = & -\epsilon\frac{\partial E}{\partial b}.\nonumber 
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Paragraph
Optimizing the Sum of Squared Differences Error
\begin_inset CommandInset label
LatexCommand label
name "par:Optimizing-the-Sum-of-Squared-DIfferences"

\end_inset


\end_layout

\begin_layout Standard
Usually in back-propagation, the error function to be minimized is the 
\emph on
sum of squared differences
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sum of squared errors
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
squared-error sum
\end_layout

\end_inset

 between the desired outputs and the actual outputs
\begin_inset Formula 
\[
E=\frac{1}{2}\sum_{k}(o_{k}-y_{k})^{2},
\]

\end_inset

 where 
\begin_inset Formula $y_{k}$
\end_inset

 is the desired value of node 
\begin_inset Formula $k$
\end_inset

 in the output layer and 
\begin_inset Formula $o_{k}$
\end_inset

 is the actual output value of node 
\begin_inset Formula $i$
\end_inset

.
 As stated in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-error-wrt-weight"

\end_inset

, the derivative of the error 
\begin_inset Formula $E$
\end_inset

 with respect to a weight 
\begin_inset Formula $w_{kj}$
\end_inset

 is
\begin_inset Formula 
\begin{equation}
\frac{\partial E}{\partial w_{kj}}=(o_{k}-y_{k})\cdot o_{k}(1-o_{k})\cdot o_{j}.\label{eq:backpropagation-error-wrt-weight-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Optimizing the Cross-entropy Error
\begin_inset CommandInset label
LatexCommand label
name "par:Optimizing-the-Cross-entropy-error"

\end_inset


\end_layout

\begin_layout Standard
Another error function is the 
\emph on
cross-entropy error
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
cross-entropy error
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "NasrJoun2002"

\end_inset

 
\begin_inset Formula 
\[
E=-\sum_{k}y_{k}\log o_{k}-\sum_{k}(1-y_{k})\log(1-o_{k}),
\]

\end_inset

 where again 
\begin_inset Formula $y_{k}$
\end_inset

 is the desired output value of node 
\begin_inset Formula $k$
\end_inset

 in the output layer and 
\begin_inset Formula $o_{k}$
\end_inset

 is the actual output value.
 The derivative of error 
\begin_inset Formula $E$
\end_inset

 with respect to a weight 
\begin_inset Formula $w_{kj}$
\end_inset

 from node 
\begin_inset Formula $j$
\end_inset

 in the last hidden layer to node 
\begin_inset Formula $k$
\end_inset

 in the output layer is then
\begin_inset Formula 
\[
\frac{\partial E}{\partial w_{kj}}=(o_{k}-y_{k})o_{j}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "GolikNey2013"

\end_inset

 note that using the cross-entropy error function requires less updates,
 since the gradient for the sum-of-squared-differences error function becomes
 low not only when the actual output 
\begin_inset Formula $o_{k}$
\end_inset

 is near the desired output 
\begin_inset Formula $y_{k}$
\end_inset

, but also when 
\begin_inset Formula $o_{k}$
\end_inset

 is near 0 or 1 (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-error-wrt-weight-1"

\end_inset

).
\end_layout

\begin_layout Subsubsection
Parameters in Training a Neural Network
\begin_inset CommandInset label
LatexCommand label
name "sub:Parameters-of-Training-a-Multilayer-Perceptron"

\end_inset


\end_layout

\begin_layout Standard
Although described here for multilayer perceptrons, the parameters apply
 to most artificial neural networks, not just multilayer perceptrons.
\end_layout

\begin_layout Paragraph
Random Weight and Bias Initialization
\end_layout

\begin_layout Standard
At the start of training, weights 
\begin_inset Formula $w$
\end_inset

 and biases 
\begin_inset Formula $b$
\end_inset

 have to be initialized.
 They must not all be initialized to the same value, because then the activation
s in the output layer 
\begin_inset Formula $o_{k}$
\end_inset

 would become equal, leading to an equal error gradient for the weights
 and biases, which would prevent learning.
 Ideally, the hidden and output layer activations 
\begin_inset Formula $o_{j}$
\end_inset

 should be in the linear region of the activation function, so that the
 error derivatives are large.
 As 
\begin_inset CommandInset citation
LatexCommand cite
key "LeCunMuller1998"

\end_inset

 note, this requires coordinating the training set  normalization, the choice
 of the activation function, and the weight and bias initialization.
\end_layout

\begin_layout Standard
Usually the biases are initialized to zero, and the weights are drawn from
 a uniform random distribution in [-1;1], or from a normal distribution
 with mean 0 and standard deviation 1.
 Another possibility is to use 
\begin_inset Quotes eld
\end_inset

fan-in
\begin_inset Quotes erd
\end_inset

 initialization, where the number of incoming connections 
\begin_inset Formula $m$
\end_inset

 to a node are taken into account.
 Then the weights are randomly drawn from a normal distribution with mean
 0 and standard deviation
\begin_inset Formula 
\[
\sigma=m^{-1/2}.
\]

\end_inset


\end_layout

\begin_layout Paragraph
Activation Function
\begin_inset CommandInset label
LatexCommand label
name "The-sigmoid-activation-function"

\end_inset


\end_layout

\begin_layout Standard
The activation of hidden and visible nodes are a function of the sum of
 their inputs.
 The function that maps the sum of the inputs of a node to its value is
 called the 
\emph on
activation function
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
activation function
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
The sigmoid activation function
\begin_inset Formula 
\[
\sigma(x)=\frac{1}{1+e^{-x}}
\]

\end_inset

 is a standard activation function, often used in neural networks.
 It is almost linear for inputs around zero, tends to 1 as its inputs go
 to positive infinity and to 0 as inputs go to negative infinity (see figure
 
\begin_inset CommandInset ref
LatexCommand vref
reference "fig:sigmoid-function"

\end_inset

).
\end_layout

\begin_layout Standard
Another commonly used activation function is the hyperbolic tangent function
 
\begin_inset Formula 
\[
tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}.
\]

\end_inset

Its graph looks very similar to the graph of the sigmoid function.
 While the output range of the sigmoid is 
\begin_inset Formula $[0;1]$
\end_inset

, it is 
\begin_inset Formula $[-1;1]$
\end_inset

 for the hyperbolic tangent function.
\end_layout

\begin_layout Standard
In this work only the sigmoid activation function was used.
\end_layout

\begin_layout Paragraph
Momentum of the Learning Rule
\begin_inset CommandInset label
LatexCommand label
name "par:Momentum-of-the-learning-rule"

\end_inset


\end_layout

\begin_layout Standard
Usually, the learning rule includes a 
\emph on
momentum
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
momentum
\end_layout

\end_inset

 term.
 In this case, the weight and bias deltas from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-deltas"

\end_inset

 are replaced with a momentum weight delta 
\begin_inset Formula $\Delta w_{momentum}$
\end_inset

 and momentum bias delta 
\begin_inset Formula $\Delta b_{momentum}$
\end_inset

.
 The momentum term 
\begin_inset Formula 
\begin{eqnarray*}
\Delta w_{momentum}^{(t)} & = & \mu\Delta w_{momentum}^{(t-1)}+(1-\mu)\Delta w^{(t)}\\
\Delta b_{momentum}^{(t)} & = & \mu\Delta b_{momentum}^{(t-1)}+(1-\mu)\Delta b^{(t)},
\end{eqnarray*}

\end_inset

 includes a coefficient 
\begin_inset Formula $\mu$
\end_inset

 that is the fraction of the weight and bias deltas of the previous time
 step 
\begin_inset Formula $t-1$
\end_inset

 to be added to the current weight deltaswhere 
\begin_inset Formula $\Delta w^{(t)}$
\end_inset

 and 
\begin_inset Formula $\Delta b^{(t)}$
\end_inset

 are taken from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-deltas"

\end_inset

.
\end_layout

\begin_layout Standard
Momentum works like a low-pass filter and reduces oscillations during learning
 by smoothing the weight and bias deltas added to the parameters of the
 network.
 However, too large a momentum coefficient can cause 
\begin_inset Quotes eld
\end_inset

explosion
\begin_inset Quotes erd
\end_inset

, or non-convergence of the model during training.
 To prevent this, 
\begin_inset Formula $\mu$
\end_inset

 is usually gradually increased to its final value during the early steps
 of training.
\end_layout

\begin_layout Standard
The coefficient 
\begin_inset Formula $\mu$
\end_inset

 is an additional meta-parameter in training.
\end_layout

\begin_layout Subsubsection
Difficulties in Training Multi-layer Neural Networks
\end_layout

\begin_layout Standard
Training a randomly initialized feed-forward neural network with more than
 one hidden layer using back-propagation is difficult and usually does not
 succeed.
 When attempting to train such a network, each node in the output layer
 often just outputs the mean value of the desired output of the training
 cases, independently of the input.
 One problem is that there are many local minima (generated by repeatedly
 adding weighted sigmoid functions) in the implicitly optimized energy function
 during back-propagation 
\begin_inset CommandInset citation
LatexCommand cite
key "GoriTesi1992"

\end_inset

.
 Another problem is that in discriminative learning, each training case
 only contributes as many bits to the specification of the network parameters
 as needed to specify the label 
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010"

\end_inset

.
\end_layout

\begin_layout Subsection
Regularizations of Neural Networks
\end_layout

\begin_layout Standard
Several regularization
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
regularization
\end_layout

\end_inset

 methods for neural networks have been developed over the years.
 They have in common that they artificially constrain the search space of
 weights and biases in order to let the model find better error minima or
 to prevent overfitting.
 The neural network should do less 
\begin_inset Quotes eld
\end_inset

learning by heart
\begin_inset Quotes erd
\end_inset

 and instead make its predictions apply to more unseen test set data.
\end_layout

\begin_layout Standard
The regularizations described here apply to most artificial neural networks,
 not just multilayer perceptrons.
\end_layout

\begin_layout Subsubsection
L1 and L2 Weight Decay
\begin_inset CommandInset label
LatexCommand label
name "sub:L1-and-L2-Weight-Decay"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
L1 and L2 weight decay are described here as implemented in 
\emph on
deepnet
\emph default
.
\end_layout

\end_inset

L1 and L2 weight decay
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
L1 weight decay
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
L2 weight decay
\end_layout

\end_inset

 penalize large weights by moving them towards zero.
 Both weight decay methods decrease the absolute value of each weight in
 each training iteration, in order to prevent large weights.
 This can be necessary because for some training samples, some weights tend
 to 
\begin_inset Quotes eld
\end_inset

escape
\begin_inset Quotes erd
\end_inset

, i.e.
 become larger and larger in absolute value, making subsequent changes to
 the weights more difficult.
\end_layout

\begin_layout Standard
Instead of the normal weight delta 
\begin_inset Formula $\Delta w$
\end_inset

 defined in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-deltas"

\end_inset

, L1 weight decay uses a penalized weight delta 
\begin_inset Formula 
\begin{eqnarray*}
\Delta w_{L1} & = & \Delta w-c*sgn(w),
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $c\in\mathbb{R}^{+}$
\end_inset

 is a small positive constant meta-parameter, the 
\begin_inset Quotes eld
\end_inset

weight-cost
\begin_inset Quotes erd
\end_inset

 of L1 weight decay, and 
\begin_inset Formula $sgn(w)$
\end_inset

 is the sign of the weight, i.e.
 -1, 0, 1, for the weight 
\begin_inset Formula $w$
\end_inset

 being negative, zero, positive, respectively.
\end_layout

\begin_layout Standard
L2 weight decay uses
\begin_inset Formula 
\[
\Delta w_{L2}=\Delta w-w*c,
\]

\end_inset

 where 
\begin_inset Formula $c\in\mathbb{R}^{+}$
\end_inset

 is the small positive 
\begin_inset Quotes eld
\end_inset

weight-cost
\begin_inset Quotes erd
\end_inset

 of L2 weight decay.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010"

\end_inset

 notes that there are four different reasons for using weight decay: better
 generalization of the resulting network, making the weights more interpretable
 by shrinking large weights, penalize network nodes that are always firmly
 on or off due to large inputs caused by large weights, and improve the
 mixing rate of contrastive divergence
\begin_inset Foot
status open

\begin_layout Plain Layout
Contrastive divergence is explained in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Training-Restricted-Boltzmann-Machines-using-Contrastive-Divergence"

\end_inset

.
\end_layout

\end_inset

, a training procedure for Restricted Boltzmann Machines, where small weights
 increase the mixing rate of the Gibbs chain.
\end_layout

\begin_layout Standard
As 
\begin_inset CommandInset citation
LatexCommand cite
key "FischerIgel2012"

\end_inset

 note, using an L2 weight decay term in the updating term corresponds to
 assuming a zero-mean Gaussian prior on the parameters in a Bayesian framework.
\end_layout

\begin_layout Subsubsection
Sparsity
\begin_inset CommandInset label
LatexCommand label
name "sub:Sparsity-Target"

\end_inset


\end_layout

\begin_layout Standard
Sparsity regularization is a method to make only a small fraction of hidden
 nodes output an activation very different from zero.
 Sparse activity helps in the network's ability to generalize, and also
 makes the trained network more interpretable 
\begin_inset CommandInset citation
LatexCommand cite
key "Ng2011,Hinton2010,NairHinton2009"

\end_inset

.
 Like other regularization methods, sparsity regularization constrains the
 space of possible parameters of the model.
\end_layout

\begin_layout Paragraph
Average Activation
\end_layout

\begin_layout Standard
We first have to define what we mean by sparse activity.
 We can define an 
\emph on
average activation
\emph default
 
\begin_inset Formula $q_{j}$
\end_inset

 of each hidden node 
\begin_inset Formula $j$
\end_inset

, and encourage the node to have an average activation 
\begin_inset Formula $q_{j}$
\end_inset

 close to a 
\emph on
sparsity target
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sparsity target 
\end_layout

\end_inset

 
\begin_inset Formula $0<p\ll1$
\end_inset

.
 We want to approximately enforce that 
\begin_inset Formula $q_{j}\approx p$
\end_inset

.
 
\end_layout

\begin_layout Standard
One way to define the average activation 
\begin_inset Formula $q_{j}$
\end_inset

 of node 
\begin_inset Formula $j$
\end_inset

 is to take into account the node's activations in the previous training
 iterations.
 The average activation 
\begin_inset Formula $q_{j}$
\end_inset

 can be defined to be an exponentially decaying average of the activation
 
\begin_inset Formula $o_{j}^{(t)}$
\end_inset


\begin_inset Formula 
\[
q_{j}^{(t)}=\lambda q_{j}^{(t-1)}+(1-\lambda)o_{j}^{(t)},
\]

\end_inset

 where 
\begin_inset Formula $\lambda\in(0;1)$
\end_inset

 is the 
\emph on
decay rate
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
decay rate (sparsity)
\end_layout

\end_inset

, 
\begin_inset Formula $o_{j}^{(t)}$
\end_inset

 is the activation of node 
\begin_inset Formula $j$
\end_inset

 at training iteration 
\begin_inset Formula $t$
\end_inset

, and 
\begin_inset Formula $q_{j}^{(t)}$
\end_inset

 is its average activation at training iteration t.
\end_layout

\begin_layout Standard
Alternatively, we can measure the average activation 
\begin_inset Formula $q_{j}$
\end_inset

 within one training iteration, by defining 
\begin_inset Formula $q_{j}$
\end_inset

 as the average activation over all 
\begin_inset Formula $m$
\end_inset

 training samples
\begin_inset Formula 
\[
q_{j}=\frac{1}{m}\sum_{s}^{m}o_{j}^{(s)},
\]

\end_inset

 where 
\begin_inset Formula $o_{j}^{(s)}$
\end_inset

 is the activation of hidden node 
\begin_inset Formula $j$
\end_inset

 when the input layer of the network has been set to training sample 
\begin_inset Formula $s$
\end_inset

.
\end_layout

\begin_layout Paragraph
Sparsity Error Term
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Derivation of sparsity.
\end_layout

\begin_layout Plain Layout
The 
\begin_inset Quotes eld
\end_inset

CS294A Lecture notes
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

~/uni/publication/zusammenfassung/neural network/sparsity/sparseAutoencoder.pdf
\begin_inset Quotes erd
\end_inset

 state that sparsity adds another additive term to the error 
\begin_inset Formula $E$
\end_inset

: (where 
\begin_inset Formula $p$
\end_inset

 is the desired average activation, and 
\begin_inset Formula $o_{j}$
\end_inset

 is the actual activation of a node)
\begin_inset Formula 
\begin{eqnarray*}
v_{j} & = & b_{j}+\sum_{i\in\mathbf{c_{j}}}o_{i}w_{ji}\\
o_{j} & = & \frac{1}{1+\exp(-v_{j})}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\begin{eqnarray*}
E_{sparsity} & = & \sum_{j}p\log\frac{p}{o_{j}}+(1-p)\log\frac{1-p}{1-o_{j}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Plain Layout
we have to find the derivative of the sparsity error with respect to a weight
 from a node 
\begin_inset Formula $i$
\end_inset

 to a node 
\begin_inset Formula $j$
\end_inset

 in the layer below:
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E_{sparsity}}{\partial w_{ji}} & = & \frac{\partial E_{sparsity}}{\partial o_{j}}\cdot\frac{\partial o_{j}}{\partial v_{j}}\cdot\frac{\partial v_{j}}{\partial w_{ji}}\\
 & = & \frac{\partial E_{sparsity}}{\partial o_{j}}\cdot o_{j}(1-o_{j})\cdot o_{i}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E_{sparsity}}{\partial o_{j}} & = & \frac{\partial}{\partial o_{j}}\sum_{j}p\log\frac{p}{o_{j}}+(1-p)\log\frac{1-p}{1-o_{j}}\\
 & = & \frac{\partial}{\partial o_{j}}p\log\frac{p}{o_{j}}+(1-p)\log\frac{1-p}{1-o_{j}}\\
 & = & \frac{\partial}{\partial o_{j}}p\log\frac{p}{o_{j}}+\frac{\partial}{\partial o_{j}}(1-p)\log\frac{1-p}{1-o_{j}}\\
 & = & p\frac{\partial}{\partial o_{j}}\log\frac{p}{o_{j}}+(1-p)\frac{\partial}{\partial o_{j}}\log\frac{1-p}{1-o_{j}}\\
 & = & p\frac{1}{\frac{p}{o_{j}}}\frac{\partial}{\partial o_{j}}\frac{p}{o_{j}}+(1-p)\frac{1}{\frac{1-p}{1-o_{j}}}\frac{\partial}{\partial o_{j}}\frac{1-p}{1-o_{j}}\\
 & = & p\frac{1}{\frac{p}{o_{j}}}p\frac{\partial}{\partial o_{j}}\frac{1}{o_{j}}+(1-p)\frac{1}{\frac{1-p}{1-o_{j}}}(1-p)\frac{\partial}{\partial o_{j}}\frac{1}{1-o_{j}}\\
 & = & p\frac{1}{\frac{p}{o_{j}}}p(-\frac{1}{o_{j}^{2}})+(1-p)\frac{1}{\frac{1-p}{1-o_{j}}}(1-p)\frac{1}{(1-o_{j})^{2}}\\
 & = & -\frac{p}{o_{j}}+\frac{1-p}{1-o_{j}}\\
 & = & \frac{1-p}{1-o_{j}}-\frac{p}{o_{j}}
\end{eqnarray*}

\end_inset

Computing the derivative of the sparsity error 
\begin_inset Formula $E_{sparsity}$
\end_inset

 with respect to the node's input 
\begin_inset Formula $v_{j}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E_{sparsity}}{\partial v_{j}} & = & \frac{\partial E_{sparsity}}{\partial o_{j}}\cdot\frac{\partial o_{j}}{\partial v_{j}}\\
 & = & (\frac{1-p}{1-o_{j}}-\frac{p}{o_{j}})\cdot o_{j}(1-o_{j})\\
 & = & (\frac{(1-p)o_{j}}{(1-o_{j})o_{j}}-\frac{p(1-o_{j})}{o_{j}(1-o_{j})})\cdot o_{j}(1-o_{j})\\
 & = & \frac{(1-p)o_{j}-p(1-o_{j})}{(1-o_{j})o_{j}}\cdot o_{j}(1-o_{j})\\
 & = & (1-p)o_{j}-p(1-o_{j})\\
 & = & o_{j}-po_{j}-p+po_{j}\\
 & = & o_{j}-p
\end{eqnarray*}

\end_inset

Substituting into the derivative of the sparsity error with respect to a
 weight 
\begin_inset Formula $w_{ji}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E_{sparsity}}{\partial w_{ji}} & = & \frac{\partial E_{sparsity}}{\partial o_{j}}\cdot\frac{\partial o_{j}}{\partial v_{j}}\cdot\frac{\partial v_{j}}{\partial w_{ji}}\\
 & = & \frac{\partial E_{sparsity}}{\partial v_{j}}\cdot o_{i}\\
 & = & (o_{j}-p)\cdot o_{i}
\end{eqnarray*}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The idea is to add to the error term 
\begin_inset Formula $\frac{\partial E}{\partial o_{j}}$
\end_inset

 of a hidden node 
\begin_inset Formula $j$
\end_inset

 an additional term that encourages the node to have an average activation
 
\begin_inset Formula $q_{j}$
\end_inset

 close to the sparsity target 
\begin_inset Formula $p$
\end_inset

.
 The term should be small when the average activation 
\begin_inset Formula $q_{j}$
\end_inset

 is close to the sparsity target 
\begin_inset Formula $p$
\end_inset

 and become larger when it deviates.
 One such term is the Kullback-Leibler divergence between a Bernoulli random
 variable with mean 
\begin_inset Formula $p$
\end_inset

 and a Bernoulli random variable with mean 
\begin_inset Formula $q_{j}$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
E_{sparsity} & = & D_{KL}(P_{Bernoulli(p)}||P_{Bernoulli(q_{j})})\\
 & = & p\log\frac{p}{q_{j}}+(1-p)\log\frac{1-p}{1-q_{j}}.
\end{eqnarray*}

\end_inset

It is zero for 
\begin_inset Formula $q_{j}=p$
\end_inset

, and approaches infinity when 
\begin_inset Formula $q_{j}=0$
\end_inset

 or 
\begin_inset Formula $q_{j}=1$
\end_inset

.
 Differentiating this sparsity error with respect to the weight 
\begin_inset Formula $w_{ji}$
\end_inset

 of a connection from node 
\begin_inset Formula $i$
\end_inset

 to node 
\begin_inset Formula $j$
\end_inset

, and approximating the average activation 
\begin_inset Formula $q_{j}$
\end_inset

 to be equal to the activation 
\begin_inset Formula $o_{j}$
\end_inset

 gives
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E_{sparsity}}{\partial w_{ji}} & = & \frac{\partial E_{sparsity}}{\partial o_{j}}\cdot\frac{\partial o_{j}}{\partial v_{j}}\cdot\frac{\partial v_{j}}{\partial w_{ji}}\\
 & \approx & \left(\frac{\partial}{\partial o_{j}}p\log\frac{p}{o_{j}}+(1-p)\log\frac{1-p}{1-o_{j}}\right)\cdot\frac{\partial o_{j}}{\partial v_{j}}\cdot\frac{\partial v_{j}}{\partial w_{ji}}\\
 & = & \left(\frac{1-p}{1-o_{j}}-\frac{p}{o_{j}}\right)\cdot\frac{\partial o_{j}}{\partial v_{j}}\cdot\frac{\partial v_{j}}{\partial w_{ji}}\\
 & = & \left(\frac{1-p}{1-o_{j}}-\frac{p}{o_{j}}\right)\cdot o_{j}(1-o_{j})\cdot o_{i}\\
 & = & (o_{j}-p)\cdot o_{i}.
\end{eqnarray*}

\end_inset

Substituting 
\begin_inset Formula $q_{j}$
\end_inset

 back for 
\begin_inset Formula $o_{j}$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{\partial E_{sparsity}}{\partial w_{ji}}\approx(q_{j}-p)\cdot o_{i}.
\]

\end_inset

Analogously, the derivative of the sparsity error with respect to the bias
 
\begin_inset Formula $b_{j}$
\end_inset

 of node 
\begin_inset Formula $j$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E_{sparsity}}{\partial b_{j}} & = & \frac{\partial E_{sparsity}}{\partial o_{j}}\cdot\frac{\partial o_{j}}{\partial v_{j}}\cdot\frac{\partial v_{j}}{\partial w_{ji}}\\
 & \approx & (q_{j}-p)\cdot1.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Paragraph
Complete Updating Rule
\end_layout

\begin_layout Standard
For a training iteration, both the bias 
\begin_inset Formula $b_{j}$
\end_inset

 of node 
\begin_inset Formula $j$
\end_inset

 and its incoming weights 
\begin_inset Formula $w_{ji}$
\end_inset

 must be adjusted by the derivative of the sparsity error, scaled by the
 sparsity cost
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sparsity cost
\end_layout

\end_inset

 
\begin_inset Formula $\lambda$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\Delta w_{ji} & = & -\epsilon\left(\frac{\partial E}{\partial w_{ji}}+\lambda\frac{\partial E_{sparsity}}{\partial w_{ji}}\right)\approx-\epsilon\left(\left(\sum_{k}\frac{\partial E}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}w_{kj}o_{j}(1-o_{j})\right)+\lambda(q_{j}-p)\right)o_{i}\\
\Delta b_{j} & = & -\epsilon\left(\frac{\partial E}{\partial b_{j}}+\lambda\frac{\partial E_{sparsity}}{\partial w_{ji}}\right)\approx-\epsilon\left(\left(\sum_{k}\frac{\partial E}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}w_{kj}o_{j}(1-o_{j})\right)+\lambda(q_{j}-p)\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
Dropout
\begin_inset CommandInset label
LatexCommand label
name "sub:Dropout"

\end_inset


\end_layout

\begin_layout Standard
Dropout
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
dropout
\end_layout

\end_inset

 is a regularization method to make the nodes in the hidden layers, which
 can be seen as feature detectors, less dependent on each other 
\begin_inset CommandInset citation
LatexCommand cite
key "SrivastavaSalakhutdinov2014"

\end_inset

.
 This is enforced by 
\begin_inset Quotes eld
\end_inset

dropping
\begin_inset Quotes erd
\end_inset

 during each training iteration a random subset of nodes in a hidden or
 visible layer.
 This prevents subsequent layers from adapting to specific combinations
 of node activations in the previous layer, in which nodes are only useful
 in the context of a large number of other nodes.
 It thereby reduces overfitting.
 Dropout can be used in any neural network whose input to a node is computed
 from several input nodes.
 
\end_layout

\begin_layout Standard
Dropout specifies a probability 
\begin_inset Formula $d$
\end_inset

 for nodes in a layer with 
\begin_inset Formula $n$
\end_inset

 nodes to be active during a training iteration.
 In each iteration, on average only 
\begin_inset Formula $d*n$
\end_inset

 nodes' output values 
\begin_inset Formula $o_{j}$
\end_inset

 are computed and the other nodes are set to contribute nothing (i.e.
 
\begin_inset Formula $o_{j}:=0$
\end_inset

) to the input to the next layer.
 To utilize all trained nodes during testing, all nodes contribute to the
 computation of input to a layer, but their total input must then be multiplied
 by 
\begin_inset Formula $d$
\end_inset

, to simulate that only a fraction of 
\begin_inset Formula $d$
\end_inset

 nodes are active.
\end_layout

\begin_layout Standard
The reasoning behind dropout is that for a network with 
\begin_inset Formula $n$
\end_inset

 nodes, there are 
\begin_inset Formula $2^{n}$
\end_inset

 possible ways to drop out those nodes.
 During testing, a network trained with dropout implicitly averages its
 output over all these 
\begin_inset Formula $2^{n}$
\end_inset

 networks.
 This is faster than to do explicit model averaging over 
\begin_inset Formula $2^{n}$
\end_inset

 networks with shared weights.
\end_layout

\begin_layout Standard
Dropping out a random fraction of nodes prevents single nodes from co-adapting
 to the specific workings of a large number of other nodes.
 
\begin_inset CommandInset citation
LatexCommand cite
key "SrivastavaSalakhutdinov2014"

\end_inset

 note that a side-effect of dropout is that the activations of nodes become
 sparse, without another sparsity-inducing regularization method being used.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: probably not describe weight normalization, because I didn't use it
 in my 
\emph on
deepnet 
\emph default
trials.
\end_layout

\begin_layout Plain Layout
Dropout is usually combined with weight normalization.
 
\end_layout

\begin_layout Paragraph
Weight normalization
\end_layout

\begin_layout Plain Layout
probably not TODO: I think weight normalization is described a bit in the
 dropout paper.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Early Stopping
\begin_inset CommandInset label
LatexCommand label
name "sub:Early-stopping"

\end_inset


\end_layout

\begin_layout Standard
Early stopping
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
early stopping
\end_layout

\end_inset

 is not a regularization method, but still a method to prevent overfitting
 in supervised training of an artificial neural network.
 The training data set is split into a training data set and a validation
 data set, and uses only the training data set for adapting the weights
 and biases during learning.
 After each learning iteration, the validation data set is used to compute
 the output error of the current network.
 After a defined number of training iterations, the neural network that
 had the lowest output error on the validation data set is used for predictions.
\end_layout

\begin_layout Standard
This prevents the training procedure from overfitting to sampling error
 present in the training data set 
\begin_inset CommandInset citation
LatexCommand cite
key "Prechelt1997"

\end_inset

.
\end_layout

\begin_layout Subsection
Autoencoder
\end_layout

\begin_layout Standard
An algorithm that can train an artificial neural network deterministically
 with more than one hidden layer is the 
\emph on
auto-associator
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
auto-associator
\end_layout

\end_inset

, or 
\emph on
autoencoder
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
autoencoder
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "BengioLarochelle2007"

\end_inset

.
 The algorithm is unsupervised and iteratively constructs deeper and deeper
 networks.
 Its essential idea is the construction of an encoder network and its anti-symme
tric counterpart, the decoder network.
 Both are trained using back-propagation, wherein the target output to be
 achieved in the output layer is the same unsupervised training sample as
 presented to the network in the input layer, hence the name of the algorithm.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset space \hfill{}
\end_inset

A 
\begin_inset Graphics
	filename images/autoencoder-1.dia
	width 40col%

\end_inset


\begin_inset space \hfill{}
\end_inset

B 
\begin_inset Graphics
	filename images/autoencoder-2.dia
	width 40col%

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Training-of-an-autoencoder"

\end_inset

Training of an autoencoder iteratively adds hidden layers.
 The layers are depicted as rectangles.
 
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Training of an autoencoder.
\end_layout

\end_inset

 A: It starts with a network architecture of an input layer, one hidden
 layer, and an output layer with the same size as the input layer.
 The parameters of this small network are initialized randomly (light gray
 area) and the network is trained.
 B: The hidden layer is copied and another hidden layer is inserted between
 encoder and decoder.
 The added weights are initialized randomly (light gray area), and the whole
 network (
\emph on
including 
\emph default
the previously trained weights, dark gray area) is trained.
 This procedure continues until the network has enough layers.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
encoder
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
encoder
\end_layout

\end_inset

 starts in the first iteration as a network that consists of the input layer
 and one hidden layer on top.
 The (overlapping) 
\emph on
decoder
\begin_inset Index idx
status open

\begin_layout Plain Layout
decoder
\end_layout

\end_inset


\emph default
 network consists of the very same hidden layer and the output layer on
 top, which must have the same dimensions as the input layer.
 Training an autoencoder slowly adds internal layers to en- and decoder,
 see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training-of-an-autoencoder"

\end_inset

.
 The network starts with three layers: input, hidden, and output layer.
 This network is trained using back-propagation.
 Once back-propagation does not improve the reconstruction error on the
 test set anymore, the second step starts.
 In the second step, both encoder and decoder are extended by one layer.
 The existing middle hidden layer is copied and a new hidden layer is inserted
 in the middle.
 The new weights are initialized randomly, for example by drawing from a
 uniform [-1;1] distribution or from a normal distribution with mean 0 and
 standard deviation 1, and the new biases are initialized to zero.
 Then back-propagation is used again to determine the paramters of the whole
 network.
 This process can be repeated until a sufficient number of hidden layers
 has been trained.
\end_layout

\begin_layout Standard
Network size increases iteratively from 3 layers, to 5 layers, to 7 layers,
 and so on, and only 2 weight layers are initialized randomly in each iteration.
 Therefore, the deep network is not stuck in a poor local optimum, because
 there are only few new added weights each iteration, and back-propagation
 finds parameters for a good (local) optimum.
\end_layout

\begin_layout Standard
The goal of training is that the network reconstructs as output patterns
 the input patterns.
 One might think that that is too easy, since the network could just learn
 the identity function at every layer, but usually the number of nodes in
 at least one hidden layer is chosen to be smaller than the number of nodes
 in the input (and output) layer.
 In this way the autoencoder is forced to reconstruct its input from a compresse
d representation.
 Another way to obtain interesting features in the middle hidden layer is
 to use a regularization method on the network.
\end_layout

\begin_layout Subsubsection
Encoder with a Classifier on Top
\end_layout

\begin_layout Standard
The autoencoder as described is an unsupervised algorithm, because it only
 reconstructs its input.
 The autoencoder can however be used in a supervised fashion by first training
 its encoder and decoder networks up to sufficient depth, and then removing
 the decoder network and replacing it by a single output layer that has
 the dimension of the training label.
 The weights between the last hidden layer of the encoder and the new output
 layer as well as the biases of the new output layer are initialized randomly
 (by drawing from a uniform or normal distribution), and trained using back-prop
agation.
\end_layout

\begin_layout Standard
An encoder network trained using an autoencoder is a generative model
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
generative model
\end_layout

\end_inset

, because it was trained with the goal to reconstruct its input, and the
 encoder's last hidden layer contains a compressed representation of the
 input.
 Hence, an encoder with a classifier on top is a generative model with a
 discriminative part put on top.
\end_layout

\begin_layout Subsection
Boltzmann Machines
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/boltzmann-machine-example.dia
	width 40col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Boltzmann-Machine-schema"

\end_inset

A schematic example of a Boltzmann Machine.
 
\begin_inset Argument 1
status open

\begin_layout Plain Layout
A schematic example of a Boltzmann Machine.
\end_layout

\end_inset

 There are visible nodes V1 to V5 and hidden nodes H1 to H3, each of which
 have a real-valued bias.
 Pairs of nodes are connected with an undirected and real-valued weight.
 All pairs of nodes can be connected by a weight different from 0, but self-conn
ections are not allowed.
 A Boltzmann Machine stores a joint probability distribution (see text).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
A Boltzmann Machine
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Boltzmann Machine
\end_layout

\end_inset

 is a stochastic version of a Hopfield network.
 It is an undirected graphical model that has a specific form of the conditional
 probability distribution defined at each node 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

.
 An schematic example can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Boltzmann-Machine-schema"

\end_inset

.
 There are visible nodes 
\series bold

\begin_inset Formula $\mathbf{V}$
\end_inset


\series default
 and hidden nodes 
\begin_inset Formula $\mathbf{H}$
\end_inset

, all of which have a binary state.
 The visible nodes correspond to variables in a training sample, while the
 hidden nodes model dependencies between those variables, and can be seen
 as feature detectors.
 Any two nodes 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 may be connected using an undirected connection with weight 
\begin_inset Formula $w_{ij}$
\end_inset

, with the restrictions that there are no self-connections (
\begin_inset Formula $w_{ii}=0$
\end_inset

) and all connections are symmetric (
\begin_inset Formula $w_{ij}=w_{ji}$
\end_inset

).
 A Boltzmann Machine stores a joint probability distribution.
 
\end_layout

\begin_layout Standard
The conditional probability distribution for a hidden node 
\begin_inset Formula $H_{i}\in\mathbf{H}$
\end_inset

 depends on the states of all other nodes 
\begin_inset Formula $\mathbf{S_{j}}$
\end_inset

 and is defined by 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSejnowski1986"

\end_inset

 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
in Eq 2, page 6, and Eq 3, page 8
\end_layout

\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(H_{i}=1\mid\mathbf{S_{j}}=\mathbf{s_{j}}:j\neq i)=\sigma\left(\sum_{j}s_{j}w_{ij}-b_{i}\right),\label{eq:Boltzmann Machine p(h=1|S)}
\end{equation}

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
(this formula is only valid for 0/1-valued nodes.)
\end_layout

\end_inset

where 
\begin_inset Formula $\mathbf{S}=\mathbf{V}\cup\mathbf{H}$
\end_inset

, 
\begin_inset Formula $s_{j}$
\end_inset

 is the state of node 
\begin_inset Formula $S_{j}$
\end_inset

, 
\begin_inset Formula $\sigma(x)=\frac{1}{1+\exp(-x)}$
\end_inset

, 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

 is the weight between hidden node 
\begin_inset Formula $H_{i}$
\end_inset

 and (visible or hidden) node 
\begin_inset Formula $S_{j}$
\end_inset

, and 
\begin_inset Formula $b_{i}$
\end_inset

 is the bias of hidden node 
\begin_inset Formula $H_{i}$
\end_inset

.
 Similarly,
\begin_inset Formula 
\[
P(V_{j}=1\mid\mathbf{S_{i}}=\mathbf{s_{i}}:i\neq j)=\sigma\left(\sum_{i}s_{i}w_{ij}-c_{j}\right),
\]

\end_inset

where 
\begin_inset Formula $V_{j}$
\end_inset

 is a visible node, 
\begin_inset Formula $w_{ij}=w_{ji}$
\end_inset

 is the weight between node 
\begin_inset Formula $S_{i}$
\end_inset

 and visible node 
\begin_inset Formula $V_{j}$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
this 
\begin_inset Formula $w_{ij}$
\end_inset

 is the same as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Boltzmann Machine p(h=1|S)"

\end_inset


\end_layout

\end_inset

, and 
\begin_inset Formula $c_{j}$
\end_inset

 is the bias of visible node 
\begin_inset Formula $V_{j}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Gibbs Sampling in Boltzmann Machines
\end_layout

\begin_layout Standard
A Boltzmann Machine is an undirected graphical model.
 Therefore the approximate Gibbs sampling inference algorithm from 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 applies, which works by iteratively drawing the state of an unknown variable
 
\begin_inset Formula $s_{i}\in\mathbf{S}=\mathbf{H}\cup\mathbf{V}$
\end_inset

 from its conditional probability distribution, given the states of all
 other variables 
\begin_inset Formula $\mathbf{s_{j:j\neq\mathbf{i}}}$
\end_inset

.
 See section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Inference-in-Markov-Random-Fields"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Training Boltzmann Machines
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
probably not TODO (since the next paragraphs look finished): rewrite the
 following using consistent variable names and fonts.
\end_layout

\begin_layout Plain Layout
Neal writes (on page 75, I don't know for which pdf file, however) about
 training of Boltzmann machines, i.e.
 adjusting the weights so that a set of training samples 
\begin_inset Formula $T$
\end_inset

 (i.e.
 measured states of the visible nodes) become as probable as possible.
 The log-likelihood is 
\begin_inset Formula $L=\log\prod_{v\in T}P(V=v)$
\end_inset

.
 The derivative of the log-likelihood with respect to a weight is: 
\begin_inset Formula $\frac{\partial L}{\partial w_{ij}}=\beta\sum_{\mathbf{v}\in\mathbf{T}}(\sum_{s}P(\mathbf{S=s}\mid\mathbf{V=v})s_{i}s_{j}-\sum_{\mathbf{s}}P(\mathbf{S=s})s_{i}s_{j})$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: derive 
\begin_inset Formula $\frac{\partial L}{\partial w_{ij}}$
\end_inset


\end_layout

\end_inset

 (
\begin_inset Formula $\beta$
\end_inset

 is defined on page 74 to be 1 for 0/1-valued nodes, and 
\begin_inset Formula $\frac{1}{2}$
\end_inset

 for -1/1-valued nodes.)
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
In 
\begin_inset Quotes eld
\end_inset

Connectionist learning for belief networks
\begin_inset Quotes erd
\end_inset

 (~/uni/publication/zusammenfassung/rbm/A11 Connectionist learning of belief
 networks.pdf), Neal derives the learning rule for Boltzmann Machines: page
 6, equation 7.
\end_layout

\end_inset

The goal of training a Boltzmann Machine is to find parameters, i.e.
 weights and biases, such that the the probability of the training data
 becomes maximal.
 Remember that Boltzmann Machines store a joint probability distribution.
 The log-likelihood is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
L & = & \log\prod_{\mathbf{v}\in\mathbf{T}}P(\mathbf{V}=\mathbf{v}),
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\mathbf{T}$
\end_inset

 is the set of training data (to be applied to the visible nodes) and its
 derivative with respect to a weight 
\begin_inset Formula $w_{ij}$
\end_inset

 is 
\begin_inset Formula 
\[
\frac{\partial L}{\partial w_{ij}}=\sum_{\mathbf{v}\in\mathbf{T}}\left(\sum_{\mathbf{s}}P(\mathbf{S=s}\mid\mathbf{V=v})s_{i}s_{j}-\sum_{\mathbf{s}}P(\mathbf{S=s})s_{i}s_{j}\right),
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{S}$
\end_inset

 is 
\begin_inset Formula $\mathbf{V}\cup\mathbf{H}$
\end_inset

, and 
\begin_inset Formula $s_{i}$
\end_inset

 is the state of node 
\begin_inset Formula $S_{i}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

.
 The goal is to find a delta 
\begin_inset Formula $\Delta w_{ij}$
\end_inset

 for each weight 
\begin_inset Formula $w_{ij}$
\end_inset

, which can be added to the weight, so that the likelihood for the training
 sample using the updated weights 
\begin_inset Formula $w_{ij}+\Delta w_{ij}$
\end_inset

 increases.
 The derivative of the log-likelihood with respect to a weight 
\begin_inset Formula $w_{ij}$
\end_inset

 multiplied by a learning rate
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
learning rate
\end_layout

\end_inset

 provides such a delta.
 This derivative can be approximated by the difference between two parallel
 Gibbs Sampling steps: the 
\emph on
positive phase
\emph default
, where 
\begin_inset Formula $P(\mathbf{S=s}\mid\mathbf{V=v})s_{i}s_{j}$
\end_inset

 is approximated, and the 
\emph on
negative phase
\emph default
, where 
\begin_inset Formula $P(\mathbf{S=s})s_{i}s_{j}$
\end_inset

 is approximated 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

.
\end_layout

\begin_layout Paragraph
Positive Phase
\end_layout

\begin_layout Standard
In the positive phase
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
positive phase
\end_layout

\end_inset

 of training a Boltzmann Machine, the visible nodes 
\begin_inset Formula $\mathbf{V}$
\end_inset

 are clamped (i.e.
 their state is held fixed) to their states as they are in the training
 sample 
\begin_inset Formula $\mathbf{v}$
\end_inset

, and then the states of the remaining (i.e.
 hidden) nodes are sampled via Gibbs Sampling.
 We start in any (for example random) configuration of the hidden nodes,
 repeatedly sample each remaining variable 
\begin_inset Formula $S$
\end_inset

 from its conditional probability distribution given the states of all other
 variables (i.e.
 
\begin_inset Formula $P(S_{i}=s_{i}\mid\mathbf{S_{j}}=\mathbf{s_{j}}:j\neq i)$
\end_inset

) until the Gibbs sampler reaches equilibrium, and record the state 
\begin_inset Formula $s_{i}$
\end_inset

 that each remaining variable 
\begin_inset Formula $S_{i}$
\end_inset

 had assumed in equilibrium.
 By repeatedly sampling the 
\begin_inset Formula $s_{i}$
\end_inset

 a few times 
\begin_inset Formula $t$
\end_inset

 when the Markov chain is in equilibrium, we determine their probability
 distributions, where 
\begin_inset Formula $t$
\end_inset

 depends on the desired resolution of the probabilities.
 The conditional probability distribution of the remaining variables 
\begin_inset Formula $P(\mathbf{S=s}|\mathbf{V=v})$
\end_inset

 is determined.
 Therefore the term 
\begin_inset Formula $\sum_{\mathbf{s}}P(\mathbf{S=s}|\mathbf{V=v})s_{i}s_{j}$
\end_inset

 can be determined, which completes the positive phase.
\end_layout

\begin_layout Paragraph
Negative Phase
\end_layout

\begin_layout Standard
In the negative phase
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
negative phase
\end_layout

\end_inset

 no nodes are clamped, and the states of all variables in equilibrium are
 recorded.
 Again, we start in any configuration of the network.
 Then we repeatedly sample from the conditional probability distributions
 
\begin_inset Formula $P(S_{i}=s_{i}\mid S_{j}=s_{j}:j\neq i)$
\end_inset

 for all variables 
\begin_inset Formula $S_{i}$
\end_inset

 until equilibrium, and record the state 
\begin_inset Formula $s_{i}$
\end_inset

 each variable 
\begin_inset Formula $S_{i}$
\end_inset

 had in equilibrium.
 Sampling a few more steps in equilibrium, we can determine their distributions
 
\begin_inset Formula $P(\mathbf{S=s})$
\end_inset

 and therefore the term 
\begin_inset Formula $\sum_{\mathbf{s}}P(\mathbf{S=s})s_{i}s_{j}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Training Iterations
\end_layout

\begin_layout Standard
The derivatives obtained by the positive and negative phases are multiplied
 by the learning rate
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
learning rate
\end_layout

\end_inset

 
\begin_inset Formula $\epsilon$
\end_inset

 (a small positive real constant) and added to the current weights
\begin_inset Formula 
\[
w_{ij}^{(t+1)}=w_{ij}^{(t)}+\epsilon\frac{\partial L}{\partial w_{ij}}=w_{ij}^{(t)}+\Delta w_{ij}^{(t)}.
\]

\end_inset

Then another training iteration is started.
 This is repeated until the derivatives all converge to zero.
\end_layout

\begin_layout Paragraph
Connections to other Graphical Models
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 notes that the Boltzmann Machine is a generalization of the Ising model
 of ferromagnetism: 
\begin_inset Quotes eld
\end_inset

Generalized to allow [parameters] to vary from spin to spin, and to allow
 interactions between any two spins, the Ising model becomes the 
\begin_inset Quotes eld
\end_inset

Boltzmann machine
\begin_inset Quotes erd
\end_inset

 of Ackley, Hinton, and Sejnowski.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsection
Restricted Boltzmann Machines
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/restricted-boltzmann-machine-example.dia
	width 40col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Restricted-Boltzmann-Machine-schema"

\end_inset

A schematic example of a Restricted Boltzmann Machine.
 
\begin_inset Argument 1
status open

\begin_layout Plain Layout
A schematic example of a Restricted Boltzmann Machine.
\end_layout

\end_inset

 There are two layers: the visible layers with nodes V1 to V5 and the hidden
 layer with nodes H1 to H3, each of which have a real-valued bias.
 Node pairs from different layers are connected with an undirected and real-valu
ed weight.
 Connections between nodes from the same layer and self-connections are
 not allowed.
 A Restricted Boltzmann Machine stores a joint probability distribution
 (see text).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Example figure of an RBM.
\end_layout

\end_inset

A Restricted Boltzmann Machine
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Restricted Boltzmann Machine
\end_layout

\end_inset

(RBM
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
RBM
\end_layout

\end_inset

) is a restricted variant of a Boltzmann Machine.
 Hence, it also is a way to store a joint probability distribution.
 An schematic example of a Restricted Boltzmann Machine can be seen in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Restricted-Boltzmann-Machine-schema"

\end_inset

.
 It has a bipartite topology: there are visible nodes 
\begin_inset Formula $\mathbf{V}$
\end_inset

 and hidden nodes 
\begin_inset Formula $\mathbf{H}$
\end_inset

, and each node in the visible layer is connected to all hidden nodes by
 undirected edges, but in contrast to general Boltzmann Machines no visible-to-v
isible node connections and no hidden-to-hidden node connections are allowed.
 In a Restricted Boltzmann Machine, the visible nodes represent the observable
 features of a training set, while the hidden nodes are feature detectors
 which are computed from the states of all visible nodes.
\end_layout

\begin_layout Standard
As originally proposed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Smolensky1986"

\end_inset

, a Restricted Boltzmann Machine has binary visible and hidden nodes.
 There are extensions to real-valued nodes, however.
 See for example 
\begin_inset CommandInset citation
LatexCommand cite
key "FischerIgel2012"

\end_inset

.
\end_layout

\begin_layout Standard
The conditional probabilities at the nodes are defined analogous to Boltzmann
 Machines (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Boltzmann Machine p(h=1|S)"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(H_{i}=1\mid\mathbf{V}=\mathbf{v})=\sigma\left(\sum_{j}v_{j}w_{ij}-b_{i}\right)
\]

\end_inset


\begin_inset Formula 
\[
P(V_{j}=1\mid\mathbf{H}=\mathbf{h})=\sigma\left(\sum_{i}h_{i}w_{ij}-c_{j}\right),
\]

\end_inset

 where 
\begin_inset Formula $H_{i}$
\end_inset

 is the binary state of hidden node 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $\sigma(\cdot)$
\end_inset

 is the sigmoid function, 
\begin_inset Formula $j$
\end_inset

 is an index over all visible nodes, 
\begin_inset Formula $v_{j}$
\end_inset

 is the state of visible node 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $w_{ij}$
\end_inset

 is the weight of the connection between node hidden node 
\begin_inset Formula $i$
\end_inset

 and visible node 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $b_{i}$
\end_inset

 is the bias of hidden node 
\begin_inset Formula $i$
\end_inset

, and 
\begin_inset Formula $V_{j}$
\end_inset

 is the state of visible node 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $i$
\end_inset

 is an index over all hidden nodes, 
\begin_inset Formula $h_{i}$
\end_inset

 is the state of hidden node 
\begin_inset Formula $i$
\end_inset

, and 
\begin_inset Formula $c_{j}$
\end_inset

 is the bias of visible node 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
When choosing a zero temperature 
\begin_inset Formula $T=0$
\end_inset

, the Restricted Boltzmann Machine becomes deterministic and equivalent
 to a Hopfield network.
\end_layout

\begin_layout Plain Layout
The energy of a hopfield network was defined in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Energy of a Hopfield network"

\end_inset

 
\begin_inset CommandInset ref
LatexCommand vpageref
reference "eq:Energy of a Hopfield network"

\end_inset

 as:
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: repeat energy of equation 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Contrastive Divergence Learning 
\begin_inset CommandInset label
LatexCommand label
name "sub:Training-Restricted-Boltzmann-Machines-using-Contrastive-Divergence"

\end_inset


\end_layout

\begin_layout Standard
A Restricted Boltzmann Machine is a restricted form of the more general
 Boltzmann Machine.
 Therefore, it can be trained using the training procedure for Boltzmann
 Machines.
 However, there is also a more direct learning procedure called 
\emph on
contrastive divergence
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
contrastive divergence
\end_layout

\end_inset

, where the positive phase is simpler, because the hidden and visible nodes
 are conditionally independent, given the nodes of other type.
 Contrastive divergence is obtained by approximating the derivative of the
 log-likelihood with respect to a weight 
\begin_inset Formula $w_{ij}$
\end_inset

.
\end_layout

\begin_layout Standard
Like for Boltzmann Machines, the goal of training is to find parameters
 such that the the probability of the training data becomes maximal.
 The log-likelihood is 
\begin_inset Formula 
\begin{eqnarray*}
L & = & \log\prod_{\mathbf{v}\in\mathbf{T}}P(\mathbf{V}=\mathbf{v}),
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\mathbf{T}$
\end_inset

 is the set of training data (to be applied to the visible nodes) and its
 derivative with respect to a weight 
\begin_inset Formula $w_{ij}$
\end_inset

 is 
\begin_inset Formula 
\[
\frac{\partial L}{\partial w_{ij}}=\sum_{\mathbf{v}\in\mathbf{T}}\left(P(H_{i}=1\mid\mathbf{V=v})v_{j}-\sum_{\mathbf{v}}P(\mathbf{V=v})P(H_{i}=1\mid\mathbf{v})v_{j}\right)
\]

\end_inset

 (see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "FischerIgel2012"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Formel 29 auf Seite 26
\end_layout

\end_inset

).
\end_layout

\begin_layout Paragraph
Positive Phase
\end_layout

\begin_layout Standard
The positive phase, i.e.
 
\begin_inset Formula $P(H_{i}=1\mid\mathbf{V=v})v_{j}$
\end_inset

 can be computed directly by setting the visible nodes to the training sample,
 and then computing 
\begin_inset Formula $P(H_{i}=1\mid\mathbf{V=v})=\sigma\left(\sum_{j}v_{j}w_{ij}-b_{i}\right)$
\end_inset

.
 Multiplying by 
\begin_inset Formula $v_{j}$
\end_inset

 completes the positive phase of computing the delta for 
\begin_inset Formula $w_{ij}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Negative Phase
\end_layout

\begin_layout Standard
The negative phase 
\begin_inset Formula $\sum_{\mathbf{v}}P(\mathbf{V=v})P(H_{i}=1\mid\mathbf{v})$
\end_inset

 is not as straightforward to compute.
 It may be approximated by running a Gibbs chain until convergence.
 We first initialize the network with any state, then alternatingly compute
 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

 and 
\begin_inset Formula $P(\mathbf{V}\mid\mathbf{H})$
\end_inset

 until the stationary distribution is reached.
 The number of iterations is 
\begin_inset Formula $k$
\end_inset

 and the gradient computed by contrastive divergence
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
contrastive divergence
\end_layout

\end_inset

 (i.e.
 the difference of positive phase and negative phase) is called 
\begin_inset Formula $CD_{k}$
\end_inset

.
 Often 
\begin_inset Formula $k=1$
\end_inset

 is used at the beginning of training and later 
\begin_inset Formula $k$
\end_inset

 is incremented.
 
\begin_inset Formula $\mathbf{v}$
\end_inset

 and 
\begin_inset Formula $\mathbf{h}$
\end_inset

 are sampled from the stationary distribution and allow computing 
\begin_inset Formula $\sum_{\mathbf{v}}P(\mathbf{V=v})P(H_{i}=1\mid\mathbf{v})v_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
(As 
\begin_inset CommandInset citation
LatexCommand cite
key "BengioDelalleau2007"

\end_inset

 showed, training using contrastive divergence (
\begin_inset Formula $CD_{1}$
\end_inset

) and training using reconstruction error as error metric in an autoencoder
 are linked.
 They showed that the weight learning gradient used with the reconstruction
 error is a more biased approximator of the log-likelihood than the gradient
 computed by 
\begin_inset Formula $CD_{1}$
\end_inset

.)
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Parameters of Training a Restricted Boltzmann Machine
\end_layout

\begin_layout Standard
In addition to the parameters for training a Multilayer Perceptron, i.e.
 the amount of momentum, the choice of the activation function, and how
 to randomly initialize weights and biases (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Parameters-of-Training-a-Multilayer-Perceptron"

\end_inset

), there are the following:
\end_layout

\begin_layout Paragraph
Interpreting the Output of a Node as a Continuous Value
\end_layout

\begin_layout Standard
The output of a node in a Restricted Boltzmann Machine, is binary (i.e.
 either 0 or 1).
 However the sigmoid activation function outputs continuous values between
 0 and 1.
 This output of the sigmoid activation function is interpreted as the probabilit
y that the node outputs value 1, and 0 otherwise.
 Using this output value directly, without sampling from a binomial distribution
, allows the output to be from the interval 
\begin_inset Formula $\{0,1\}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Linear nodes with independent Gaussian noise
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006"

\end_inset

 proposed a way to extend Restricted Boltzmann Machines with only binary
 values to nodes with real values.
 However, this extension was largely replaced by rectified linear nodes
 because they performed better.
\end_layout

\begin_layout Paragraph
Rectified linear activation function
\begin_inset CommandInset label
LatexCommand label
name "par:Rectified-linear-activation-function"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "NairHinton2010"

\end_inset

 then modified the idea in 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006"

\end_inset

 to rectified linear nodes, in which the sampled output of a node is given
 by 
\begin_inset Formula $\max(0,x+N(0,\sigma(x))$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is the sum of the inputs of the node, 
\begin_inset Formula $\sigma(x)$
\end_inset

 is the sigmoid function, and 
\begin_inset Formula $N(0,\sigma(x))$
\end_inset

 is normally distributed noise with mean 
\begin_inset Formula $0$
\end_inset

 and variance 
\begin_inset Formula $\sigma(x)$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Rainer wants to know where the 
\begin_inset Formula $N(0,\sigma(x))$
\end_inset

 term comes from, but I can't find the paper offline.
\end_layout

\end_inset

.
 This allows using any positive real value for the random variables of a
 RBM.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Say something about the time taken to train RBMs.
 Training time is not deterministic, since the energy landscape is affected
 by the training samples.
 Training is quadratic in the number of features if a multiple-layer neural
 network is trained and the number of features is gradually reduced.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Deep Belief Networks
\begin_inset CommandInset label
LatexCommand label
name "sub:Deep-Belief-Network"

\end_inset


\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Belief network
\end_layout

\end_inset

Belief Network is another name for directed graphical model.
 
\emph on
Deep Belief Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset

(DBN
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DBN
\end_layout

\end_inset

) are Belief Networks in which the nodes are organized in layers, and where
 the value of a node in a layer only depends on the values of nodes in the
 layer above.
 There are no loops in Deep Belief Networks.
 The word 
\begin_inset Quotes eld
\end_inset

deep
\begin_inset Quotes erd
\end_inset

 refers to the number of (more than a few) hidden layers of a Deep Belief
 Network.
 Deep Belief Networks can be seen as the stochastic counterpart of deterministic
 feed-forward networks.
\end_layout

\begin_layout Standard
Deep Belief Networks are directed graphical models, therefore the computation
 of the values of children nodes does not affect the value of parent nodes.
 In contrast to undirected graphical models, this allows generating (drawing
 a sample) from the model in a single pass.
 Thus, a Deep Belief Network can be used in an unsupervised algorithm to
 generate samples distributed like a training data set.
\end_layout

\begin_layout Paragraph
Sigmoid Belief Networks
\end_layout

\begin_layout Standard
Sigmoid Belief Networks were defined by 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

 as a directed graphical model with a sigmoid conditional probability function.
 A Sigmoid Belief Network is a Belief Network in which the conditional probabili
ty associated with node 
\begin_inset Formula $N_{i}$
\end_inset

 depends only on previous nodes 
\begin_inset Formula $\mathbf{N_{j}}$
\end_inset

 (where parents must come before children).
 The conditional probabilities can be expressed as a sigmoid function 
\begin_inset Formula 
\begin{eqnarray*}
P(N_{i} & = & 1_{i}\mid\mathbf{N_{j}=n_{j}}:j<i)=\sigma(\sum_{j<i}n_{j}w_{ij}-b_{i}),
\end{eqnarray*}

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Note that Neal defines on page 10, 
\begin_inset Formula $P(S_{i}=s_{i}\mid S_{j}=s_{j}:j<i)=\sigma(s_{i}^{*}\sum_{j<i}s_{j}w_{ij})$
\end_inset

.
 The 
\begin_inset Formula $s_{i}^{*}$
\end_inset

 is defined as 
\begin_inset Formula $2s_{i}-1$
\end_inset

 for 0/1-valued nodes.
 Since I write 
\begin_inset Formula $P(N_{i}=1\mid...)$
\end_inset

 above, I can substitute 
\begin_inset Formula $1$
\end_inset

 where 
\begin_inset Formula $s_{i}^{*}$
\end_inset

 is written on the right side.
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Neal says on the bottom of page 3: 
\begin_inset Quotes eld
\end_inset

"Bias" weights, wi0, from a fictitious unit 0 whose value is aways 1 are
 also assumed to be present.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

 where 
\begin_inset Formula $n_{j}$
\end_inset

 is the binary state of node 
\begin_inset Formula $N_{j}$
\end_inset

, 
\begin_inset Formula $w_{ij}$
\end_inset

 is the directed weight from node 
\begin_inset Formula $N_{j}$
\end_inset

 to node 
\begin_inset Formula $N_{i}$
\end_inset

, and 
\begin_inset Formula $b_{i}$
\end_inset

 is the bias of node 
\begin_inset Formula $N_{i}$
\end_inset

.
 Thus the conditional probabilities of a Sigmoid Belief Network are parameterize
d with the weights and biases.
\end_layout

\begin_layout Paragraph
Arbitrary Modelling Capability
\end_layout

\begin_layout Standard
Both Boltzmann machines and Sigmoid Belief Networks can represent arbitrary
 probability distributions over a set of an arbitrary number of visible
 nodes, provided that a sufficient number of hidden nodes is available 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

It turns out that each of these three networks can represent probability
 distributions over the full set of units that the other two networks cannot.
 With the help of "hidden" units, all these networks can represent arbitrary
 distributions over a set of "visible" units.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
 However, as 
\begin_inset CommandInset citation
LatexCommand cite
key "Hastad1987"

\end_inset

 showed, a network with one hidden layer less needs up to an exponential
 factor more hidden nodes.
 Thus, a Deep Belief Network with the same total number of hidden nodes
 needs less computation steps to draw from a probability distribution.
\end_layout

\begin_layout Subsubsection
Training Samples Viewed as Generated by a Deep Belief Network
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/deep-belief-network.dia
	width 40page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:training-a-DBN"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Training a Deep Belief Network
\end_layout

\end_inset

The training samples are generated by a Deep Belief Network, a directed
 graphical model.
 In the depicted example, the top layer 
\begin_inset Formula $\mathbf{F}$
\end_inset

 consists of the random variables that represent the causes, leading to
 an observable training sample in the bottom layer 
\begin_inset Formula $\mathbf{V}$
\end_inset

.
 The distribution of a random variable in the layers below the first (
\begin_inset Formula $\mathbf{G}$
\end_inset

, 
\begin_inset Formula $\mathbf{H}$
\end_inset

, and 
\begin_inset Formula $\mathbf{V}$
\end_inset

) is determined by the states of the random variables in the layer above.
 Only the states of the random variables in the bottom visible layer 
\begin_inset Formula $\mathbf{V}$
\end_inset

 are observable.
 Training a Deep Belief Network means finding weights for the connections
 between the layers and biases for each variable, so that the whole model
 could have generated the training data.
\end_layout

\end_inset


\end_layout

\end_inset

Training a Deep Belief Network is unsupervised, and we have unlabeled training
 data consisting of a set of vectors 
\begin_inset Formula $\mathbf{v_{p}}$
\end_inset

, each with the same dimension.
 (Hence the training data can be represented by a matrix.) We view these
 training samples as being the result of the probabilistic evaluation of
 a Deep Belief Network.
 This is depicted in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:training-a-DBN"

\end_inset

.
 Unsupervised training of a Deep Belief Network means finding weights between
 hidden and visible nodes such that the likelihood given the training samples
 becomes maximal.
\end_layout

\begin_layout Paragraph
Gradient Ascent of the Whole Model is Infeasible
\end_layout

\begin_layout Standard
We could try doing gradient ascent of the whole model.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

 showed that this would mean computing the derivative of the likelihood
 
\begin_inset Formula $L$
\end_inset

 with respect to a weight 
\begin_inset Formula $w_{kj}$
\end_inset

 of the connection from node 
\begin_inset Formula $j$
\end_inset

 to node 
\begin_inset Formula $k$
\end_inset

 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
in equation 27.
 We leave away the summation over training samples 
\begin_inset Formula $\sum_{\tilde{v}\in T}$
\end_inset

, and disallow within-layer connections from nodes 
\begin_inset Formula $k$
\end_inset

 to nodes 
\begin_inset Formula $i$
\end_inset

, where 
\begin_inset Formula $k<i$
\end_inset

.
 We allow only connections between nodes from adjacent layers.
\end_layout

\end_inset


\begin_inset Formula 
\[
\frac{\partial L}{\partial w_{kj}}=\sum_{\mathbf{h}}P(\mathbf{H}=\mathbf{h}\mid\mathbf{V}=\mathbf{v})h_{j}\sigma\left(-h_{k}\sum_{i}h_{i}w_{ji}\right),
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{H}=(H_{i})_{i=1,..n}$
\end_inset

 are the nodes from layers above the visible layer, 
\begin_inset Formula $\mathbf{h}=(h_{i})_{i=1,..n}$
\end_inset

 are their states, 
\begin_inset Formula $\mathbf{V}$
\end_inset

 and 
\begin_inset Formula $\mathbf{v}$
\end_inset

 are the visible nodes and their states, 
\begin_inset Formula $h_{j}$
\end_inset

 is the state of node 
\begin_inset Formula $j$
\end_inset

, and 
\begin_inset Formula $h_{i}$
\end_inset

 is the state of node 
\begin_inset Formula $i$
\end_inset

, which is in the layer above node 
\begin_inset Formula $j$
\end_inset

.
 In the derivative we would have to evaluate the conditional probability
 
\begin_inset Formula $P(\mathbf{H}=\mathbf{h}\mid\mathbf{V}=\mathbf{v})$
\end_inset

, which is an inference problem.
 However, exact inference of the hidden nodes given the visible nodes is
 intractable.
 Therefore we would have to resort to approximate Gibbs Sampling.
 This would work by alternatingly updating each random variable using the
 conditional probability of the variable given all other variables (see
 page 
\begin_inset CommandInset ref
LatexCommand pageref
reference "par:Gibbs-Sampling-in-Markov-Random-Fields"

\end_inset

).
 However, in Gibbs Sampling, all hidden variables (of all layers simultaneously)
 are inferred together (at the same time), and this scales poorly as models
 become larger.
 Therefore another learning algorithm is needed.
\end_layout

\begin_layout Subsubsection
A Fast Learning Algorithm for Deep Belief Networks
\begin_inset CommandInset label
LatexCommand label
name "sub:A-Fast-Learning-Algorithm-for-Deep-Belief-Networks"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 showed that there is a fast greedy learning algorithm for Deep Belief Networks,
 even with many hidden layers and millions of parameters.
 It does not train the weights between all layers at once, but starts with
 the weights between the lowest two layers, and iteratively adds layers
 and their weights.
\end_layout

\begin_layout Standard
There are computational problems with inferring the hidden variables from
 visible ones: Inference requires marginalizing out all variables of a layer
 except one due to explaining away, and it requires integrating over all
 variables above that layer (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Exact-Inference-in-Deep-Belief-Networks-is-Complicated"

\end_inset

).
 In addition, updating a weight requires knowing all weights above in the
 network.
 The problems would go away if the posterior of the hidden given the visible
 nodes were independent between individual hidden nodes, because this would
 eliminate explaining away.
\end_layout

\begin_layout Standard
Hence, 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 came up with a trick: The posterior is equal to the product of prior times
 likelihood.
 If the prior were so that it would cancel the correlations of the likelihood,
 then the product would factor according to the hidden nodes 
\begin_inset Formula $\mathbf{H}$
\end_inset

 
\begin_inset Formula 
\[
P(\mathbf{H}\mid\mathbf{V})=\prod_{i}P(H_{i}=h_{i}\mid\mathbf{V}),
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{V}$
\end_inset

 are the visible nodes.
 They showed that such 
\emph on
complementary 
\emph default
priors
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
complementary prior
\end_layout

\end_inset

 exist and are a functional family of the form
\begin_inset Formula 
\[
P(\mathbf{H})=\frac{1}{C}\exp\left(\log\Omega(\mathbf{h})+\sum_{i}\alpha_{i}(h_{i})\right),
\]

\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is a normalization constant, 
\begin_inset Formula $\Omega$
\end_inset

 is a function of the states of the hidden variables and the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are functions depending on the hidden states individually.
 In the desired factorial form of the posterior all the 
\begin_inset Formula $H_{i}$
\end_inset

 must be conditionally independent (given visible variables 
\begin_inset Formula $\mathbf{V}$
\end_inset

).
 By the Hammersley-Clifford theorem (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:The-Hammersley-Clifford-theorem-of-Undirected-Graphical-Model"

\end_inset

) these conditions are fulfilled in an undirected graphical model that has
 edges between a hidden and a visible variable and edges between all visible
 nodes with a joint probability of the form
\begin_inset Formula 
\begin{equation}
P(\mathbf{V},\mathbf{H})=\frac{1}{C}\exp\left(\sum_{i}\Phi_{i}(\mathbf{v},h_{i})+\beta(\mathbf{v})+\sum_{i}\alpha_{i}(h_{i})\right).\label{eq:DBN-joint-probability-with-dependencies-between-visible}
\end{equation}

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Susi findet, dass das 
\begin_inset Formula $\Phi$
\end_inset

 hier dem Phi aus Hammersley-Clifford zu ähnlich ist.
\end_layout

\end_inset

For reasons that will be explained in a moment, we also want to get rid
 of the edges (i.e.
 dependencies) between the visible nodes.
 The conditional probabilities are then of the form
\begin_inset Formula 
\begin{equation}
P(\mathbf{H}\mid\mathbf{V})=\prod_{i}P(h_{i}\mid\mathbf{v}).\label{eq:DBN-y-given-x}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(\mathbf{V}\mid\mathbf{H})=\prod_{k}P(v_{k}\mid\mathbf{h})\label{eq:DBN-x-given-y}
\end{equation}

\end_inset

Also by the Hammersley-Clifford theorem, the joint probability then specializes
 from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DBN-joint-probability-with-dependencies-between-visible"

\end_inset

 to
\begin_inset Formula 
\[
P(\mathbf{V},\mathbf{H})=\frac{1}{C}\exp\left(\sum_{i}\Phi_{i}(\mathbf{v},h_{i})+\sum_{k}\gamma_{k}(v_{k})+\sum_{i}\alpha_{i}(h_{i})\right).
\]

\end_inset


\end_layout

\begin_layout Standard
The reason we wanted to have both independencies as encoded by equations
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DBN-x-given-y"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DBN-y-given-x"

\end_inset

 is that these are the (in)dependecies described by a Restricted Boltzmann
 Machine.
 Inference in an RBM works by repeatedly and alternatingly evaluating these
 two conditional probabilities.
 The correctly inferred distribution is obtained once the Markov chain reaches
 equilibrium in iterating.
 However, we can also view this iterative inference as taking place in an
 infinitely deep directed graphical model that has alternating visible and
 hidden layers and has shared (
\begin_inset Quotes eld
\end_inset

tied
\begin_inset Quotes erd
\end_inset

) weights
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
tied weights
\end_layout

\end_inset

 at all layers.
 The weights matrix between the layers in the directed graphical model are
 
\begin_inset Formula $W$
\end_inset

 from hidden to visible and 
\begin_inset Formula $W^{T}$
\end_inset

 from visible to hidden layers.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
TODO: insert graphics like Figure 3 in 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

.
\end_layout

\end_inset

 It is this idea of unrolling the RBM in time that gives rise to the following
 training procedure for the weights of a Deep Belief Network.
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier fehlt noch die Herleitung, dass RBMs benutzt werden können.
 Siehe die Formeln in 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

, ab 
\begin_inset Quotes eld
\end_inset

Since we can sample from the true posterior, we can compute the deriva-
 tives of the log probability of the data.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
The Greedy Training Procedure
\end_layout

\begin_layout Standard
The idea is to train a stack of Restricted Boltzmann Machines, where in
 each individual RBM, the hidden nodes infer features derived from the visible
 nodes, and serve as input to be used in the visible layer of the next RBM
 in the stack.
 Training starts with a single RBM, whose visible variables are set to a
 training sample.
 After training it to represent the joint probability distribution of the
 whole training data set, we obtain, for each training sample, the states
 of the hidden nodes.
 These hidden features comprise a new training data set, to be used in the
 next RBM.
 Iteratively deriving new features and using these to train the next RBM,
 we therefore obtain parameters for each RBM in the stack.
\end_layout

\begin_layout Standard
We will now more formally describe the training procedure.
 At the first layer, start with a single RBM containing visible nodes 
\begin_inset Formula $\mathbf{V}$
\end_inset

 and hidden nodes 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 and train it using contrastive divergence to learn the weights 
\begin_inset Formula $W_{0}$
\end_inset

.
 Use these weights in the first layer of the Deep Belief Network.
 Split each of the undirected connections between 
\begin_inset Formula $\mathbf{V}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 into a connection going upwards and one going downwards.
 The upward weights 
\begin_inset Formula $W_{0}^{T}$
\end_inset

 serve the purpose of inferring the 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 representation of the training data, and the downward weights 
\begin_inset Formula $W_{0}$
\end_inset

 are generative and part of the model.
\end_layout

\begin_layout Standard
Then infer a training data set for another RBM on hidden layers 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{1}}$
\end_inset

.
 This RBM needs training data in 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

, but our training samples are for layer 
\begin_inset Formula $\mathbf{V}$
\end_inset

.
 The 
\emph on
re-representation
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
re-representation of training samples
\end_layout

\end_inset

 works by placing a training sample into the 
\begin_inset Formula $\mathbf{V}$
\end_inset

 nodes, and then the upward connections are used to get a new representation
 of the data at 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

.
 This is done for all training samples.
\end_layout

\begin_layout Standard
We now place an RBM between hidden layers 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{1}}$
\end_inset

.
 Up to now the model is equivalent to running the RBM for one more iteration,
 which is implemented by the extra directed layer below the RBM.
 This is because the weights between the two sets of layers are constrained
 to be equal.
 Now 
\begin_inset Quotes eld
\end_inset

untie
\begin_inset Quotes erd
\end_inset

 the upward weights
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
untied weights
\end_layout

\end_inset

 
\begin_inset Formula $W_{0}^{T}$
\end_inset

 between 
\begin_inset Formula $\mathbf{V}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 from the weights 
\begin_inset Formula $W_{i}$
\end_inset

 (where 
\begin_inset Formula $i>0$
\end_inset

), which are constrained to be the same.
 Train the RBM between 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{1}}$
\end_inset

 on the converted training samples, obtaining new weights 
\begin_inset Formula $W_{1}$
\end_inset

.
 As we untied the weights 
\begin_inset Formula $W_{0}^{T}$
\end_inset

 from 
\begin_inset Formula $W_{1}$
\end_inset

, the inferring weights 
\begin_inset Formula $W_{0}^{T}$
\end_inset

 between 
\begin_inset Formula $\mathbf{V}\rightarrow\mathbf{H_{0}}$
\end_inset

 became incorrect in theory.
 In practice, however, this does not matter that much.
 As 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 note, the gain by training the RBM on top of re-represented data outweighs
 the incorrectness of inference.
 They argue that the greedy algorithm is guaranteed to improve the generative
 model, because 
\begin_inset Formula $P(\mathbf{V})$
\end_inset

 has a lower bound that increases (or stays the same for a fully trained
 model) when training an additional layer.
 This guarantee is given only for maximum likelihood Restricted Boltzmann
 Machine learning.
 In practice we use contrastive divergence (
\begin_inset Formula $CD$
\end_inset

) for speed.
 However, the guarantee still holds if we use 
\begin_inset Formula $CD_{k}$
\end_inset

 with a large enough number of iterations 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Standard
The greedy algorithm now proceeds iteratively, i.e.
 the steps of inferring a training data set for a deeper layer and training
 an RBM on this data continue until the model is sufficiently deep.
 Above, we constrained the model to have an equal number of nodes in each
 layer.
 The greedy training procedure also works for layers of different sizes.
 Thus training using gradient descent and approximate Gibbs Sampling, which
 is feasible only for a few layers and variables, can be replaced by the
 tractable greedy algorithm.
\end_layout

\begin_layout Subsubsection
Deep Belief Networks Interpreted as Feed-forward Neural Networks
\end_layout

\begin_layout Paragraph
Pre-training
\end_layout

\begin_layout Standard
A trained DBN can be reinterpreted as a feed-forward neural network.
 In particular, the weights and biases of an unsupervisedly trained DBN
 can be transferred to a multi-layer feed-forward neural network with the
 same architecture as the DBN, thereby making the stochastic DBN a deterministic
 neural network.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
TODO: See 
\begin_inset Quotes eld
\end_inset

An Introduction to Restricted Boltzmann Machines
\begin_inset Quotes erd
\end_inset

 by Asja Fischer and Christian Igel.
 Paragraph starting at 
\begin_inset Quotes eld
\end_inset

It is an important property that single as well as stacked RBMs
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

 The process of training a DBN using stacks of RBMs, and transferring the
 weights and biases is called 
\emph on
pre-training
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
pre-training
\end_layout

\end_inset

.
\end_layout

\begin_layout Paragraph
Fine-tuning
\end_layout

\begin_layout Standard
Furthermore, another neural network can be put on top of the pre-trained
 converted DBN, where the final (output) layer has neurons corresponding
 to variables to be predicted.
 Usually the network consists of only one layer, due to difficulties in
 training freshly initialized multi-layer neural networks.
 The resulting network can then be 
\emph on
fine-tuned
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
fine-tuning
\end_layout

\end_inset

, using standard back-propagation, into a configuration that can predict
 from input variables (input at the bottom of the network) the output variables
 (read off at the top of the network).
\end_layout

\begin_layout Paragraph
Re-representation of the Data
\end_layout

\begin_layout Standard
In such a composite structure, the (unsupervisedly trained) DBN weights
 take on the responsibility of re-representing the data so that it is in
 an abstracted form that is easier to learn on.
 Correlated variables, for example, are represented by a single variable
 indicating whether a feature is present in the sample or not.
 The (supervisedly trained) weights on top of the network have the responsibilit
y to label the sample, i.e.
 indicate whether the abstracted representation of the sample is of a certain
 form or not.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: HIER WEITERMACHEN.
 Erklären, wie wake-sleep funktioniert, oder wie ein DBN mittels stacking
 RBMs gelernt werden kann.
 Wie geht das mit den uncoupled upward weights? Warum optimiert das inference
 Network die falsche Kullback-Leibler divergence?
\end_layout

\end_inset


\end_layout

\end_body
\end_document

#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
% Added by lyx2lyx
\end_preamble
\use_default_options true
\begin_modules
fix-cm
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 2
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Ich glaube ich sollte die model-beschreibungen (directed/undirected
 graphical, RBM, DBN) immer gleich strukturieren: 1.
 structure (i.e.
 die nodes und weights erkl√§ren) 2.
 How to generate data from the model 3.
 How to train a model so that it fits data.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here we will introduce the methods used to train deep neural networks, as
 applied in the results part.
\end_layout

\begin_layout Section
Notation
\end_layout

\begin_layout Description
Random
\begin_inset space ~
\end_inset

variable,
\begin_inset space ~
\end_inset

Node Random variables and nodes are written upper-case.
 For example: 
\begin_inset Formula $X$
\end_inset

 or 
\begin_inset Formula $N_{4}$
\end_inset

.
\end_layout

\begin_layout Description
Value,
\begin_inset space ~
\end_inset

Scalar
\begin_inset space ~
\end_inset

Variable The value of a random variable and a scalar variable are written
 lower-case.
 For example, the value of random variable 
\begin_inset Formula $X$
\end_inset

 is written 
\begin_inset Formula $x$
\end_inset

, and 
\begin_inset Formula $i$
\end_inset

 is a normal variable.
\end_layout

\begin_layout Description
Vector,
\begin_inset space ~
\end_inset

Set Vectors or sets are written in bold font.
 For example, the vector 
\begin_inset Formula $\mathbf{X}$
\end_inset

 could represent the random variables 
\begin_inset Formula $\{X_{1},X_{2},X_{3}\}$
\end_inset

.
 And the vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 could mean the value 
\begin_inset Formula $\{x_{1},x_{2},x_{3}\}$
\end_inset

 of the variable 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
\end_layout

\begin_layout Section
Machine Learning
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe TODO (but I already have a short section on these in the introduction):
 Definition of supervised, unsupervised, and semi-supervised learning.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Generative and Discriminative Models
\end_layout

\begin_layout Standard
An often-cited quote by 
\begin_inset CommandInset citation
LatexCommand cite
key "Vapnik1998"

\end_inset

 is: 
\begin_inset Quotes eld
\end_inset

If you possess a restricted amount of information for solving some problem,
 try to solve the problem directly and never solve a more general problem
 as an intermediate step.
 It is possible that the available information is sufficient for a direct
 solution but is insufficient for solving a more general intermediate problem.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
A generative model
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
generative model
\end_layout

\end_inset

 is such a more general problem: its aim is to model the input data set
 such that hypothetical samples can be generated from the model which might
 as well be found in the original input data set.
 A discriminative model
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
discriminative model
\end_layout

\end_inset

 on the other hand receives the input samples and models the desired output
 values from these input samples.
 Usually the desired output values have much lower dimension than the input
 samples.
\end_layout

\begin_layout Standard
Restricted Boltzmann Machines and Deep Belief Networks are both generative
 models.
 On the other hand, a neural network trained only with back-propagation
 is a discriminative model.
 In a discriminative model, the possible values of the parameters (weights)
 are constrained by the information it takes to specify the class label
 of a training sample.
 In a generative model, however, the parameters need to encode the whole
 sample, and are therefore constrained by the information it takes to encode
 the training sample.
 The number of bits required to specify the class label is usually much
 smaller than the number of bits required to specify a whole training sample
 
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
guideTR.pdf: 
\begin_inset Quotes eld
\end_inset

When learning generative models of high-dimensional data, however, it is
 the number of bits that it takes to specify a data vector that determines
 how much constraint each training case imposes on the parameters of the
 model.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Another advantage of a generative model is that one can draw samples from
 its distribution (
\begin_inset Quotes eld
\end_inset

generate samples
\begin_inset Quotes erd
\end_inset

) to easily find out what the model has learned.
 Due to their greater generality, generative models have the disadvantage
 that they are slower than discriminative models.
 As 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 note, however, the class of too computationally intensive models is being
 eroded by Moore's Law.
\end_layout

\begin_layout Standard
The neural networks will be discussed later.
 Graphical Models will be needed for Restricted Boltzmann Machines and Deep
 Belief Networks, and will be introduced next.
\end_layout

\begin_layout Section
Graphical Models
\end_layout

\begin_layout Subsection
Graphs
\end_layout

\begin_layout Standard
In the following we will define some terms having to do with graphs.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: I think I should write nodes in upper-case in the whole document.
 This is logical since I interchangeably refer to both nodes and random
 variables as the same entity, and random variables are upper-case.
 So write 
\begin_inset Formula $\mathbf{E}\ni e=(N_{1},N_{2})$
\end_inset

 below.
\end_layout

\begin_layout Plain Layout
TODO: I also should write edges upper-case since they are sets with two
 elements, and sets are written upper-case.
 Do this in the whole document.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A 
\emph on
graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
graph
\end_layout

\end_inset

 is a tuple 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 of nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

 and edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

.
 An 
\emph on
edge
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
edge
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{E}\ni E=(N_{1},N_{2})$
\end_inset

 consists of a pair of nodes 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

.
 Two nodes 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

 connected by an edge are called 
\emph on
neighbors
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
neighbor
\end_layout

\end_inset

.
 A 
\emph on
complete graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
complete graph
\end_layout

\end_inset

 is a graph where there exists an edge 
\begin_inset Formula $E=(N_{1},N_{2})$
\end_inset

 for every distinct pair of nodes 
\begin_inset Formula $\mathbf{N}\ni N_{1}\neq N_{2}\in\mathbf{N}$
\end_inset

.
\end_layout

\begin_layout Standard
An edge can be 
\emph on
directed
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
directed edge
\end_layout

\end_inset

 or 
\emph on
undirected
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
undirected edge
\end_layout

\end_inset

, which means that the edge 
\begin_inset Formula $(N_{1},N_{2})$
\end_inset

 is either distinct from the edge 
\begin_inset Formula $(N_{2},N_{1})$
\end_inset

 or they are the same.
 If all edges of a graph are directed, then the graph is called a 
\emph on
directed graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
directed graph
\end_layout

\end_inset

; if all edges are undirected, then the graph is called an 
\emph on
undirected graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
undirected graph
\end_layout

\end_inset

.
 In a directed edge 
\begin_inset Formula $E=(P,C)$
\end_inset

, also written as 
\begin_inset Formula $P\rightarrow C$
\end_inset

, 
\begin_inset Formula $P$
\end_inset

 is called the 
\emph on
parent
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
parent
\end_layout

\end_inset

 and 
\begin_inset Formula $C$
\end_inset

 the 
\emph on
child
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
child
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
A 
\emph on
path
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
path
\end_layout

\end_inset

 is an ordered list of edges 
\begin_inset Formula $\mathbf{P}=[E_{1},E_{2},\dots,E_{n}]$
\end_inset

, so that the child of the previous edge is the parent of the next edge:
 If 
\begin_inset Formula $E_{i}=(P_{i},C_{i})$
\end_inset

 and 
\begin_inset Formula $E_{i+1}=(P_{i+1},C_{i+1})$
\end_inset

, then 
\begin_inset Formula $C_{i}=P_{i+1}$
\end_inset

.
 In the path 
\begin_inset Formula $\mathbf{P}$
\end_inset

, 
\begin_inset Formula $P_{p}$
\end_inset

 is called 
\emph on
ancestor
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
ancestor
\end_layout

\end_inset

 of 
\begin_inset Formula $C_{c}$
\end_inset

 if 
\begin_inset Formula $p\leq c$
\end_inset

, and 
\begin_inset Formula $C_{c}$
\end_inset

 is called  
\emph on
descendant
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
descendant
\end_layout

\end_inset

 of 
\begin_inset Formula $P_{p}$
\end_inset

 if 
\begin_inset Formula $p\leq c$
\end_inset

.
 If the child 
\begin_inset Formula $C_{j}$
\end_inset

 of any edge 
\begin_inset Formula $E_{j}$
\end_inset

 in 
\begin_inset Formula $\mathbf{P}$
\end_inset

 is equal to the parent 
\begin_inset Formula $P_{i}$
\end_inset

 of the same or a previous edge (i.e.
 
\begin_inset Formula $i\leq j$
\end_inset

), then the sub-path 
\begin_inset Formula $\mathbf{C}=[(P_{i},C_{i}),(P_{i+1},C_{i+1}),\dots,(P_{j},C_{j})]$
\end_inset

 is called a 
\emph on
cycle
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
cycle
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
A 
\emph on
directed acyclic graph
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
directed acyclic graph
\end_layout

\end_inset

 (DAG
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DAG
\end_layout

\end_inset

) is a directed graph that does not contain directed cycles.
\end_layout

\begin_layout Standard
A 
\emph on
clique
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
clique
\end_layout

\end_inset

 in an undirected graph is a subset of nodes 
\begin_inset Formula $\mathbf{N_{C}}\subset\mathbf{N}$
\end_inset

, such that every pair of nodes in the clique 
\begin_inset Formula $N_{1},N_{2}\in\mathbf{N_{C}}$
\end_inset

 is an edge in the graph: 
\begin_inset Formula $(N_{1},N_{2})\in\mathbf{E}$
\end_inset

.
 A 
\emph on
maximal clique
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
maximal clique
\end_layout

\end_inset

 is a clique where there are no edges that can be added to it so that the
 resulting subset of nodes is still a clique.
\end_layout

\begin_layout Standard
A set of nodes 
\begin_inset Formula $\mathbf{N_{A}}\subset\mathbf{N}$
\end_inset

 is 
\emph on
separated
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
separation of nodes
\end_layout

\end_inset

 from a set of nodes 
\begin_inset Formula $\mathbf{N_{B}}\subset\mathbf{N}$
\end_inset

 by a set of nodes 
\begin_inset Formula $\mathbf{N_{S}}\subset\mathbf{N}$
\end_inset

, if it is impossible to go (along the edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

 of the graph) from a node 
\begin_inset Formula $N_{1}\in\mathbf{N_{A}}$
\end_inset

 to a node 
\begin_inset Formula $N_{2}\in\mathbf{N_{B}}$
\end_inset

 without passing through any of the nodes in 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

.
\end_layout

\begin_layout Subsection
Definition of Graphical Models
\end_layout

\begin_layout Standard
Graphical models are an encoding of a factorizable joint probability distributio
n with the help of a graph.
 There are different types of graphical models, but there are defining elements
 of all graphical models.
 Each node of the graph corresponds to a random variable of the joint probabilit
y distribution.
 The edges of the graph encode the conditional probability distributions.
 The graph, together with probability functions over the structural elements
 of the graph is equivalent to the joint probability distribution.
\end_layout

\begin_layout Paragraph
Directed and Undirected Graphical Models
\end_layout

\begin_layout Standard
In the following, we will introduce and discuss directed graphical models
 and undirected graphical models.
 Directed graphical models are also called 
\emph on
Bayesian networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Bayesian network
\end_layout

\end_inset

 or 
\emph on
Belief Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Belief network
\end_layout

\end_inset

, and undirected graphical models are also called 
\emph on
Markov random fields
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov random field
\end_layout

\end_inset

 or 
\emph on
Markov Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov network
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
There is also an unification of Bayesian networks and Markov random fields,
 i.e.
 a graphical model that can have both directed and undirected edges.
 These networks are called 
\emph on
chain graphs
\emph default
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
probably not (since I won't go into chain graphs): reference.
 Referenz zu chain graph ist 
\begin_inset Quotes eld
\end_inset

Chain graph models and their causal interpretations Steffen L.
 Lauritzen Aalborg University, Denmark and Thomas S.
 Richardson
\begin_inset Quotes erd
\end_inset

 oder http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html (
\begin_inset Quotes erd
\end_inset

It is possible to have a model with both directed and undirected arcs, which
 is called a chain graph.
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_inset

, or 
\emph on
partially directed acyclic graphs
\emph default
 and are not discussed here.
\end_layout

\end_inset

.
 We will consider only variables with discrete values in the joint probability
 distribution.
\end_layout

\begin_layout Subsubsection
Undirected Graphical Models
\begin_inset CommandInset label
LatexCommand label
name "par:Hammersley-Clifford-theorem"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Hier mu√ü eine formale Definition von 
\begin_inset Quotes eld
\end_inset

undirected graphical model
\begin_inset Quotes erd
\end_inset

 stehen: d.h.
 Erkl√§re wie man aus dem graph zusammen mit den potentials (die √ºber die
 Cliquen des Graphs definiert sind) die joint probability distribution berechnen
 kann.
 (das produkt der potentials ist die joint).
\end_layout

\begin_layout Plain Layout
I should at some point write that in undirected graphical models, the probabilit
y is factored into potentials (written 
\begin_inset Formula $\phi$
\end_inset

), where each factor corresponds to a clique in the undirected graph.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We want to encode a joint probability distribution 
\begin_inset Formula $P$
\end_inset

 in an undirected graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

.
 Every random variable corresponds to a node.
 The missing edges encode conditional independencies.
 A complete graph would encode no conditional independencies.
 However, to minimize computation time we normally want to get a minimal
 graph that still encodes the joint probability distribution faithfully.
 What properties does the joint probability distribution 
\begin_inset Formula $P$
\end_inset

 have to fulfill and what does the minimal undirected graph 
\begin_inset Formula $G$
\end_inset

 look like?
\end_layout

\begin_layout Paragraph
The Hammersley-Clifford theorem
\begin_inset CommandInset label
LatexCommand label
name "par:The-Hammersley-Clifford-theorem-of-Undirected-Graphical-Model"

\end_inset


\end_layout

\begin_layout Standard
A formal definition is provided by the 
\emph on
Hammersley-Clifford theorem
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Hammersley-Clifford theorem
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "HammersleyClifford1971"

\end_inset

.
 It states that when we have a vector of random variables 
\begin_inset Formula $\mathbf{N}=\{N_{1},\dots,N_{n}\}$
\end_inset

, its strictly positive joint probability distribution 
\begin_inset Formula $P(\mathbf{N})$
\end_inset

 with 
\begin_inset Formula $P(\mathbf{n})>0$
\end_inset

 for all possible values 
\begin_inset Formula $\mathbf{n}$
\end_inset

 of 
\series bold

\begin_inset Formula $\mathbf{N}$
\end_inset


\series default
, and an undirected graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 with each node corresponding to a random variable (i.e.
 
\begin_inset Formula $\mathbf{N}=\{N_{1},\dots,N_{n}\}$
\end_inset

), then the following statements are equivalent:
\end_layout

\begin_layout Itemize
\begin_inset Formula $P(\mathbf{N})$
\end_inset

 is a 
\emph on
Gibbs distribution
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Gibbs distribution
\end_layout

\end_inset

 that factorizes according to the maximal cliques 
\begin_inset Formula $\mathbf{C_{1}},\dots,\mathbf{C_{m}}$
\end_inset

 in 
\begin_inset Formula $G$
\end_inset

, i.e.
 
\begin_inset Formula $P(\mathbf{N})=\frac{1}{Z}\phi_{1}(\mathbf{C_{1}})\cdot\ldots\cdot\phi_{m}(\mathbf{C_{m}})$
\end_inset

 where 
\begin_inset Formula $Z$
\end_inset

 is the 
\emph on
partition function
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
partition function
\end_layout

\end_inset

 such that 
\begin_inset Formula $\sum_{\mathbf{n}}P(\mathbf{N}=\mathbf{n})=1$
\end_inset

, and 
\begin_inset Formula $\phi_{i}(\mathbf{C_{i}})$
\end_inset

 are the 
\emph on
potential functions
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
potential function
\end_layout

\end_inset

, which depend only on the states of the random variables in the clique
 
\begin_inset Formula $\mathbf{C_{i}}=(N_{i_{1}},\dots,N_{i_{n}})$
\end_inset

 and must be positive for all possible states.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
On page 21 of the Hammersley-Clifford paper, the authors define the name
 
\begin_inset Quotes eld
\end_inset

light-coloured potential function
\begin_inset Quotes erd
\end_inset

.
 There, they also define 
\begin_inset Quotes eld
\end_inset

Gibbsian ensemble
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset CommandInset label
LatexCommand label
name "local-Markov-property"

\end_inset

the 
\emph on
local Markov property
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
local Markov property
\end_layout

\end_inset

 holds for the graph 
\begin_inset Formula $G$
\end_inset

 and the joint probability distribution 
\begin_inset Formula $P$
\end_inset

: A node 
\begin_inset Formula $N_{i}$
\end_inset

 is conditionally independent from all non-neighbor nodes 
\begin_inset Formula $\mathbf{N}\backslash\mathbf{N_{neighbor(i)}}$
\end_inset

, given the states of the random variables 
\begin_inset Formula $\mathbf{N_{neighbor(i)}}$
\end_inset

 immediately connected to 
\begin_inset Formula $N$
\end_inset

: 
\begin_inset Formula $P(N_{i}\mid\mathbf{N_{neighbor(i)}})=P(N_{i}\mid\mathbf{N})$
\end_inset

.
\end_layout

\begin_layout Itemize
the 
\emph on
global Markov property
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
global Markov property
\end_layout

\end_inset

 holds for the graph 
\begin_inset Formula $G$
\end_inset

 and the joint probability distribution 
\begin_inset Formula $P$
\end_inset

: Given any disjoint subsets 
\begin_inset Formula $\mathbf{N_{A}},\mathbf{N_{B}},\mathbf{N_{S}}\subset\mathbf{N}$
\end_inset

 where 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

 separates the nodes 
\begin_inset Formula $\mathbf{N_{A}}$
\end_inset

 from the nodes 
\begin_inset Formula $\mathbf{N_{B}}$
\end_inset

, and given the states of the random variables of 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

, the nodes 
\begin_inset Formula $\mathbf{N_{A}}$
\end_inset

 are conditionally independent of the nodes 
\begin_inset Formula $\mathbf{N_{B}}$
\end_inset

: 
\begin_inset Formula $P(\mathbf{N_{A}}\mid\mathbf{N_{S}})=P(\mathbf{N_{A}\mid}\mathbf{N_{S}},\mathbf{N_{B}})$
\end_inset

.
\end_layout

\begin_layout Standard
This means that when we have a strictly positive joint probability distribution
 
\begin_inset Formula $P$
\end_inset

, we can determine the conditional independencies by considering the local
 or global Markov property, and construct the minimal undirected graph 
\begin_inset Formula $G$
\end_inset

 representing 
\begin_inset Formula $P$
\end_inset

 by starting from the completely connected graph and deleting the edges
 
\begin_inset Formula $E_{12}=(N_{1},N_{2})$
\end_inset

 whose ends are conditionally independent 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
NOTE: conditionally independent on what? I think that's the crux of being
 NP; we probably have to enumerate all possible separators...?
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
On the other hand, if we have a graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 consisting of a given set of nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

 and edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

, and local conditional probabilities 
\begin_inset Formula $P(N\mid\mathbf{N}_{\mathbf{Parents}})$
\end_inset

 at each node fulfilling the Markov property, then we can derive from that
 the joint probability distribution over all random variables, or equivalently
 over all nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Example of an Undirected Graphical Model
\begin_inset CommandInset label
LatexCommand label
name "par:Example-of-an-Undirected-Graphical-Model"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/undirected-graphical-model-example.dia
	width 45col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-an-Undirected-Graph"

\end_inset

Example of an Undirected Graph encoding the following conditional independencies
: the node pairs (N1,N4), (N1,N5), (N2,N4), (N2,N5),(N4,N5) are conditionally
 independent given node N3.
 (But (N1,N2) are not conditionally independent given N3.)
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
probably don't add potential functions, since I cannot show local and global
 Markov property.
 Potentials would be defined from the conditional independencies of the
 graph, and confirming them by computation from the potentials would be
 self-fulfilling, since they were defined from the structure of the graph.
\end_layout

\begin_layout Plain Layout
Der to do Eintrag war mal: In 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Example-of-an-Undirected-Graphical-Model"

\end_inset

 eine Figure mit einem MRF: insert example graphics with an undirected graph
 with node names.
 The figure text should denote the maximal cliques text and illustrate the
 local Markov property for a node 
\begin_inset Formula $N$
\end_inset

 and the global Markov property for two nodes 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

 separated by a set of nodes 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

.
 Maybe also print definitions of the potential functions for the maximal
 cliques (but these are probably too large).
 The graphics could be similar to Figure 2.3 in 
\begin_inset Quotes eld
\end_inset

~/uni/publication/zusammenfassung/graphical model/Koller+al_SRL07.pdf
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For example, if there are the three cliques 
\begin_inset Formula $\mathbf{C_{1}}=\{N_{1},N_{2},N_{3}\}$
\end_inset

, 
\begin_inset Formula $\mathbf{C_{2}}=\{N_{3},N_{4}\}$
\end_inset

, and 
\begin_inset Formula $\mathbf{C_{3}}=\{N_{3},N_{5}\}$
\end_inset

 (see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-an-Undirected-Graph"

\end_inset

), then the joint probability distribution 
\begin_inset Formula $P$
\end_inset

 (also called 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Gibbs distribution
\end_layout

\end_inset

Gibbs distribution) can be written as: 
\begin_inset Formula 
\[
P(N_{1},\dots,N_{5})=\frac{1}{Z}\phi_{1}(\mathbf{C_{1}})\phi_{2}(\mathbf{C_{2}})\phi_{3}(\mathbf{C_{3}}),
\]

\end_inset

where 
\begin_inset Formula $\phi_{1}(\mathbf{C_{1}})=\phi(N_{1},N_{2},N_{3})$
\end_inset

 is the potential function of clique 1 (
\emph on
clique potential
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
clique potential
\end_layout

\end_inset

), and is a function of the 3 random variables 
\begin_inset Formula $N_{1},N_{2},N_{3}$
\end_inset

 in the clique.
 
\begin_inset Formula $Z$
\end_inset

 is the partition function and must normalize the function so that 
\begin_inset Formula $P$
\end_inset

 is a probability: 
\begin_inset Formula 
\[
Z=\sum_{X_{1},\dots,X_{n}}\phi_{1}(\mathbf{C_{1}})\phi_{2}(\mathbf{C_{2}})\phi_{3}(\mathbf{C_{3}}).
\]

\end_inset


\end_layout

\begin_layout Standard
In practice, 
\begin_inset Formula $\phi_{1}(N_{1},N_{2},N_{3})$
\end_inset

 can be represented by a table that has, for each possible combination of
 states of the three random variables, a positive real number.
 For example, if each of the three random variables has two states, then
 the table (with 
\begin_inset Formula $2^{3}$
\end_inset

 entries) could look like in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Example-of-potential-function"

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n_{3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\phi(N_{1}=n_{1},N_{2}=n_{2},N_{3}=n_{3})$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.124
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.553
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.842
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.258
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Example-of-potential-function"

\end_inset

Example of a potential function 
\begin_inset Formula $\phi$
\end_inset

 represented as a table.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Directed Graphical Models
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/directed-graphical-model-example.dia
	width 30col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-a-Directed-Graph"

\end_inset

Example of a Directed Graph.
 Note that the graph is acyclic, and the nodes are layed out in layers.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Directed graphical models are also called 
\emph on
Bayesian Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Bayesian network
\end_layout

\end_inset

 or 
\emph on
Belief Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Belief network
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Like undirected graphical models, directed graphical models represent an
 implicit joint probability distribution over all random variables present
 in the model.
 The graph must be directed and acyclic, and each node of the graph is associate
d with a random variable.
\end_layout

\begin_layout Standard
A directed graphical model is defined by the directed graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

, a prior probability distribution 
\begin_inset Formula $P(N_{noparents})$
\end_inset

 at the nodes that do not have parents 
\begin_inset Formula $\mathbf{N_{noparents}}$
\end_inset

, and conditional probability distributions 
\begin_inset Formula $P(N_{hasparents}\mid\mathbf{N_{parents}})$
\end_inset

 at the nodes that have parents 
\begin_inset Formula $\mathbf{N_{hasparents}}$
\end_inset

.
 In the latter nodes the conditional probability distribution may only condition
 on its immediate parents, not on distant ancestors.
 The directed graph encodes the set of conditional independencies between
 the random variables: A random variable 
\begin_inset Formula $X$
\end_inset

 of the graph is conditionally independent of all random variables that
 are not descendants of 
\begin_inset Formula $X$
\end_inset

 (i.e.
 
\begin_inset Formula $\mathbf{N}\backslash\mathbf{N_{descendant(\mathbf{\mathrm{X}})}}$
\end_inset

) given the values of the parent variables of 
\begin_inset Formula $X$
\end_inset

 (this is called the 
\emph on
local Markov property
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
local Markov property
\end_layout

\end_inset

 of directed graphs):
\begin_inset Formula 
\[
X\perp(\mathbf{N}\backslash\mathbf{N_{descendant(\mathrm{X})}})\mid\mathbf{N_{parent(\mathrm{X})}}.
\]

\end_inset


\end_layout

\begin_layout Paragraph
The Joint Probability Distribution Encoded by the Graphical Model
\begin_inset CommandInset label
LatexCommand label
name "par:The-Joint-Encoded-by-Directed-Graphical-Model"

\end_inset


\end_layout

\begin_layout Standard
In a directed graphical model, the random variables 
\begin_inset Formula $\mathbf{N}$
\end_inset

 can be totally ordered such that 
\begin_inset Formula $N_{i}$
\end_inset

 comes before 
\begin_inset Formula $N_{j}$
\end_inset

 if there is a directed path from 
\begin_inset Formula $N_{i}$
\end_inset

 to 
\begin_inset Formula $N_{j}$
\end_inset

 in the graph, and the order is unspecified if there is no directed path
 between 
\begin_inset Formula $N_{i}$
\end_inset

 and 
\begin_inset Formula $N_{j}$
\end_inset

.
 (This means there are graphs which have more than one compatible ordering,
 for example in the graph 
\begin_inset Formula $A\rightarrow C\leftarrow B$
\end_inset

, the ordering can be 
\begin_inset Formula $A,B,C$
\end_inset

 as well as 
\begin_inset Formula $B,A,C$
\end_inset

.) In the following the ordering is expressed as the subscript 
\begin_inset Formula $i\in\mathbb{N}$
\end_inset

 of the random variable 
\begin_inset Formula $N_{i}$
\end_inset

.
 The node 
\begin_inset Formula $N_{i}$
\end_inset

 has associated with it the conditional probability 
\begin_inset Formula $P(N_{i}=n_{i}\mid N_{j}=n_{j}\forall j<i)$
\end_inset

.
 (
\begin_inset Quotes eld
\end_inset

The probability that the random variable 
\begin_inset Formula $N_{i}$
\end_inset

 is equal to 
\begin_inset Formula $n_{i}$
\end_inset

 given that the random variables 
\series bold

\begin_inset Formula $N_{j}$
\end_inset

 
\series default
are equal to 
\begin_inset Formula $n_{j}$
\end_inset

 where all subscripts 
\begin_inset Formula $j$
\end_inset

 that are smaller than 
\begin_inset Formula $i$
\end_inset

.
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Standard
The joint probability distribution encoded by the graph is
\begin_inset Formula 
\[
P(\mathbf{N})=P(N_{1}=n_{1},N_{2}=n_{2},\dots,N_{n}=n_{n})=\prod_{i=1}^{n}P(N_{i}=n_{i}\mid N_{j}=n_{j}\forall j<i).
\]

\end_inset


\end_layout

\begin_layout Paragraph
Example How to Represent the Conditional Probability Distribution
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe TODO: hier muss ein Beispiel mit einer Tabelle einer joint probability
 distribution stehen.
\end_layout

\end_inset

For example, if the 
\begin_inset Formula $N_{i}$
\end_inset

 can only assume discrete 
\begin_inset Formula $n_{i}$
\end_inset

, the conditional probability distribution can be represented by a table
 with a size exponential in the number of involved nodes.
 When a node has 
\begin_inset Formula $a$
\end_inset

 ancestors, each of which has 
\begin_inset Formula $s$
\end_inset

 possible states, and the node itself also has 
\begin_inset Formula $s$
\end_inset

 possible states, then the table must contain one probability for each of
 the 
\begin_inset Formula $s^{a}\cdot(s-1)$
\end_inset

 possible states.
 (
\begin_inset Formula $s^{a}$
\end_inset

 for the combinations of the values of the ancestors and 
\begin_inset Formula $(s-1)$
\end_inset

 for the values the node itself can assume.)
\end_layout

\begin_layout Subsection
Exact Inference in Graphical Models
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Do 
\begin_inset Formula $\mathbf{K},\mathbf{W},\mathbf{U}$
\end_inset

 have to fulfill some relationship for directed acyclic graphs in the inference
 example below? (For example, does 
\begin_inset Formula $\mathbf{K}$
\end_inset

 have to be a subset of the parents of 
\series bold

\begin_inset Formula $\mathbf{W}$
\end_inset


\series default
?)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
NOTE: Maybe I should explain the algorithm 
\begin_inset Quotes eld
\end_inset

ELIMINATE
\begin_inset Quotes erd
\end_inset

 from 
\begin_inset Quotes eld
\end_inset

~/uni/publication/zusammenfassung/graphical model/inference/jordan.pdf
\begin_inset Quotes erd
\end_inset

 and exercise it for an instance of a directed and undirected graph.
 This would be more systematic than it is now.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Inference in a graphical model is the task of answering a query about the
 joint probability distribution encoded by the graph, or of a part of the
 joint probability distribution.
 For example, one might be interested in the overall probability of the
 configuration of a sub-set of variables.
 However, in the general case this takes exponential time.
\end_layout

\begin_layout Standard
We will now discuss a naive approach to inference.
\end_layout

\begin_layout Subsubsection
Naive Approach: Marginalizing the Joint 
\begin_inset CommandInset label
LatexCommand label
name "par:Exact-Inference-in-Directed-and-Undirected Graphical Models"

\end_inset


\end_layout

\begin_layout Standard
Since a graphical model is a representation of a joint probability distribution,
 it can answer queries about probabilities of the joint probability distribution
 encoded by the graphical model by first explicitly calculating the joint,
 then marginalizing out non-interesting variables.
\end_layout

\begin_layout Standard
How to calculate the joint encoded by the graphical model was defined in
 
\begin_inset CommandInset ref
LatexCommand vref
reference "par:The-Hammersley-Clifford-theorem-of-Undirected-Graphical-Model"

\end_inset

 for undirected graphical models and in 
\begin_inset CommandInset ref
LatexCommand vref
reference "par:The-Joint-Encoded-by-Directed-Graphical-Model"

\end_inset

 for directed graphical models.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: I'm not sure I know how 
\begin_inset Quotes eld
\end_inset

Inference
\begin_inset Quotes erd
\end_inset

 is defined.
 I should look it up in a book, e.g.
 
\begin_inset Quotes eld
\end_inset

Probabilistic Graphical Model, by Koller and Friedman
\begin_inset Quotes erd
\end_inset

.
 But that book is not downloadable.
\end_layout

\begin_layout Plain Layout
There is a paper, however: http://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pd
f
\end_layout

\end_inset


\end_layout

\begin_layout Standard
For example, we might want to infer the probability of a configuration of
 variables when the values of only some of the variables are known.
 This means that we can partition the variables 
\begin_inset Formula $\mathbf{V}$
\end_inset

 of a graphical model into three disjoint groups:
\end_layout

\begin_layout Enumerate
the known variables 
\begin_inset Formula $\mathbf{K}$
\end_inset

,
\end_layout

\begin_layout Enumerate
the unknown variables 
\begin_inset Formula $\mathbf{W}$
\end_inset

 that we want to know the probability distribution of,
\end_layout

\begin_layout Enumerate
the unknown variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

 that we do not care about.
\end_layout

\begin_layout Standard
Let the known values of 
\begin_inset Formula $\mathbf{K}$
\end_inset

 be written 
\series bold

\begin_inset Formula $\mathbf{k}$
\end_inset


\series default
.
 The unknown values of 
\begin_inset Formula $\mathbf{W}$
\end_inset

 are named 
\series bold

\begin_inset Formula $\mathbf{w}$
\end_inset


\series default
, and the values of 
\begin_inset Formula $\mathbf{U}$
\end_inset

, 
\begin_inset Formula $\mathbf{u}$
\end_inset

.
 
\end_layout

\begin_layout Standard
When we want to find the probability of configuration 
\begin_inset Formula $\mathbf{W}=\mathbf{w}$
\end_inset

, given 
\begin_inset Formula $\mathbf{K}=\mathbf{k}$
\end_inset

, we can first write the query in terms of the joint probability distribution.
 After that we marginalize out the unknown variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

 that we do not care about, and condition on 
\begin_inset Formula $\mathbf{K}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray}
P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k}) & = & \sum_{\mathbf{U}}P(\mathbf{W}=\mathbf{w},\mathbf{U}=\mathbf{u}|\mathbf{K}=\mathbf{k})\nonumber \\
 & = & \sum_{\mathbf{U}}\frac{P(\mathbf{W}=\mathbf{w},\mathbf{U}=\mathbf{u},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}.\label{eq:Inference in graphical models}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In the above formula there is a sum over all variables 
\series bold

\begin_inset Formula $\mathbf{U}$
\end_inset


\series default
.
 Writing this out, we obtain 
\begin_inset Formula 
\begin{eqnarray}
P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k})\nonumber \\
= & \sum_{\mathbf{U}}\frac{P(\mathbf{W}=\mathbf{w},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\nonumber \\
= & \sum_{U_{1}}\sum_{U_{2}}\cdots\sum_{U_{n}}\frac{P(\mathbf{W}=\mathbf{w},U_{1}=u_{1},U_{2}=u_{2},\dots,U_{n}=u_{n},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}.\label{eq:Inference in graphical models, written out}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In the general case (if the joint probability cannot be factorized), this
 nested sum needs 
\begin_inset Formula $O(|\mathbf{u}|^{|\mathbf{U}|})=O(|\mathbf{u}|^{n})$
\end_inset

 operations to compute, where 
\begin_inset Formula $|\mathbf{u}|$
\end_inset

 is the number of possible values a variable 
\begin_inset Formula $U_{i}$
\end_inset

 can have (assuming for simplicity that all random variables 
\begin_inset Formula $U_{i}$
\end_inset

 have the same number of possible values 
\begin_inset Formula $|\mathbf{u}|$
\end_inset

) and 
\begin_inset Formula $|\mathbf{U}|$
\end_inset

 is the number of unknown variables 
\begin_inset Formula $U_{i}$
\end_inset

.
 This is because all possible combinations of variable assignments have
 to be considered.
 Thus, for this naive marginalization run-time is exponential in the number
 of variables, and therefore intractable.
\end_layout

\begin_layout Standard
However, in this example we have not considered the structure of the graph.
 We can improve run-time in some cases of graphs and for some sets of variables
 
\begin_inset Formula $\mathbf{K}$
\end_inset

, 
\begin_inset Formula $\mathbf{W}$
\end_inset

, 
\begin_inset Formula $\mathbf{U}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Factorization in Undirected Graphical Models (Markov Random Fields)
\end_layout

\begin_layout Standard
In the case of an undirected graphical model, this can be improved by factorizin
g the joint probability into independent sub-joint-probabilities, if possible.
 For example, if the random variables 
\begin_inset Formula $\mathbf{U}=\{U_{1},U_{2},\dots,U_{m}\}$
\end_inset

 are composed of cliques 
\begin_inset Formula $\mathbf{C_{1}},\mathbf{C_{2}},\dots,\mathbf{C_{n}}$
\end_inset

, so that 
\begin_inset Formula $P(\mathbf{U})=\frac{1}{Z}\phi_{1}(\mathbf{C_{1}})\phi_{2}(\mathbf{C_{2}})\dots\phi_{n}(\mathbf{C_{n}})$
\end_inset

, then the above sum can be written, using Hammersley-Clifford, as
\begin_inset Formula 
\begin{eqnarray*}
 & P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k})\\
= & \sum_{U_{1}}\sum_{U_{2}}\cdots\sum_{U_{n}}\frac{P(\mathbf{W}=\mathbf{w},U_{1}=u_{1},U_{2}=u_{2},\dots,U_{n}=u_{n},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\\
= & \sum_{\mathbf{C}_{1}}\cdots\sum_{\mathbf{C}_{n}\backslash\{C_{1},\dots,C_{n-1}\}}\frac{\frac{1}{Z}\mbox{\phi}_{1}(\mathbf{W}=\mathbf{w},\mathbf{C}_{1}=\mathbf{c}_{1},\mathbf{K}=\mathbf{k})\cdot\ldots\cdot\mbox{\phi}_{n}(\mathbf{W}=\mathbf{w},\mathbf{C}_{n}=\mathbf{c}_{n},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\\
= & \frac{\frac{1}{Z}\left(\sum_{\mathbf{C}_{1}}\mbox{\phi}_{1}(\mathbf{W}=\mathbf{w},\mathbf{C}_{1}=\mathbf{c}_{1},\mathbf{K}=\mathbf{k})\cdot\ldots\cdot\left(\sum_{\mathbf{C}_{n}\backslash\{C_{1},\dots,C_{n-1}\}}\mbox{\phi}_{n}(\mathbf{W}=\mathbf{w},\mathbf{C}_{n}=\mathbf{c}_{n},\mathbf{K}=\mathbf{k})\right)\right)}{P(\mathbf{K}=\mathbf{k})}.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The sums in the last line are nested sums that sum over all possible states
 in the corresponding cluster.
 (If we order cliques descendingly by the number of clique members then
 
\begin_inset Formula $\mathbf{C_{1}}$
\end_inset

 is the largest clique.) Because we still need to sum over the state combinations
 in the largest clique, this means that the run-time is at least 
\begin_inset Formula $O(|\mathbf{u}|^{|\mathbf{C_{m}}|})$
\end_inset

, where 
\begin_inset Formula $\mathbf{C_{m}}$
\end_inset

 is the clique with the largest number of variables in it.
 This is still an exponential run-time.
\end_layout

\begin_layout Standard
An algorithm that systematizes exact inference in graphical model is the
 
\emph on
junction tree
\emph default
 
\emph on
algorithm
\emph default
.
 (It is not discussed here, see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Jordan2004"

\end_inset

.)
\end_layout

\begin_layout Subsubsection
Example of Exact Inference in a Directed Graphical Model
\begin_inset CommandInset label
LatexCommand label
name "sub:Example-of-Exact-Inference-in-a-directed-graphical-model"

\end_inset


\end_layout

\begin_layout Standard
Here we show an example how inference in a specific directed graphical model
 is done.
 The directed graphical model shall be composed of several densely connected
 layers, where the nodes within a layer are not connected, and they have
 outgoing directed connections only to nodes in the adjacent layer below.
 
\end_layout

\begin_layout Standard
When the probability distribution of the parent nodes are known, inferring
 the probability distributions of child nodes is easy: just multiply the
 probability of the parents with the conditional probability of the child.
 To keep the example interesting, given the probability distributions of
 the nodes in the bottom layer, we want to infer the probability distributions
 for all the other nodes.
 We want to determine the exact probability distributions, hence the term
 
\begin_inset Quotes eld
\end_inset

exact inference
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Paragraph
Deep Belief Networks
\end_layout

\begin_layout Standard
For example let a directed acyclic graph be defined by the following directed
 connections between its nodes: 
\begin_inset Formula $G_{1}\rightarrow H_{1}$
\end_inset

, 
\begin_inset Formula $G_{1}\rightarrow H_{2}$
\end_inset

, 
\begin_inset Formula $G_{2}\rightarrow H_{1}$
\end_inset

, 
\begin_inset Formula $G_{2}\rightarrow H_{2}$
\end_inset

, 
\begin_inset Formula $H_{1}\rightarrow V_{1}$
\end_inset

, 
\begin_inset Formula $H_{1}\rightarrow V_{2}$
\end_inset

, 
\begin_inset Formula $H_{2}\rightarrow V_{1}$
\end_inset

, 
\begin_inset Formula $H_{2}\rightarrow V_{2}$
\end_inset

.
 Such a graph is displayed in figure 
\begin_inset CommandInset ref
LatexCommand vref
reference "fig:Example-of-a-Directed-Graph"

\end_inset

.
 Furthermore the following conditional probability distributions are given:
 
\begin_inset Formula $P(H_{1}|G_{1},G_{2})$
\end_inset

, 
\begin_inset Formula $P(H_{2}|G_{1},G_{2})$
\end_inset

, 
\begin_inset Formula $P(V_{1}|H_{1},H_{2})$
\end_inset

, 
\begin_inset Formula $P(V_{2}|H_{1},H_{2})$
\end_inset

.
 This layered architecture, where each node in a layer is connected to all
 nodes in adjacent layers, defines a directed graphical model called 
\emph on
Deep Belief Network
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset

.
 (We will discuss them in section  
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Deep-Belief-Network"

\end_inset

.)
\end_layout

\begin_layout Paragraph
Posterior = Likelihood * Prior
\end_layout

\begin_layout Standard
Now assume that 
\begin_inset Formula $P(V_{1})$
\end_inset

, 
\begin_inset Formula $P(V_{2})$
\end_inset

 are given and we want to infer 
\begin_inset Formula $P(G_{1}\mid\mathbf{V})$
\end_inset

, 
\begin_inset Formula $P(G_{2}\mid\mathbf{V})$
\end_inset

, 
\begin_inset Formula $P(H_{1}\mid\mathbf{V})$
\end_inset

, 
\begin_inset Formula $P(H_{2}\mid\mathbf{V})$
\end_inset

.
 Using Bayes' Theorem (
\begin_inset Formula $\mbox{posterior}=\mbox{likelihood}*\mbox{prior}$
\end_inset

) we get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
P(H_{1},H_{2}|V,V_{2}) & = & \frac{P(V_{1},V_{2}|H_{1},H_{2})P(H_{1},H_{2})}{P(V_{1},V_{2})}\nonumber \\
 & = & \frac{P(V_{1},V_{2}|H_{1},H_{2})\left(\sum_{g_{1}}\sum_{g_{2}}P(g_{1})P(g_{2})P(H_{1}|g_{1},g_{2})P(H_{2}|g_{1},g_{2})\right)}{P(V_{1},V_{2})}\nonumber \\
 & = & \frac{1}{P(V_{1},V_{2})}*P(V_{1}|H_{1},H_{2})P(V_{2}|H_{1},H_{2})\nonumber \\
 &  & *\left(\sum_{g_{1}}\sum_{g_{2}}P(g_{1})P(g_{2})P(H_{1}|g_{1},g_{2})P(H_{2}|g_{1},g_{2})\right)\label{eq:deep-network-conditional-probability}
\end{eqnarray}

\end_inset


\begin_inset Formula 
\[
P(V_{1},V_{2})=\sum_{h_{1}}\sum_{h_{2}}P(V_{1}|H_{1}=h_{1},H_{2}=h_{2})P(V_{2}|H_{1}=h_{1},H_{2}=h_{2})P(H_{1}=h_{1},H_{2}=h_{2}).
\]

\end_inset

We can make the last transformation because 
\begin_inset Formula $\ensuremath{V_{1}}$
\end_inset

 and 
\begin_inset Formula $\ensuremath{V_{2}}$
\end_inset

 are independent given 
\begin_inset Formula $\ensuremath{H_{1}},\ensuremath{H_{2}}$
\end_inset

 (local Markov property).
 To determine 
\begin_inset Formula $P(H_{1}|V_{1},V_{2})$
\end_inset

 and 
\begin_inset Formula $P(H_{2}|V_{1},V_{2})$
\end_inset

, we have to marginalize the other variable in 
\begin_inset Formula $\mathbf{H}$
\end_inset

 out:
\begin_inset Formula 
\begin{eqnarray*}
P(H_{1}|V_{1},V_{2}) & = & \sum_{h_{2}}P(H_{1},H_{2}=h_{2}|V_{1},V_{2})\\
P(H_{2}|V_{1},V_{2}) & = & \sum_{h_{1}}P(H_{1}=h_{1},H_{2}|V_{1},V_{2})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Since we now have 
\begin_inset Formula $P(H_{1}|\mathbf{V})$
\end_inset

, using 
\begin_inset Formula $P(H_{1})=\sum_{v_{1}}\sum_{v_{2}}P(H_{1},V_{1}=v_{1},V_{2}=v_{2})$
\end_inset

 and 
\begin_inset Formula $P(H_{1}|V_{1},V_{2})=P(H_{1},V_{1},V_{2})/P(V_{1},V_{2})$
\end_inset

, we can determine 
\begin_inset Formula $P(H_{1})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(H_{1}) & = & \sum_{v_{1}}\sum_{v_{2}}P(H_{1}|V_{1}=v_{1},V_{2}=v_{2})P(V_{1}=v_{1},V_{2}=v_{2}).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(H_{2})$
\end_inset

 can be computed similarly.
 Now that we know 
\begin_inset Formula $P(H_{1})$
\end_inset

 and 
\begin_inset Formula $P(H_{2})$
\end_inset

, we can repeat the steps to determine 
\begin_inset Formula $P(G_{1}\mid\mathbf{H})$
\end_inset

 and 
\begin_inset Formula $P(G_{2}\mid\mathbf{H})$
\end_inset

.
\end_layout

\begin_layout Paragraph
Exact Inference in Deep Belief Networks is Complicated
\begin_inset CommandInset label
LatexCommand label
name "par:Exact-Inference-in-Deep-Belief-Networks-is-Complicated"

\end_inset


\end_layout

\begin_layout Standard
This shows that exact inference in a directed graphical model with densely
 connected layers is complicated.
 If there are 
\begin_inset Formula $n$
\end_inset

 binary variables in 
\begin_inset Formula $\mathbf{H}$
\end_inset

 and 
\begin_inset Formula $\mathbf{G}$
\end_inset

, then the computation of 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

 takes 
\begin_inset Formula $O(2^{n-1}n)$
\end_inset

 due to having to marginalize out all variables in 
\begin_inset Formula $\mathbf{H}$
\end_inset

 except one (the term 
\begin_inset Formula $2^{n-1}$
\end_inset

), and this for all variables (the term 
\begin_inset Formula $n$
\end_inset

).
 In addition, this applies only if 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

 is known already.
 But in the computation of 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

, equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:deep-network-conditional-probability"

\end_inset

, there are sums over the variables 
\begin_inset Formula $g_{1}$
\end_inset

 and 
\begin_inset Formula $g_{2}$
\end_inset

, which takes another 
\begin_inset Formula $O(2^{n})$
\end_inset

.
 The phenomenon that leads to this computational problem is called 
\emph on
explaining away
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
explaining away
\end_layout

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: 
\begin_inset Quotes eld
\end_inset

~/uni/publication/zusammenfassung/graphical model/explaining_away/pami93.pdf
\begin_inset Quotes erd
\end_inset

 says that explaining away can not only decrease but also INCREASE belief:
 (in the abstract) 
\begin_inset Quotes eld
\end_inset

The opposite of explaining away also can occur, where the confirmation of
 one cause increases belief in another.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The posterior of 
\begin_inset Formula $H_{1}$
\end_inset

 depends on all conditional probabilities of the model, in this example,
 
\begin_inset Formula $P(\mathbf{V}\mid\mathbf{H})$
\end_inset

 and 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{G})$
\end_inset

.
 For Deep Belief Networks, which are a kind of Directed Graphical Model
 with densely connected layers, the conditional probabilities 
\begin_inset Formula $P(\mathbf{V}\mid\mathbf{H})$
\end_inset

 and 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{G})$
\end_inset

 have parameters called 
\begin_inset Quotes eld
\end_inset

weights
\begin_inset Quotes erd
\end_inset

 associated with them, and inference of the layer immediately above 
\begin_inset Formula $\mathbf{V}$
\end_inset

, namely 
\begin_inset Formula $\mathbf{H}$
\end_inset

, requires knowing all weights in the graph, not just those of 
\begin_inset Formula $P(\mathbf{V}\mid\mathbf{H})$
\end_inset

.
 In addition, explaining away requires us to marginalize out all variables
 in 
\begin_inset Formula $\mathbf{H}$
\end_inset

 except one, and this for all variables in 
\begin_inset Formula $\mathbf{H}$
\end_inset

.
 A further problem is that we have to integrate over all variables in all
 layers above 
\begin_inset Formula $\mathbf{H}$
\end_inset

 if we are interested in 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

.
\end_layout

\begin_layout Standard
These procedures becomes infeasible in a Belief Network with more than a
 few parents per node.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
probably not TODO since we don't need them for later: 
\end_layout

\begin_layout Subsubsection
General Algorithm
\end_layout

\begin_layout Plain Layout
In the following paragraph we summarize the ELIMINATE algorithm from 
\begin_inset CommandInset citation
LatexCommand cite
key "JordanWeiss2002"

\end_inset

.
 This algorithm works for both directed and undirected graphical models
 and computes the marginal probabilities of all nodes.
\end_layout

\begin_layout Paragraph
The Junction-tree Algorithm
\end_layout

\begin_layout Plain Layout
The run-time can be shortened by marginalizing variables out as early as
 possible.
 This must be done according to the factorization imposed by the graph structure.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Intractability of Exact Inference
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "ChandrasekaranHarsha2012"

\end_inset

 showed that under some assumptions, and given that arbitrary potential
 functions should be allowed, 
\emph on
treewidth
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
treewidth
\end_layout

\end_inset

 is the only structural property that ensures polynomial run-time of exact
 inference on arbitrary graphs
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
the paper says on page 1: 
\begin_inset Quotes eld
\end_inset

Among these problems is inference in graphi- cal models, which, as mentioned
 earlier, can be solved in polynomial-time if the treewidth of the underly-
 ing graphs is bounded.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
For a triangulated graph, the treewidth is the number of nodes contained
 in the largest clique minus one.
 For a graph of any form, the treewidth is the treewidth of the triangulation
 that minimizes the treewidth.
 For directed acyclic graphs, the number of parents is the critical number,
 since it determines the treewidth of the moralized
\begin_inset Foot
status open

\begin_layout Plain Layout
You obtain a moralized graph from a directed acyclic graph by introducing
 edges between all parents of a node, and then replacing directed edges
 by undirected edges.
\end_layout

\end_inset

 graph.
 (This was also shown by 
\begin_inset CommandInset citation
LatexCommand cite
key "KwisthoutVanderGaag2010"

\end_inset

).
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
NOTE: Stimmt das Folgende? Ich dachte, man k√∂nnte das aufgrund der Struktur
 von Formel (1) im Paper sagen, aber glaube gerade, dass das nicht reicht:
 
\begin_inset Quotes eld
\end_inset

For directed graphs, factorization according to the edges of the graph serves
 the purpose of cliques in the proof.
 Thus, the cliques are defined on the moral graph of the directed graph.
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The assumptions of the proof are that 
\begin_inset Formula $\mathbf{NP}\nsubseteq\mathbf{P/poly}$
\end_inset

, which is a version of the 
\begin_inset Formula $\mathbf{NP}\neq\mathbf{P}$
\end_inset

 assumption, and that the grid-minor hypothesis is true.
 (For an explanation of the grid-minor hypothesis, see Section 2.4 in 
\begin_inset CommandInset citation
LatexCommand cite
key "ChandrasekaranHarsha2012"

\end_inset

.)
\end_layout

\begin_layout Standard
If these assumptions are correct then the only way to reduce the computational
 cost of exact inference on a general graph is to reduce
\begin_inset Note Note
status open

\begin_layout Plain Layout
Page 2, section 
\begin_inset Quotes eld
\end_inset

Main Result
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

 the treewidth or to choose particularly simple potential functions
\begin_inset Note Note
status open

\begin_layout Plain Layout
Page 2, section 
\begin_inset Quotes eld
\end_inset

Main Result
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Quotes eld
\end_inset

there exists a choice of potential functions ...
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
 Therefore, in practice, the joint probability is approximated, for example
 by Gibbs sampling.
\end_layout

\begin_layout Subsection
Approximate Inference
\end_layout

\begin_layout Standard
Approximate Inference in general graphical models by means of Gibbs Sampling
 was first described by 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

.
 To describe this, we first have to introduce Markov Chains.
\end_layout

\begin_layout Subsubsection
Markov Chains
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Bild von einer Markov Chain.
 Evtl.
 von einer Markov Chain, an der ich die verschiedenen Eigenschaften wie
 period, irreducible, etc.
 zeigen kann.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Markov property: Memorylessness
\end_layout

\begin_layout Standard
A 
\emph on
Markov Chain
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov Chain
\end_layout

\end_inset

 is a time sequence of random variables 
\begin_inset Formula $X_{t}$
\end_inset

, where 
\begin_inset Formula $t\in\mathbb{N}_{0}$
\end_inset

 denotes the discrete index of time.
 In a Markov Chain, each random variable 
\begin_inset Formula $X_{t}$
\end_inset

 may depend only on the state of the random variable at the immediate previous
 time point 
\begin_inset Formula $t-1$
\end_inset

, i.e.
 
\begin_inset Formula 
\[
P(X_{t}=x_{t}\mid X_{0}=x_{0},\dots,X_{t-1}=x_{t-1})=P(X_{t}=x_{t}\mid X_{t-1}=x_{t-1})
\]

\end_inset

must hold for all 
\begin_inset Formula $t\geq1$
\end_inset

.
 This memorylessness is called the 
\emph on
Markov property
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov property
\end_layout

\end_inset

.
 In a Markov Chain, possible states 
\begin_inset Formula $x_{t}$
\end_inset

 at each time point are discrete and from the same set 
\begin_inset Formula $\mathbb{S}$
\end_inset

.
 (I.e.
 
\begin_inset Formula $x_{t}\in\mathbb{S}$
\end_inset

 for all 
\begin_inset Formula $t$
\end_inset

.) When the state a Markov Chain is in is not specific to a certain time
 point 
\begin_inset Formula $t$
\end_inset

, we will also use single-letter variables like 
\begin_inset Formula $i\in\mathbb{S}$
\end_inset

 and 
\begin_inset Formula $j\in\mathbb{S}$
\end_inset

 to denote the state, e.g.
 
\begin_inset Formula $X_{1}=i$
\end_inset

.
\end_layout

\begin_layout Paragraph
Time-homogeneous Markov Chain and Transition Matrix
\end_layout

\begin_layout Standard
A 
\emph on
time-homogeneous Markov Chain
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
time-homogeneous Markov Chain
\end_layout

\end_inset

 is a Markov Chain in which the conditional probability 
\begin_inset Formula $P(X_{t}=x_{t}|X_{t-1}=x_{t-1})$
\end_inset

 is the same for all time points 
\begin_inset Formula $t$
\end_inset

, i.e.
 
\begin_inset Formula 
\[
P(X_{t}=x_{t}\mid X_{t-1}=x_{t-1})=P(X_{t-1}=x_{t-1}\mid X_{t-2}=x_{t-2})
\]

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
for all time points 
\begin_inset Formula $t\geq2$
\end_inset

.
 If this is the case, then this conditional probability
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: In the following, do I have to replace 
\begin_inset Formula $p$
\end_inset

 with 
\begin_inset Formula $P$
\end_inset

, because it is a matrix?
\end_layout

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula 
\[
P(X_{t}=j\mid X_{t-1}=i)\eqqcolon p_{ij}
\]

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 is independent of the current time 
\begin_inset Formula $t$
\end_inset

 and can be written as the 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
two-dimensional
\end_layout

\end_inset

matrix 
\begin_inset Formula $p$
\end_inset

, called the 
\family default
\series default
\shape default
\size default
\emph on
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
transition matrix
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
transition matrix
\end_layout

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
 In the following we will only be dealing with time-homogeneous Markov chains.
\end_layout

\begin_layout Paragraph
Computing future states from the initial distribution
\end_layout

\begin_layout Standard
It makes sense to talk about the distribution of states a Markov chain can
 have at a certain time point 
\begin_inset Formula $t$
\end_inset

.
 Let it be named the 
\emph on
probability vector
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
probability vector
\end_layout

\end_inset

 
\begin_inset Formula $d^{(t)}$
\end_inset

, a row vector with its length equal to 
\begin_inset Formula $|\mathbb{S}|$
\end_inset

 (the number of different possible states of the random variables 
\begin_inset Formula $X_{t}$
\end_inset

) and its entry 
\begin_inset Formula $d_{i}^{(t)}=P(X_{t}=x_{i})$
\end_inset

 equal to the probability of 
\begin_inset Formula $X_{t}$
\end_inset

 having state 
\begin_inset Formula $x_{i}$
\end_inset

.
 (This means 
\begin_inset Formula $\sum_{i}d_{i}^{(t)}=1$
\end_inset

 for all 
\begin_inset Formula $t$
\end_inset

.) The distribution at the next time point 
\begin_inset Formula $t+1$
\end_inset

 can be computed by matrix multiplication from 
\begin_inset Formula $d^{(t)}$
\end_inset

 and the transition matrix 
\begin_inset Formula $(p_{ji})$
\end_inset

: 
\begin_inset Formula 
\[
d^{(t+1)}=d^{(t)}p
\]

\end_inset


\end_layout

\begin_layout Standard
Given an initial distribution 
\begin_inset Formula $d^{(0)}$
\end_inset

 and the transition matrix 
\begin_inset Formula $p$
\end_inset

, the probabilities of all states the Markov chain can assume are defined
 for all time points 
\begin_inset Formula $t\geq0$
\end_inset

: 
\begin_inset Formula 
\[
d^{(t)}=d^{(0)}p^{t}.
\]

\end_inset


\end_layout

\begin_layout Paragraph
Stationary distribution
\end_layout

\begin_layout Standard
There are time-homogeneous Markov chains whose state distribution stays
 constant once it has assumed a certain state distribution.
 Such state distributions are called 
\emph on
invariant
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
invariant distribution
\end_layout

\end_inset

 or 
\emph on
stationary distribution
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
stationary distribution
\end_layout

\end_inset

.
 A stationary distribution 
\begin_inset Formula $\pi$
\end_inset

 must fulfill the following equation: 
\begin_inset Formula 
\[
\pi p=\pi
\]

\end_inset

If 
\begin_inset Formula $d^{(t)}=\pi$
\end_inset

, then 
\begin_inset Formula $d^{(t+u)}=\pi$
\end_inset

 for all 
\begin_inset Formula $u\geq0$
\end_inset

.
 A Markov Chain can have more than one stationary distribution.
\end_layout

\begin_layout Paragraph
Detailed balance/Reversibility
\end_layout

\begin_layout Standard
A Markov chain with transition matrix 
\begin_inset Formula $p$
\end_inset

 satisfies 
\emph on
detailed balance
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
detailed balance
\end_layout

\end_inset


\emph on
 
\emph default
if there exists a probability distribution 
\begin_inset Formula $\pi=(\pi_{1},\pi_{2},\dots\pi_{n})$
\end_inset

 such that 
\begin_inset Formula 
\[
\pi_{j}p_{ji}=\pi_{i}p_{ij}\,\,\forall i,j.
\]

\end_inset

Such a Markov chain is also
\begin_inset Note Note
status open

\begin_layout Plain Layout
Is this true? In 
\begin_inset Quotes eld
\end_inset

neal.pdf
\begin_inset Quotes erd
\end_inset

, it says 
\begin_inset Quotes eld
\end_inset

Often, we will use time reversible homogeneous Markov chains that satisfy
 the 
\series bold
more restrictive 
\series default
condition of detailed balance
\begin_inset Quotes erd
\end_inset

.
 So Markov chains satisfying detailed balance seem to be a subset of the
 Markov chains satisfying time reversibility.
\end_layout

\end_inset

 called a 
\emph on
reversible
\emph default
 Markov chain
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
reversible Markov chain
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Norris1997"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Theorem 1.9.3
\end_layout

\end_inset

.
 A Markov chain with the detailed balance property 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: why is detailed balance not enough for a Markov chain to have an unique
 stationary distribution in AltRBM-proof.pdf? why do we need irreducibility
 and aperiodicity?
\end_layout

\begin_layout Plain Layout
irreducibility has to be shown so that we can be sure that the markov chain
 has an unique stationary distribution.
\end_layout

\begin_layout Plain Layout
aperiodicity: 
\begin_inset Quotes eld
\end_inset

AltRBM-proof.pdf
\begin_inset Quotes erd
\end_inset

 says 
\begin_inset Quotes eld
\end_inset

One can show that an irreducible and aperiodic Markov chain on a finite
 state space is guarantied to converge to its stationary distribution (see,
 e.g., [6]).
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

has at least one stationary distribution, where the stationary distributions
 fulfill the 
\begin_inset Formula $\pi$
\end_inset

 from the detailed balance condition 
\begin_inset CommandInset citation
LatexCommand cite
key "Norris1997"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Lemma 1.9.2
\end_layout

\end_inset

.
 While having detailed balance implies that a Markov chain has a stationary
 distribution, the reverse is not true: there are Markov chains with a stationar
y distribution but not satisfying detailed balance.
 (For example, a Markov chain with transition probabilities 
\begin_inset Formula $p_{ij}=p_{jk}=p_{ki}=1$
\end_inset

 for 
\begin_inset Formula $i\neq j$
\end_inset

, 
\begin_inset Formula $j\neq k$
\end_inset

, 
\begin_inset Formula $k\neq i$
\end_inset

, and the other entries of the transition matrix equal to 0 does not satisfy
 detailed balance, but 
\begin_inset Formula $\pi_{i}=\pi_{j}=\pi_{k}=\frac{1}{3}$
\end_inset

 is a stationary distribution of this Markov chain.)
\end_layout

\begin_layout Paragraph
Fundamental Theorem of Markov Chains
\begin_inset CommandInset label
LatexCommand label
name "par:Fundamental-Theorem-of-Markov-Chains"

\end_inset


\end_layout

\begin_layout Standard
Under what conditions does a Markov chain have an unique stationary distribution
? This is answered by the 
\emph on
Fundamental Theorem of Markov Chains
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Fundamental Theorem of Markov Chains
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: which was proved by Kolmogorov?
\end_layout

\end_inset

 (refer to e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Behrends2000"

\end_inset

).
 It states that, in the long run (i.e.
 
\begin_inset Formula $\lim_{t\rightarrow\infty}d^{(0)}p^{t}$
\end_inset

), the probability vector converges to the unique stationary distribution
 irrespective of the starting state 
\begin_inset Formula $d^{(0)}$
\end_inset

, given that 
\begin_inset Formula $p$
\end_inset

 is irreducible, positive-recurrent and aperiodic
\begin_inset Note Note
status open

\begin_layout Plain Layout
Theorem 7.4 in ~/uni/publication/zusammenfassung/markov chains/markovbuch2008.pdf.
\end_layout

\end_inset

.
 The definitions of irreducibility, positive recurrence, and aperiodicity
 follow.
\end_layout

\begin_layout Paragraph
Irreducibility
\end_layout

\begin_layout Standard
A Markov Chain is called 
\emph on
irreducible
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
irreducible
\end_layout

\end_inset

, if it is possible to go from any state 
\begin_inset Formula $i$
\end_inset

 of the Markov Chain to any state 
\begin_inset Formula $j$
\end_inset

 (possibly in more than 1 steps).
 Formally, a Markov Chain is called irreducible, if its states are all in
 the same (and only) closed subset.
 A subset 
\begin_inset Formula $C$
\end_inset

 of 
\begin_inset Formula $\mathbb{S}$
\end_inset

 is called 
\emph on
closed 
\emph default
if 
\begin_inset Formula $p_{ij}=0$
\end_inset

 whenever 
\begin_inset Formula $i\in C$
\end_inset

 and 
\begin_inset Formula $j\notin C$
\end_inset

.
 (Remember that 
\begin_inset Formula $\mathbb{S}$
\end_inset

 is the set of possible states of the Markov Chain.)
\end_layout

\begin_layout Paragraph
Positive Recurrence
\end_layout

\begin_layout Standard
A state 
\begin_inset Formula $i$
\end_inset

 of a Markov Chain is 
\emph on
positive recurrent
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
positive recurrent
\end_layout

\end_inset

, if we expect the Markov Chain to take an finite number of steps until
 it is in state 
\begin_inset Formula $i$
\end_inset

 again, when it started in state 
\begin_inset Formula $i$
\end_inset

 at time point 
\begin_inset Formula $0$
\end_inset

.
 To define positive recurrence formally, we have to define auxiliary measures
 first.
 The probability that state 
\begin_inset Formula $j$
\end_inset

 is visited at time step 
\begin_inset Formula $k$
\end_inset

 for the first time after the Markov Chain had been in state 
\begin_inset Formula $i$
\end_inset

 at time point 
\begin_inset Formula $0$
\end_inset

 is 
\begin_inset Formula 
\[
f_{ij}^{(k)}:=P(X_{1}\neq j,X_{2}\neq j,\dots,X_{k-1}\neq j,X_{k}=j\mid X_{0}=i).
\]

\end_inset

The probability that state 
\begin_inset Formula $j$
\end_inset

 is ever reached from state 
\begin_inset Formula $i$
\end_inset

 is 
\begin_inset Formula 
\[
f_{ij}^{*}:=\sum_{k=1}^{\infty}f_{ij}^{(k)}.
\]

\end_inset

With this, we can define the expected number of steps for the Markov Chain
 to reach state 
\begin_inset Formula $j$
\end_inset

, when starting at state 
\begin_inset Formula $i$
\end_inset

:
\begin_inset Formula 
\[
\mu_{ij}:=\sum_{k=1}^{\infty}kf_{ij}^{(k)}.
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
I won't use 
\begin_inset CommandInset citation
LatexCommand cite
key "Behrends2000"

\end_inset

 for positive recurrence, because he defines (on page 46 of markovbuch2008.pdf)
 
\begin_inset Formula $\mu_{ij}:=\sum_{k=1}^{\infty}kf_{ij}^{(k)}+(1-f_{ij}^{*})\infty$
\end_inset

, which is not defined for 
\begin_inset Formula $f_{ij}^{*}=1$
\end_inset

.
 I don't understand his definition.
\end_layout

\end_inset

This number is also called the 
\emph on
mean recurrence time
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
mean recurrence time
\end_layout

\end_inset

.
 With these definitions, we can define a state to be 
\emph on
transient
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
transient
\end_layout

\end_inset

, positive recurrent, or 
\emph on
null recurrent
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
null recurrent
\end_layout

\end_inset

:
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $f_{ii}^{*}<1$
\end_inset

, the state 
\begin_inset Formula $i$
\end_inset

 is called transient.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $f_{ii}^{*}=1$
\end_inset

, the state 
\begin_inset Formula $i$
\end_inset

 is called recurrent.
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $f_{ii}^{*}=1$
\end_inset

 and 
\begin_inset Formula $\mu_{ii}<\infty$
\end_inset

, the state 
\begin_inset Formula $i$
\end_inset

 is called 
\emph on
positive recurrent
\emph default
.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $f_{ii}^{*}=1$
\end_inset

 and 
\begin_inset Formula $\mu_{ii}=\infty$
\end_inset

, the state 
\begin_inset Formula $i$
\end_inset

 is called 
\emph on
null recurrent
\emph default
.
\end_layout

\end_deeper
\begin_layout Standard
It can be proven that when there are finitely many states, there are no
 null recurrent states (see Proposition 7.2.
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Behrends2000"

\end_inset

).
\end_layout

\begin_layout Paragraph
Aperiodicity
\end_layout

\begin_layout Standard
The definition of an 
\emph on
aperiodic
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
aperiodic
\end_layout

\end_inset

 state is shorter.
 The period of a state
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
period of a state
\end_layout

\end_inset

 
\begin_inset Formula $i$
\end_inset

 is defined as the greatest common denominator of the number of time steps
 needed for a Markov chain so that it is possible to be in state 
\begin_inset Formula $i$
\end_inset

 again, after it was in state 
\begin_inset Formula $i$
\end_inset

 before: 
\begin_inset Formula 
\[
period(i)=gcd(\{k\mid k\geq0,(p^{k})_{ii}>0\}).
\]

\end_inset

If 
\begin_inset Formula $period(i)=1$
\end_inset

, state 
\begin_inset Formula $i$
\end_inset

 is called 
\emph on
aperiodic
\emph default
.
 If all states of a Markov chain are aperiodic, the Markov chain is called
 aperiodic.
\end_layout

\begin_layout Paragraph
Markov Chain Monte Carlo (MCMC)
\end_layout

\begin_layout Standard

\emph on
Markov Chain Monte Carlo
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Markov Chain Monte Carlo algorithm
\end_layout

\end_inset

 is an algorithm to sample from a multivariate probability distribution.
 It sets up a Markov Chain that has the desired stationary distribution
 and iterates it through time until the stationary distribution is approximated
 sufficiently.
\end_layout

\begin_layout Standard
A trick can be useful.
 To efficiently calculate the stationary distribution 
\begin_inset Formula $\pi$
\end_inset

, compute only the transition matrix for time steps that are a power of
 2, i.e.
 
\begin_inset Formula $p^{2^{t}}$
\end_inset

.
 This can be done by starting with 
\begin_inset Formula $p^{1}:=p$
\end_inset

, repeated squaring: 
\begin_inset Formula $p^{2t}:=p^{t}p^{t}$
\end_inset

, and assigning the stationary distribution 
\begin_inset Formula $\pi=d^{(0)}p^{2^{t}}$
\end_inset

 for a large enough 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
MCMC will be described in the next section.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: And also write down how the target distribution is specified.
 At this point I think that this depends on the specific instance of MCMC,
 e.g.
 Gibbs Sampling needs conditional probabilities.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Gibbs Sampling
\end_layout

\begin_layout Standard
Here, we will introduce Gibbs Sampling to show how 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 used it to approximate inference in directed and undirected graphical models.
\end_layout

\begin_layout Standard

\emph on
Gibbs Sampling
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Gibbs sampling
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "GemanGeman1984"

\end_inset

 is an instance of a Markov Chain Monte Carlo (MCMC) algorithm.
 Its goal is to generate samples from a multivariate joint probability distribut
ion, without having to know its closed form.
 The generated samples can then be used to compute an approximation of the
 mean of a distribution, for example.
\end_layout

\begin_layout Standard
To use a Gibbs Sampler, one must construct a Markov chain with its (only)
 stationary distribution equal to the target distribution.
 Because its goal is to sample from a multivariate probability distribution,
 each of the random variables are updated in turn, based on the conditional
 probabilities.
\end_layout

\begin_layout Paragraph
Gibbs Sampling Requires Closed-form Conditional Probabilities
\end_layout

\begin_layout Standard
This means that a precondition to being able to use a Gibbs Sampler is that
 the following conditional probabilities of the target distribution are
 known, so that they can be evaluated.
 Suppose that the multivariate target distribution is 
\begin_inset Formula $P(X_{1},\dots,X_{n})$
\end_inset

.
 Then for each random variable 
\begin_inset Formula $X_{i}\in\{X_{1},\dots,X_{n}\}$
\end_inset

 the conditional probability of the variable given all other variables must
 be known in closed form: 
\begin_inset Formula 
\[
P(X_{i}\mid\mathbf{X_{j}},j\in\{1,\dots,n\}\backslash i)=P(X_{i}\mid X_{1},\dots,X_{i-1},X_{i+1},X_{n}).
\]

\end_inset

(Another prerequisite is that Gibbs Sampling, which requires sampling from
 the conditional probability distribution and iterating, should be faster
 than sampling from the joint target probability distribution directly.
 We already showed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Exact-Inference-in-Deep-Belief-Networks-is-Complicated"

\end_inset

 that exact inference is infeasible in Deep Belief Networks.)
\end_layout

\begin_layout Paragraph
A Markov Chain in Multiple Dimensions
\end_layout

\begin_layout Standard
The variable updates in Gibbs sampling can be regarded as a Markov chain.
 However, we must first define how the random variables of Gibbs sampling
 are mapped to the random variable of the Markov chain.
 One theoretic possibility is to re-map the random variables and their states
 to a single random variable.
\end_layout

\begin_layout Standard
Above, Markov chains were defined for a single state variable 
\begin_inset Formula $X$
\end_inset

.
 But in Gibbs sampling there are usually more than one variable, written
 
\begin_inset Formula $X_{i}\in\{X_{1},\dots,X_{n}\}$
\end_inset

 above.
 If there are a finite number of random variables in Gibbs sampling, i.e.,
 if 
\begin_inset Formula $n$
\end_inset

 is finite, and the state space of these variables is also finite, say of
 size 
\begin_inset Formula $m$
\end_inset

, then the (therefore also finite) number of states of these variables can
 be encoded in a single variable with a state space of size 
\begin_inset Formula $m^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
For example, say there are 3 variables, each of which can assume 2 states,
 and we want to encode these 
\begin_inset Formula $2^{3}$
\end_inset

 different possible states in 1 variable.
 Then we have 1 variable with 
\begin_inset Formula $2*2*2$
\end_inset

 different possible states.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Constructing a Markov Chain from Base Transitions
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
hier schreiben, dass die Markov-Kette aus dem Produkt der einzelnen Variable-Upd
ates (
\begin_inset Quotes eld
\end_inset

base transitions
\begin_inset Quotes erd
\end_inset

) gebildet wird.
 Siehe Neal93, Kapitel 4.1 (Seite 47).
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Neal schreibt auf Seite 44 ganz unten: 
\begin_inset Quotes eld
\end_inset

For example, each 
\begin_inset Formula $B_{k}$
\end_inset

 might change only some subset of the variables making up the state.
\begin_inset Quotes erd
\end_inset

 Also definiert er anscheinend Markov-Ketten mit mehrdimensionalem state
 vector?
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 suggests
\begin_inset Note Note
status open

\begin_layout Plain Layout
on page 45
\end_layout

\end_inset

 constructing a non-homogeneous Markov Chain by applying base transitions
 in turn, each of which describe the probability of a state change of one
 random variable.
 The base transitions are named 
\begin_inset Formula $B_{k}(x,x')$
\end_inset

, where 
\begin_inset Formula $k\in1,2,\ldots,s$
\end_inset

 is the number of the base transition, 
\begin_inset Formula $x$
\end_inset

 is the starting state of the transition, 
\begin_inset Formula $x'$
\end_inset

 is the target state of the transition, and the whole term 
\begin_inset Formula $B_{k}(x,x')$
\end_inset

 is the probability of the transition, which must be strictly greater than
 zero for all values of 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $x'$
\end_inset

, to make the Markov chain irreducible.
 At each time-point 
\begin_inset Formula $a*s+k-1$
\end_inset

 with 
\begin_inset Formula $a\in\mathbf{\mathbb{N}}$
\end_inset

, the next single base transition 
\begin_inset Formula $B_{k}(x,x')$
\end_inset

 is then applied:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
T_{as+k-1}(x,x')=B_{k}(x,x').
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 also notes that the required properties for a Markov chain to converge
 are fulfilled: If each of the base transitions 
\begin_inset Formula $B_{k}$
\end_inset

 have a stationary distribution, then the non-homogeneous 
\begin_inset Formula $T$
\end_inset

 also has a stationary distribution.
\begin_inset Note Note
status open

\begin_layout Plain Layout
vielleicht TODO: Hinzuf√ºgen, dass Neal auf Seite 45 schreibt: 
\begin_inset Quotes eld
\end_inset

Unlike the case with mixtures, though, even when all the B k satisfy detailed
 balance, T generally does not.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Initialization of the Gibbs Sampler
\end_layout

\begin_layout Standard
A Gibbs Sampler starts by specifying a start value 
\begin_inset Formula $x_{i}^{(0)}$
\end_inset

 for each random variable 
\begin_inset Formula $X_{i}$
\end_inset

.
 Since the Markov chain must be constructed such that it converges to its
 only stationary distribution the choice of start values is not critical,
 but it influences the numbers of iterations needed until the Gibbs Sampler
 returns samples from the target distribution.
 So the start value should be close to the 
\begin_inset Quotes eld
\end_inset

center of mass
\begin_inset Quotes erd
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
i.e.
 the expected value
\end_layout

\end_inset

 of the distribution.
\end_layout

\begin_layout Paragraph
Iterating
\end_layout

\begin_layout Standard
Then an iterative process is started.
 In each iteration 
\begin_inset Formula $t$
\end_inset

, each random variable 
\begin_inset Formula $X_{i}$
\end_inset

 is updated by sampling a new value 
\begin_inset Formula $x_{i}^{(t)}$
\end_inset

 from the conditional probability distribution 
\begin_inset Formula 
\begin{equation}
P(X_{i}\mid X_{1}=x_{1}^{(t-1)},\dots,X_{i-1}=x_{i-1}^{(t-1)},X_{i+1}=x_{i+1}^{(t-1)},\dots,X_{n}=x_{n}^{(t-1)}).\label{eq:Gibbs sampling: CPD}
\end{equation}

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
 Then all the other variables 
\begin_inset Formula $X_{i}$
\end_inset

 must be updated.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are different alternative ways in which the random variables are updated,
 for example updating the random variables can be done in random order,
 or sequentially, or a whole 
\begin_inset Quotes eld
\end_inset

block
\begin_inset Quotes erd
\end_inset

 of multiple random variables can be sampled from the conditional distribution
 given all the other random variables (e.g.
 
\begin_inset Formula $P(X_{i_{1}},X_{i_{2}},X_{i_{3}}\mid\mathbf{X_{j}=x_{j}^{(t-1)}},j\in\{1,\dots,n\}\backslash\{i_{1},i_{2},i_{3}\})$
\end_inset

).
\end_layout

\begin_layout Standard
If the Markov chain fulfills irreducibility, positive recurrence, and aperiodici
ty, then it is guaranteed to converge to its stationary distribution (see
 section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Fundamental-Theorem-of-Markov-Chains"

\end_inset

).
 However there is currently no known analytic method to determine when the
 Markov chain has done so, which leads to the following two practices, 
\emph on
burn-in
\emph default
 and 
\emph on
thinning
\emph default
.
\end_layout

\begin_layout Paragraph
Burn-in Period
\end_layout

\begin_layout Standard
The starting value of the random variable might be far from the 
\begin_inset Quotes eld
\end_inset

center
\begin_inset Quotes erd
\end_inset

 of the distribution.
 But after some number of throw-away iterations, the Gibbs Sampler's values
 
\begin_inset Formula $\mathbf{x}=(x_{1},\dots,x_{n})$
\end_inset

 will start coming from the target joint distribution 
\begin_inset Formula $P(X_{1},\dots,X_{n})$
\end_inset

.
 This 
\begin_inset Quotes eld
\end_inset

some number of throw-away iterations
\begin_inset Quotes erd
\end_inset

 is called the 
\emph on
burn-in period
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
burn-in period
\end_layout

\end_inset

 and can be considerable depending on the starting values and the joint
 probability distribution underlying the conditional probability distributions.
 If one knows where the 
\begin_inset Quotes eld
\end_inset

center
\begin_inset Quotes erd
\end_inset

 of the equilibrium distribution is then one should use a value near that
 center as the starting point, however, in many cases such things are not
 known (and are the goal of Gibbs sampling in the first place).
\end_layout

\begin_layout Standard
There is no known analytic method to determine when a chain is burned-in.
 Several convergence diagnostics methods have been proposed, see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "CowlesCarlin1996"

\end_inset

 for a review.
\end_layout

\begin_layout Paragraph
Thinning
\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
thinning
\end_layout

\end_inset

Even after the Markov chain is burned in, there is still a problem with
 the returned samples, which prevent them from being used in those applications
 needing 
\emph on
independent 
\emph default
samples.
 The samples of two adjacent time steps 
\begin_inset Formula $\mathbf{x}^{(t)}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}^{(t+1)}$
\end_inset

 are correlated however because the latter is dependent on the former (by
 definition).
\end_layout

\begin_layout Standard
This can be mitigated by returning only the states of every 
\begin_inset Formula $n$
\end_inset

th iteration, where 
\begin_inset Formula $n$
\end_inset

 is a sufficiently large number.
 This is called 
\begin_inset Quotes eld
\end_inset

thinning
\begin_inset Quotes erd
\end_inset

 of the Gibbs sampler.
\end_layout

\begin_layout Standard
Again, there is currently no straightforward analytic way to determine what
 a sufficiently large 
\begin_inset Formula $n$
\end_inset

 is for the adjacent 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to be regarded independent.
 In practice one resorts to heuristics like autocorrelation.
\end_layout

\begin_layout Subsubsection
Inference in Markov Random Fields
\begin_inset CommandInset label
LatexCommand label
name "sub:Inference-in-Markov-Random-Fields"

\end_inset


\end_layout

\begin_layout Paragraph
Gibbs Sampling in Markov Random Fields
\begin_inset CommandInset label
LatexCommand label
name "par:Gibbs-Sampling-in-Markov-Random-Fields"

\end_inset


\end_layout

\begin_layout Standard
In section 
\begin_inset CommandInset ref
LatexCommand vref
reference "par:Exact-Inference-in-Directed-and-Undirected Graphical Models"

\end_inset

 we described how exact inference in Markov Random Fields can be done by
 conditioning on the known random variables and marginalizing out the uninterest
ing variables.
 It was stated there that the exponential runtime of exact inference can
 be circumvented by approximate methods like Gibbs sampling.
 Here we address that claim.
\end_layout

\begin_layout Standard
The random variables of the Markov Random Field will be named like those
 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Exact-Inference-in-Directed-and-Undirected Graphical Models"

\end_inset

.
 The interesting variables are named 
\begin_inset Formula $\mathbf{W}$
\end_inset

, the variables whose states are known (for example because they were measured)
 
\begin_inset Formula $\mathbf{K}$
\end_inset

, and the variables we are not interested in 
\begin_inset Formula $\mathbf{U}$
\end_inset

.
 The union of all random variables is called 
\begin_inset Formula $\mathbf{X}:=\mathbf{U}\cup\mathbf{K}\cup\mathbf{W}$
\end_inset

.
\end_layout

\begin_layout Standard
Gibbs Sampling in Markov Random Fields uses a converging Markov chain to
 sample from the target joint probability distribution.
\end_layout

\begin_layout Enumerate
A prerequisite is that the conditional probability distributions are known
 in closed form.
\end_layout

\begin_layout Enumerate
Initialize the state 
\begin_inset Formula $x_{i}^{(1)}$
\end_inset

 of the random variable 
\begin_inset Formula $X_{i}\in\mathbf{W}\cup\mathbf{U}$
\end_inset

 with an arbitrary value.
\end_layout

\begin_layout Enumerate
Initialize the state 
\begin_inset Formula $x_{i}^{(1)}$
\end_inset

 of the known random variables 
\begin_inset Formula $X_{i}\in\mathbf{K}$
\end_inset

 with their known state.
\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:For-each-time"

\end_inset

For each time point 
\begin_inset Formula $t\in\{1,2,\dots\}$
\end_inset

 do
\end_layout

\begin_deeper
\begin_layout Enumerate
Keep the known variables fixed (i.e.
 
\begin_inset Formula $x_{i}^{(t+1)}:=x_{i}^{(t)}$
\end_inset

 for all 
\begin_inset Formula $X_{i}\in\mathbf{K}$
\end_inset

).
\end_layout

\begin_layout Enumerate
For each random variable 
\begin_inset Formula $X_{i}\in\mathbf{W}\cup\mathbf{U}$
\end_inset

 do
\end_layout

\begin_deeper
\begin_layout Enumerate
Given the states of all variables 
\begin_inset Formula $\mathbf{X}\backslash X_{i}$
\end_inset

, sample a new 
\begin_inset Formula $X_{i}$
\end_inset

 from its conditional distribution 
\begin_inset Formula $P(X_{i}=x_{i}^{(t+1)}\mid X_{1}=x_{1}^{(t)},\dots,X_{i-1}=x_{i-1}^{(t)},X_{i+1}=x_{i+1}^{(t)},\dots,X_{n}=x_{n}^{(t)})$
\end_inset

.
 Due to the Hammersley-Clifford theorem (section 
\begin_inset CommandInset ref
LatexCommand vref
reference "par:Hammersley-Clifford-theorem"

\end_inset

) this conditional probability is equal to 
\begin_inset Formula $P(X_{i}=x_{i}^{(t+1)}\mid\mathbf{X}_{Neighborhood(X_{i})})$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Repeat step (
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:For-each-time"

\end_inset

) until the Markov chain converges.
\end_layout

\begin_layout Enumerate
Discard the states of the uninteresting random variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Return the states of the interesting random variables 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Gibbs Sampling in Bayesian Networks
\end_layout

\begin_layout Standard
The conditional probability of 
\begin_inset Formula $X_{i}$
\end_inset

 given all other nodes is equal to its conditional probability given the
 values of the nodes in 
\begin_inset Formula $X_{i}$
\end_inset

's Markov Blanket 
\begin_inset Formula $X_{MarkovBlanket(X_{i})}$
\end_inset


\begin_inset Formula 
\[
P(X_{i}=x_{i}\mid\mathbf{X_{j}}=\mathbf{x_{j}}:j\neq i)=P(X_{i}=x_{i}\mid\mathbf{X}_{MarkovBlanket(X_{i})}=\mathbf{x}_{MarkovBlanket(X_{i})}),
\]

\end_inset

where 
\begin_inset Formula $\mathbf{X}_{MarkovBlanket(X_{i})}$
\end_inset

 is the set of 
\begin_inset Formula $X_{i}$
\end_inset

's parents and children, and its children's parents.
 This conditional probability is needed for Gibbs Sampling from a Bayesian
 Network: We use the same algorithmic structure as for Markov Random Fields
 above (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Gibbs-Sampling-in-Markov-Random-Fields"

\end_inset

), but sample from 
\begin_inset Formula $P(X_{i}=x_{i}\mid\mathbf{X}_{MarkovBlanket(X_{i})}=\mathbf{x}_{MarkovBlanket(X_{i})})$
\end_inset

 when updating the state of 
\begin_inset Formula $X_{i}$
\end_inset

 in step 4(b).
\end_layout

\begin_layout Standard
Due to the definition of the joint probability distribution in section 
\begin_inset CommandInset ref
LatexCommand vref
reference "par:The-Joint-Encoded-by-Directed-Graphical-Model"

\end_inset

, and parallel to section 4.1 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

, 
\begin_inset Formula $P(X_{i}=x_{i}\mid\mathbf{X}_{MarkovBlanket(X_{i})})$
\end_inset

 is equal to
\begin_inset Formula 
\begin{eqnarray*}
P(x_{i}\mid\{x_{i}:i\neq k\}) & = & P(x_{i}\mid\mathbf{x}_{MarkovBlanket(X_{i})})\\
 & = & \frac{P(x_{i}\mid\mathbf{x}_{Parent(i)})\prod_{j\in Child(i)}P(x_{j}\mid x_{i},\mathbf{x}_{Parent(j)\backslash i})}{\sum_{\tilde{x}_{i}}P(\tilde{x}_{i}\mid\mathbf{x}_{Parent(i)})\prod_{j\in Child(i)}P(x_{j}\mid\tilde{x}_{i},\mathbf{x}_{Parent(j)\backslash i})}.
\end{eqnarray*}

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
rewrote the algorithm from 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

: 
\begin_inset Quotes eld
\end_inset

We start the Gibbs sampling procedure by fixing the observed variables to
 their known values, and setting the unobserved variables arbitrarily.
 We then repeatedly visit each unobserved variable in turn, each time randomly
 selecting a new value for the variable from its conditional distribution
 given the current values of the other variables.
 From the joint distribution of equation (2.15), we see that the conditional
 distribution for X k is as follows:
\begin_inset Quotes erd
\end_inset

 Diese Formel wurde hier eingef√ºgt.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Comparison of Gibbs Sampling in Markov Random Fields and Bayesian Networks
\end_layout

\begin_layout Standard
Gibbs Sampling in Markov Random Fields is easier than Gibbs Sampling in
 directed graphical models.
 This is because the conditional probability distributions needed for Gibbs
 Sampling are simpler than those in Bayesian Networks.
 The conditional probability distribution needed in Gibbs Sampling was given
 in equation 
\begin_inset CommandInset ref
LatexCommand vref
reference "eq:Gibbs sampling: CPD"

\end_inset

.
 This conditional probability is particularly simple for Markov Random Fields,
 because of the local Markov property (refer to section 
\begin_inset CommandInset ref
LatexCommand vref
reference "local-Markov-property"

\end_inset

): each random variable 
\begin_inset Formula $X_{i}$
\end_inset

 is independent of all other random variables given the states of the neighborin
g random variables.
 This means that in Markov Random Fields, the conditional probability (see
 equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Gibbs sampling: CPD"

\end_inset

) is 
\begin_inset Formula 
\[
P(X_{i}\mid X_{1},\dots,X_{i-1},X_{i+1},\dots,X_{n})=P(X_{i}\mid\mathbf{X}_{Neighborhood(X_{i})}).
\]

\end_inset

For comparison, in Bayesian Networks the conditional probability would be
 
\begin_inset Formula 
\[
P(X_{i}\mid X_{1},\dots,X_{i-1},X_{i+1},\dots,X_{n})=P(X_{i}\mid\mathbf{X}_{MarkovBlanket(X_{i})}),
\]

\end_inset

where 
\begin_inset Formula $\mathbf{X}_{MarkovBlanket(X_{i})}$
\end_inset

 are the random variables in the Markov Blanket of 
\begin_inset Formula $X_{i}$
\end_inset

.
 The Markov Blanket of a random variable is more complicated to compute
 than its neighborhood.
 The Markov Blanket of a random variable 
\begin_inset Formula $X$
\end_inset

 is the set of its parents and children, and its children's parents.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: the above definition is from wikipedia, but I have to cite some paper.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Artificial Neural Networks
\end_layout

\begin_layout Standard
Here we will introduce artificial neural networks that have been developed
 as models of biological neural networks by various people since about the
 middle of the last century.
 The artificial neural networks introduced here are related with Deep Belief
 Networks, namely Hopfield networks, Multilayer Perceptrons, and (Restricted)
 Boltzmann Machines.
\end_layout

\begin_layout Paragraph
Distinction Between a Deterministic and Stochastic Network
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: maybe move the definition of a deterministic network and a stochastic
 network somewhere else.
\end_layout

\end_inset

The difference between a stochastic
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
stochastic network
\end_layout

\end_inset

 and a deterministic network
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
deterministic network
\end_layout

\end_inset

 is that in a deterministic network a node represents a single value, while
 in a stochastic network a single node represents a probability distribution.
 In particular, if there is a set of 
\begin_inset Quotes eld
\end_inset

output
\begin_inset Quotes erd
\end_inset

 nodes, then in a deterministic setting the output can be interpreted as
 a single point in a high-dimensional space, while in a stochastic network
 the output is the joint probability distribution over all the random variables
 associated with the output nodes.
\end_layout

\begin_layout Standard
A stochastic network is more general than its deterministic counterpart,
 since a stochastic network can be converted to a deterministic network,
 but not vice-versa.
 This greater generality comes at a higher cost, however, since inference
 and learning in stochastic networks take longer than the computation of
 the output and the learning in deterministic networks.
\end_layout

\begin_layout Subsection
Hopfield Networks
\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
A Hopfield Network
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Hopfield Network
\end_layout

\end_inset

 is a deterministic recurrent network with 
\begin_inset Formula $m$
\end_inset

 nodes
\begin_inset Note Note
status open

\begin_layout Plain Layout
As this is not a stochastic network, I don't think it makes sense to define
 random variables 
\begin_inset Formula $\mathbf{N}=(N_{1},\dots,N_{m})$
\end_inset

.
\end_layout

\end_inset

, each having a binary state 
\begin_inset Formula $n_{i}\in\{0,1\}$
\end_inset

 for all nodes 
\begin_inset Formula $i$
\end_inset

.
 Each node has a connection with all others (but not itself).
 The connection from node 
\begin_inset Formula $N_{i}$
\end_inset

 to node 
\begin_inset Formula $N_{j}$
\end_inset

 is directed and has a weight 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

.
 As there is no connection from node 
\begin_inset Formula $i$
\end_inset

 to node 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $w_{ii}=0$
\end_inset

 for all nodes 
\begin_inset Formula $i$
\end_inset

.
 This means each node has 
\begin_inset Formula $m-1$
\end_inset

 outgoing connections and 
\begin_inset Formula $m-1$
\end_inset

 incoming connections.
 There is also a real-valued bias 
\begin_inset Formula $b_{i}\in\mathbb{R}$
\end_inset

 for each node 
\begin_inset Formula $i$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-a-Hopfield-Network"

\end_inset

 shows an example of the structure of a Hopfield Network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/hopfield-network-example.dia
	width 30col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-a-Hopfield-Network"

\end_inset

Example of a Hopfield Network.
 The circles are the nodes; the arrows are the (directed) connections.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Updating Rule
\end_layout

\begin_layout Standard
The network is updated asynchronously: At each time point 
\begin_inset Formula $t$
\end_inset

, a node 
\begin_inset Formula $i$
\end_inset

 is chosen at random out of the 
\begin_inset Formula $m$
\end_inset

 possible nodes and it is updated, while all other nodes remain constant.
 The state 
\begin_inset Formula $n_{i}$
\end_inset

 of node 
\begin_inset Formula $i$
\end_inset

 at time point 
\begin_inset Formula $t$
\end_inset

 is denoted 
\begin_inset Formula $n_{i}^{(t)}$
\end_inset

 and depends on the state of all other nodes at time step 
\begin_inset Formula $t-1$
\end_inset

: 
\begin_inset Formula 
\[
n_{i}^{(t)}=f\left(\sum_{j\neq i}n_{j}^{(t-1)}w_{ji}+b_{i}\right).
\]

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Note that I have added 
\begin_inset Formula $b_{j}$
\end_inset

 here (to be consistent with the rest of this text), while in 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

 the bias is on the right-hand side in eq.
 2: 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $<U_{i}$
\end_inset


\begin_inset Quotes erd
\end_inset

.
 To make up for this sign change, I also swap signs in the energy (eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Energy of a Hopfield network"

\end_inset

) below.
\end_layout

\end_inset

(In Hopfield's formulation in 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

 there is also an external input 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Der Input, der in jedem Zeitschritt auf den Input addiert wird, hei√üt in
 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

 
\begin_inset Formula $I_{i}$
\end_inset

.
\end_layout

\end_inset

to each node, constant over all times 
\begin_inset Formula $t$
\end_inset

.
 Because the bias also does not depend on 
\begin_inset Formula $t$
\end_inset

, both are combined into 
\begin_inset Formula $b_{i}$
\end_inset

 here.)
\begin_inset Newline newline
\end_inset

The activation function 
\begin_inset Formula $f$
\end_inset

 is a stepping function
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: there must be a better word instead of 
\begin_inset Quotes eld
\end_inset

stepping function
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

, with nonpositive values mapped to 0, and positive values mapped to 1:
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: stimmt die Definition von 
\begin_inset Formula $f$
\end_inset

 √ºberhaupt?
\end_layout

\end_inset

 
\begin_inset Formula 
\[
f(x)=\begin{cases}
0 & \mbox{for }x\leq0\\
1 & \mbox{for }x>0
\end{cases}.
\]

\end_inset

While this updating rule was described here as asynchronous (i.e.
 at each time step a node is picked at random and its state is updated,
 which is how Hopfield described it in 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
page 1: 
\begin_inset Quotes eld
\end_inset

Each neuron samples its input at random times.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

), updating the network synchronously (i.e.
 all nodes are updated at the same time) is also possible
\begin_inset Note Note
status open

\begin_layout Plain Layout
 as this is a special case of asynchronous updating
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Although a Hopfield network is recurrent, 
\end_layout

\end_inset

Hopfield proved that the updating rule converges to a (possibly local) minimum
 of the energy function, given that the weights are symmetric (i.e.
 
\begin_inset Formula $w_{ij}=w_{ji}$
\end_inset

) and there are no single-node loops (i.e.
 
\begin_inset Formula $w_{ii}=0$
\end_inset

).
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Aus 
\begin_inset CommandInset citation
LatexCommand cite
key "Hopfield1984"

\end_inset

: 
\begin_inset Quotes eld
\end_inset

There is a simple mathematical condition which guarantees that the state
 space flow algorithm converges on stable states.
 Any symmetric T with zero diagonal elements (i.e., 
\begin_inset Formula $T_{ij}$
\end_inset

 = 
\begin_inset Formula $T_{ji}$
\end_inset

, 
\begin_inset Formula $T_{ii}$
\end_inset

 = 0) will produce such a flow.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Energy of a Hopfield Network
\end_layout

\begin_layout Standard
This can be proven by showing that a single number, associated with the
 state of the network, can only decrease when updating a node.
 It is the so-called 
\emph on
energy 
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
energy
\end_layout

\end_inset

 
\begin_inset Formula $E$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E^{(t)}=-\frac{1}{2}\sum_{i}\sum_{j\neq i}w_{ij}n_{i}^{(t)}n_{j}^{(t)}-\sum_{i}b_{i}n_{i}^{(t)}.\label{eq:Energy of a Hopfield network}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Each update of a single node either doesn't change the energy 
\begin_inset Formula $E$
\end_inset

 or decreases it.
 As time progresses, 
\begin_inset Formula $E$
\end_inset

 becomes smaller and smaller, i.e.
 
\begin_inset Formula $E^{(t)}\leq E^{(t-1)}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Training a Hopfield Network
\end_layout

\begin_layout Standard
Training a Hopfield network is the task of finding weights 
\begin_inset Formula $w_{ij}$
\end_inset

 and biases 
\begin_inset Formula $b_{i}$
\end_inset

, so that desirable states (training patterns) have a low energy and undesirable
 states have a high energy.
 Due to this property Hopfield networks can be used as 
\emph on
associative memory
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
associative memory
\end_layout

\end_inset

.
 Training is not described here.
\end_layout

\begin_layout Paragraph
Recalling a Known State by the Updating Rule
\end_layout

\begin_layout Standard
Recalling a partially known state works by updating.
 After training, a Hopfield network can be initialized with a distorted
 pattern, in which the state of some nodes is swapped.
 After iteratively updating until its state doesn't change anymore, the
 stationary state will be equal to a similar pattern used in training if
 training was successful.
\end_layout

\begin_layout Subsection
Multilayer Perceptrons
\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
A Multilayer Perceptron belongs to the class of deterministic feedforward
 neural networks
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
feedforward neural network
\end_layout

\end_inset

.
 In it, the neurons are arranged in layers, with the value of nodes in a
 layer only depending on the values of nodes in the layer above.
 Feedforward networks are in contrast to recurrent networks
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
recurrent neural network
\end_layout

\end_inset

, which contain a directed cycle of neuronal connections.
\end_layout

\begin_layout Subsubsection
Multilayer Feedforward Networks as Universal Function Approximators
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HornikWhite1989"

\end_inset

 found that artificial feedforward neural networks with as few as one hidden
 layer are capable of modeling virtually any (Borel measurable) function
 within a given error, provided the following conditions are met:
\end_layout

\begin_layout Itemize
The activation function must be a 
\begin_inset Quotes eld
\end_inset

squashing
\begin_inset Quotes erd
\end_inset

 function: A squashing function 
\begin_inset Formula $s(x)$
\end_inset

 must be non-decreasing, 
\begin_inset Formula $\lim_{x\rightarrow-\infty}s(x)=0$
\end_inset

 and 
\begin_inset Formula $\lim_{x\rightarrow\infty}s(x)=1$
\end_inset

.
 An example is the sigmoid function 
\begin_inset Formula $\frac{1}{1+exp(-x)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Sufficiently many hidden nodes must be available.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HornikWhite1989"

\end_inset

 also note that 
\begin_inset Quotes eld
\end_inset

This [result] implies that any lack of success in applications must arise
 from inadequate learning, insufficient numbers of hidden units[nodes] or
 the lack of a deterministic relationship between input and target.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsubsection
Back-propagation
\end_layout

\begin_layout Standard

\emph on
Back-propagation
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
back-propagation
\end_layout

\end_inset

 is the adaptation of weights and biases of the network to make its set
 of actual outputs better fit a set of desired outputs for a given set of
 inputs.
 Technically it is just running the network for a given input, observing
 the outputs in the output layer, computing the errors to the desired outputs
 and back-propagating them to adapt the weights and biases between all the
 layers.
 This will make the network output values closer to the desired values next
 time this particular input pattern is given to the network.
 The back-propagation algorithm is a supervised learning step and thus prone
 to overfitting.
\end_layout

\begin_layout Paragraph
Optimizing the Sum of Squared Differences
\begin_inset CommandInset label
LatexCommand label
name "par:Optimizing-the-Sum-of-Squared-DIfferences"

\end_inset


\end_layout

\begin_layout Standard
Usually in back-propagation, the sum of the squared differences
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sum of squared errors
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
squared-error sum
\end_layout

\end_inset

 between the desired outputs and the actual outputs in the output layer
 are minimized:
\begin_inset Formula 
\[
\sum_{i}(\hat{x_{i}}-x_{i})^{2},
\]

\end_inset

 where 
\begin_inset Formula $x_{i}$
\end_inset

 is the desired value for node 
\begin_inset Formula $i$
\end_inset

 of the output layer and 
\begin_inset Formula $\hat{x_{i}}$
\end_inset

 is the actual output value of node 
\begin_inset Formula $i$
\end_inset

.
 The minimization is done by computing the derivative of the error of one
 node (which is a function of the outputs of the nodes in the above layer
 and the desired output value) with respect to a particular weight and then
 increasing or decreasing the weight by the computed derivative multiplied
 by the learning rate
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
learning rate
\end_layout

\end_inset

.
 Likewise, the bias is updated by taking the derivative of the error with
 respect to the bias, and using this as a delta to update the bias.
\end_layout

\begin_layout Paragraph
Optimizing the Cross-entropy Error
\begin_inset CommandInset label
LatexCommand label
name "par:Optimizing-the-Cross-entropy-error"

\end_inset


\end_layout

\begin_layout Standard
One can also minimize the cross-entropy error
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
cross-entropy error
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "NasrJoun2002"

\end_inset

:
\begin_inset Formula 
\[
-\sum_{i}p_{i}\log\hat{p_{i}}-\sum_{i}(1-p_{i})\log(1-\hat{p_{i}}),
\]

\end_inset

 where 
\begin_inset Formula $p_{i}$
\end_inset

 is the desired output value of node 
\begin_inset Formula $i$
\end_inset

 in the output layer and 
\begin_inset Formula $\hat{p_{i}}$
\end_inset

 is the actual output value output by node 
\begin_inset Formula $i$
\end_inset

.
 This makes sense if only the node having the maximal value in a layer is
 interesting, and not its exact value.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: formula for the updating in the back-propagation algorithm.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Parameters in Training a Multilayer Perceptron
\begin_inset CommandInset label
LatexCommand label
name "sub:Parameters-of-Training-a-Multilayer-Perceptron"

\end_inset


\end_layout

\begin_layout Standard
Although described here for multilayer perceptrons, the parameters apply
 to most artificial neural networks, not just multilayer perceptrons.
\end_layout

\begin_layout Paragraph
Sigmoid activation function
\begin_inset CommandInset label
LatexCommand label
name "The-sigmoid-activation-function"

\end_inset


\end_layout

\begin_layout Standard
The value (activation) of hidden and visible nodes are a function of the
 sum of their inputs.
 The function that maps the sum of the inputs of a node to its value is
 called the 
\emph on
activation function
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
activation function
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
The sigmoid activation function is a standard activation function, often
 used in neural networks.
 It has the property that it is almost linear for inputs around zero, tends
 to 1 as its inputs go to positive infinity and to 0 as inputs go to negative
 infinity (see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sigmoid-function"

\end_inset

).
\begin_inset Formula 
\[
\sigma(x)=\frac{1}{1+e^{-x}}
\]

\end_inset


\end_layout

\begin_layout Standard
Another commonly used activation function is the hyperbolic tangent function
 
\begin_inset Formula $tanh$
\end_inset

: 
\begin_inset Formula 
\[
tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}.
\]

\end_inset


\end_layout

\begin_layout Standard
In this work, only the sigmoid activation function was used.
\end_layout

\begin_layout Paragraph
Momentum of the Learning Rule
\begin_inset CommandInset label
LatexCommand label
name "par:Momentum-of-the-learning-rule"

\end_inset


\end_layout

\begin_layout Standard
Usually, the learning rule includes a 
\emph on
momentum
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
momentum
\end_layout

\end_inset

 term.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Formel hinschreiben
\end_layout

\end_inset

 This term works like a low-pass filter and reduces oscillations during
 learning by smoothing the weight and bias deltas added to the parameters
 of the network.
\end_layout

\begin_layout Standard
Because the momentum term includes a coefficient that is the fraction of
 the weight deltas in the previous time step to be added to the current
 weight deltas, it adds another meta-parameter to training.
 Too large a momentum coefficient can cause 
\begin_inset Quotes eld
\end_inset

explosion
\begin_inset Quotes erd
\end_inset

, or non-convergence of the model during training.
 To prevent this, the coefficient is changed during training, and usually
 gradually increased during the early steps of training to its final value.
\end_layout

\begin_layout Subsubsection
Difficulties in Training Multi-layer Neural Networks
\end_layout

\begin_layout Standard
Training a randomly initialized feed-forward neural network with more than
 1 hidden layer using back-propagation is difficult and usually does not
 succeed.
 When attempting to train such a network, each node in the output layer
 often just outputs the mean value of the desired output of the training
 cases, independently of the input.
 One problem is that there are many local minima (generated by repeatedly
 adding weighted sigmoid functions) of the implicitly optimized energy function
 during back-propagation.
 Another problem is that in discriminative learning, each training case
 only contributes as many bits to the specification of the parameters of
 the network as needed to specify the label.
 This makes generative models
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
generative model
\end_layout

\end_inset

 with a discriminative part put on top, as will be shown later, more attractive
 than pure discriminative models
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
discriminative model
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Examples for generative models are the Restricted Boltzmann Machine or Deep
 Belief Network.
\end_layout

\begin_layout Subsection
Autoassociator
\end_layout

\begin_layout Standard
In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Autoassociator-introduction"

\end_inset

, an autoassociator was described as one of the first networks with a deep
 architecture that could be trained effectively.
 The training is done by iteratively adding layers to the network, and it
 starts with three layers: input layer, where input data is applied, the
 internal hidden layer, which usually has a smaller size than the input
 and output layers, and the output layer, which has the same size as the
 input layer and where the original input should be reconstructed as closely
 as possible.
\end_layout

\begin_layout Standard
When training this first iteration of the network, the input patterns are
 taken from the training data set.
 Standard back-propagation is used to adapt the weights between input layer
 and hidden layer and between hidden layer and output layer.
 The goal of training is that the network reconstructs as output patterns
 the input patterns.
 One might think that that is too easy, but usually the number of nodes
 in the hidden layer is chosen to be smaller than the number of nodes in
 the input (and output) layer.
 In this way the autoassociator is forced to reconstruct its input from
 a compressed representation.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
(TODO: hier ein Bild eines autoassociator networks.)
\end_layout

\end_inset

 Once back-propagation does not improve the reconstruction error on the
 test set anymore, the second step starts.
\end_layout

\begin_layout Standard
In the second step, both encoder and decoder are extended by one layer.
 The encoder network is enlarged by an additional hidden layer put on top
 of its existing 1st hidden layer.
 The weights between the 1st and 2nd (new) hidden layer are initialized
 randomly.
 The biases of the nodes in the 2nd (new) hidden layer are also initialized
 (randomly or constantly zero).
 The decoder network is constructed on top of the new hidden 2nd layer by
 inserting weights initialized randomly, copying the 1st hidden layer to
 form the 3rd hidden layer, copying the weights between hidden layer 1 (from
 the first iteration) to the output layer, and copying the output layer
 (from the first iteration).
 Then back-propagation is used to train the autoassociator consisting of
 the new encoder network with the new decoder network on top.
 This establishes values for the values of the initialized weights and biases.
 This process can be repeated until a sufficient number of hidden layers
 has been trained.
\end_layout

\begin_layout Subsubsection
Autoassociator with a classifier on top
\end_layout

\begin_layout Standard
The autoassociator as described is an unsupervised algorithm, because it
 only reconstructs its input.
 The autoassociator can however be used in a supervised fashion by first
 training its encoder and decoder networks up to sufficient depth, and then
 removing the decoder network and replacing it by a single output layer
 that has the dimension of the training label.
 The weights between the last hidden layer of the encoder and the new output
 layer as well as the biases of the new output layer are initialized randomly
 (or to constantly zero), and trained using back-propagation.
\end_layout

\begin_layout Subsection
Regularizations of Neural Networks
\end_layout

\begin_layout Standard
Several regularization
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
regularization
\end_layout

\end_inset

 methods for neural networks have been developed over the years.
 They have in common that they artificially constrain the search space of
 weights and biases in order to let the model find better error minima or
 to prevent overfitting.
 The neural network should do less 
\begin_inset Quotes eld
\end_inset

learning by heart
\begin_inset Quotes erd
\end_inset

 and instead make its predictions more general.
\end_layout

\begin_layout Standard
The regularizations described here apply to most artificial neural networks,
 not just multilayer perceptrons.
\end_layout

\begin_layout Subsubsection
L1 and L2 Weight Decay
\begin_inset CommandInset label
LatexCommand label
name "sub:L1-and-L2-Weight-Decay"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
L1 and L2 weight decay are described here as implemented in 
\emph on
deepnet
\emph default
.
\end_layout

\end_inset

L1 and L2 weight decay
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
L1 weight decay
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
L2 weight decay
\end_layout

\end_inset

 penalize large weights by moving them towards zero.
 Both weight decay methods decrease the absolute value of each weight in
 each training iteration, in order to prevent large weights.
 This can be necessary because for some training samples, some weights tend
 to escape, i.e.
 become larger and larger in value, making subsequent changes to the weights
 more difficult.
\end_layout

\begin_layout Standard
L1 weight decay uses in the updating of the weights, instead of the default
 weight delta 
\begin_inset Formula $\Delta w_{ij}$
\end_inset

, the penalized 
\begin_inset Formula $\Delta w_{ij}-c*sgn(w_{ij})$
\end_inset

, where 
\begin_inset Formula $c\in\mathbb{R}^{+}$
\end_inset

 is a small positive constant meta-parameter, the 
\begin_inset Quotes eld
\end_inset

weight-cost
\begin_inset Quotes erd
\end_inset

 of L1 weight decay, and 
\begin_inset Formula $sgn(x)$
\end_inset

 is the sign of 
\begin_inset Formula $x$
\end_inset

, i.e.
 -1, 0, 1, for 
\begin_inset Formula $x$
\end_inset

 being negative, zero, positive, respectively.
 L2 weight decay replaces 
\begin_inset Formula $\Delta w_{ij}$
\end_inset

 with 
\begin_inset Formula $\Delta w_{ij}-w_{ij}*c$
\end_inset

, where 
\begin_inset Formula $c\in\mathbb{R}^{+}$
\end_inset

 is the small positive 
\begin_inset Quotes eld
\end_inset

weight-cost
\begin_inset Quotes erd
\end_inset

 of L2 weight decay.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010"

\end_inset

 notes that there are four different reasons for using weight decay: better
 generalization of the resulting network, making the weights more interpretable
 by shrinking large weights, penalize network nodes that are always firmly
 on or off due to large inputs caused by large weights, and improve the
 mixing rate of contrastive divergence
\begin_inset Foot
status open

\begin_layout Plain Layout
Contrastive divergence is explained in section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sub:Training-Restricted-Boltzmann-Machines-using-Contrastive-Divergence"

\end_inset

.
\end_layout

\end_inset

, where small weights increase the mixing rate of the Gibbs chain.
\end_layout

\begin_layout Standard
As 
\begin_inset CommandInset citation
LatexCommand cite
key "FischerIgel2012"

\end_inset

 note, using an L2 weight decay term in the updating term corresponds to
 assuming a zero-mean Gaussian prior on the parameters in a Bayesian framework.
\end_layout

\begin_layout Subsubsection
Sparsity Target
\begin_inset CommandInset label
LatexCommand label
name "sub:Sparsity-Target"

\end_inset


\end_layout

\begin_layout Standard
Sparsity
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sparsity
\end_layout

\end_inset

 regularization is a method to make only a small fraction of nodes output
 an activation different from zero.
 This helps to generalize the output, and also makes neural networks more
 interpretable 
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010,NairHinton2009"

\end_inset

.
 Sparse activity constrains the space of possible parameters of the model.
\end_layout

\begin_layout Standard
Setting a sparsity target works by scaling the total input to a node by
 a value controlled by three meta-parameters: the sparsity target, the decay
 rate, and the sparsity cost.
 The idea is to scale the total input to a node so that its probability
 of being active 
\begin_inset Formula $q$
\end_inset

 approaches that of a choosable target probability 
\begin_inset Formula $p\ll1$
\end_inset

 (the sparsity target).
 The actual probability of being active 
\begin_inset Formula $q$
\end_inset

 is registered by using an exponentially decaying average of the mean probabilit
y
\begin_inset Formula 
\[
q_{new}=\lambda q_{old}+(1-\lambda)q_{current},
\]

\end_inset

where 
\begin_inset Formula $\lambda$
\end_inset

 is the decay rate.
 The penalty to the input is measured using the cross entropy between the
 desired distribution and actual activity distribution 
\begin_inset Formula 
\[
\mbox{sparsity penalty}\propto-p\log q-(1-p)\log(1-q).
\]

\end_inset

The derivative of the cross entropy with respect to the total input to a
 node is 
\begin_inset Formula $q-p$
\end_inset

, for nodes with sigmoid activation function.
 (The derivative of the penalty is 0 when the desired is equal to the actual
 activity distribution.) Both the bias and the incoming weights must then
 be adjusted by this derivative, scaled by the sparsity cost.
\end_layout

\begin_layout Subsubsection
Dropout
\begin_inset CommandInset label
LatexCommand label
name "sub:Dropout"

\end_inset


\end_layout

\begin_layout Standard
Dropout
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
dropout
\end_layout

\end_inset

 is a regularization method to make the nodes in the hidden layers, which
 can be seen as feature detectors, less dependent on each other 
\begin_inset CommandInset citation
LatexCommand cite
key "SrivastavaSalakhutdinov2014"

\end_inset

.
 This is enforced by 
\begin_inset Quotes eld
\end_inset

dropping
\begin_inset Quotes erd
\end_inset

 each training iteration a random subset of nodes in a layer.
 This prevents subsequent layers from adapting to specific combinations
 of node activations in the previous layer.
 Dropout can be used in any neural network whose input to a node is computed
 from several input nodes.
 
\end_layout

\begin_layout Standard
Dropout specifies a probability 
\begin_inset Formula $d$
\end_inset

 for nodes in a layer to be active during a training iteration.
 In each iteration, on average only 
\begin_inset Formula $d*n$
\end_inset

 nodes' output values are computed and the other nodes are set to contribute
 nothing (zero) to the input to the next layer.
 This greatly constrains complex co-adaptations between nodes, in which
 nodes are only useful in the context of a large number of other nodes.
 It thereby reduces overfitting.
 To utilize all trained nodes during testing, all nodes contribute to the
 computation of input to a layer, but their total input must then be multiplied
 by 
\begin_inset Formula $d$
\end_inset

, to simulate that only a fraction of 
\begin_inset Formula $d$
\end_inset

 nodes are active.
\end_layout

\begin_layout Standard
The theory behind dropout is that for a network with 
\begin_inset Formula $n$
\end_inset

 nodes, there are 
\begin_inset Formula $2^{n}$
\end_inset

 possible ways to drop out those nodes.
 During testing, a network trained with dropout implicitly averages its
 output over all these 
\begin_inset Formula $2^{n}$
\end_inset

 networks with shared weights.
 Dropping out a random fraction of nodes prevents single nodes from co-adapting
 to the specific working of a large number of other nodes.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: probably not describe weight normalization, because I didn't use it
 in my 
\emph on
deepnet 
\emph default
trials.
\end_layout

\begin_layout Plain Layout
Dropout is usually combined with weight normalization.
 
\end_layout

\begin_layout Paragraph
Weight normalization
\end_layout

\begin_layout Plain Layout
probably not TODO: I think weight normalization is described a bit in the
 dropout paper.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Early Stopping
\begin_inset CommandInset label
LatexCommand label
name "sub:Early-stopping"

\end_inset


\end_layout

\begin_layout Standard
Early stopping
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
early stopping
\end_layout

\end_inset

 is not really a regularization method, but nevertheless a method to prevent
 overfitting in supervised training of an artificial neural network.
 It is used by splitting the training data set into a real training data
 set and a validation data set, and using only the real training data set
 for adapting the weights and biases during learning.
 After each learning iteration, the validation data set is used to compute
 the output error of the current network.
 After a defined number of training iterations, the neural network that
 had the lowest output error on the validation data set is used as the final
 network to be used in making predictions.
\end_layout

\begin_layout Standard
This prevents the training procedure from overfitting to sampling error
 present in the training data set.
\end_layout

\begin_layout Subsection
Boltzmann Machines
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Elaborate the definition
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
A Boltzmann Machine
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Boltzmann Machine
\end_layout

\end_inset

 is an undirected graphical model that has a specific form of the conditional
 probability distribution defined at each node.
 A Boltzmann Machine is a stochastic version of a Hopfield network.
 There are visible (
\series bold

\begin_inset Formula $\mathbf{V}$
\end_inset


\series default
) and hidden nodes (
\begin_inset Formula $\mathbf{H}$
\end_inset

), all of which have a binary state.
 In contrast to the Restricted Boltzmann Machine, its two kinds of nodes
 are not arranged in the visible and hidden layer, but the (undirected)
 connections can be between any two nodes.
 Like Restricted Boltzmann Machines, a Boltzmann Machine is a way to store
 a joint probability distribution.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Example figure of a BM.
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
The conditional probability distribution for a hidden node 
\begin_inset Formula $H_{i}\in\mathbf{H}$
\end_inset

 depends on the states of all other nodes and is defined by 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSejnowski1986"

\end_inset

 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
in Eq 2, page 6, and Eq 3, page 8
\end_layout

\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(H_{i}=1\mid\mathbf{S_{j}}=\mathbf{s_{j}}:j\neq i)=\sigma\left(\sum_{j}s_{j}w_{ij}-b_{i}\right),\label{eq:Boltzmann Machine p(h=1|S)}
\end{equation}

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
(this formula is only valid for 0/1-valued nodes.)
\end_layout

\end_inset

where 
\begin_inset Formula $\mathbf{S}=\mathbf{V}\cup\mathbf{H}$
\end_inset

, 
\begin_inset Formula $s_{j}$
\end_inset

 is the state of node 
\begin_inset Formula $S_{j}$
\end_inset

, 
\begin_inset Formula $\sigma(x)=\frac{1}{1+\exp(-x)}$
\end_inset

, 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

 is the weight between hidden node 
\begin_inset Formula $H_{i}$
\end_inset

 and (visible or hidden) node 
\begin_inset Formula $S_{j}$
\end_inset

, and 
\begin_inset Formula $b_{i}$
\end_inset

 is the bias of hidden node 
\begin_inset Formula $H_{i}$
\end_inset

.
 Similarly,
\begin_inset Formula 
\[
P(V_{j}=1\mid\mathbf{S_{i}}=\mathbf{s_{i}}:i\neq j)=\sigma\left(\sum_{i}s_{i}w_{ij}-c_{j}\right),
\]

\end_inset

where 
\begin_inset Formula $V_{j}$
\end_inset

 is a visible node, 
\begin_inset Formula $w_{ij}=w_{ji}$
\end_inset

 is the weight between node 
\begin_inset Formula $S_{i}$
\end_inset

 and visible node 
\begin_inset Formula $V_{j}$
\end_inset

, and 
\begin_inset Formula $c_{j}$
\end_inset

 is the bias of visible node 
\begin_inset Formula $V_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
Finding the values 
\begin_inset Formula $s_{i}$
\end_inset

 from the conditional probabilities 
\begin_inset Formula $P(H_{i}=1\mid\mathbf{S})$
\end_inset

 or 
\begin_inset Formula $P(V_{j}=1\mid\mathbf{S})$
\end_inset

 works by drawing a one with these conditional probabilities and a zero
 otherwise.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: rewrite the following using consistent variable names and fonts.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Gibbs Sampling in Boltzmann Machines
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: probably move this somewhere else
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Since a Boltzmann Machine is an undirected graphical model, the inference
 algorithm from 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 applies.
 See section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Inference-in-Markov-Random-Fields"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Training Boltzmann Machines
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
probably not TODO (since the next paragraphs look finished): rewrite the
 following using consistent variable names and fonts.
\end_layout

\begin_layout Plain Layout
Neal writes (on page 75, I don't know for which pdf file, however) about
 training of Boltzmann machines, i.e.
 adjusting the weights so that a set of training samples 
\begin_inset Formula $T$
\end_inset

 (i.e.
 measured states of the visible nodes) become as probable as possible.
 The log-likelihood is 
\begin_inset Formula $L=\log\prod_{v\in T}P(V=v)$
\end_inset

.
 The derivative of the log-likelihood with respect to a weight is: 
\begin_inset Formula $\frac{\partial L}{\partial w_{ij}}=\beta\sum_{\mathbf{v}\in\mathbf{T}}(\sum_{s}P(\mathbf{S=s}\mid\mathbf{V=v})s_{i}s_{j}-\sum_{\mathbf{s}}P(\mathbf{S=s})s_{i}s_{j})$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: derive 
\begin_inset Formula $\frac{\partial L}{\partial w_{ij}}$
\end_inset


\end_layout

\end_inset

 (
\begin_inset Formula $\beta$
\end_inset

 is defined on page 74 to be 1 for 0/1-valued nodes, and 
\begin_inset Formula $\frac{1}{2}$
\end_inset

 for -1/1-valued nodes.)
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
In 
\begin_inset Quotes eld
\end_inset

Connectionist learning for belief networks
\begin_inset Quotes erd
\end_inset

 (~/uni/publication/zusammenfassung/rbm/A11 Connectionist learning of belief
 networks.pdf), Neal derives the learning rule for Boltzmann Machines: page
 6, equation 7.
\end_layout

\end_inset

The goal of training a Boltzmann Machine is to find parameters, i.e.
 weights and biases, such that the the probability of the training data
 becomes maximal.
 The visible nodes are set to a training sample.
 Remember that Boltzmann Machines store a joint probability distribution.
 The log-likelihood is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
L & = & \log\prod_{\mathbf{v}\in\mathbf{T}}P(\mathbf{V}=\mathbf{v}),
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\mathbf{T}$
\end_inset

 is the set of training data (to be applied to the visible nodes) and its
 derivative with respect to a weight 
\begin_inset Formula $w_{ij}$
\end_inset

 is 
\begin_inset Formula 
\[
\frac{\partial L}{\partial w_{ij}}=\sum_{\mathbf{v}\in\mathbf{T}}\left(\sum_{\mathbf{s}}P(\mathbf{S=s}\mid\mathbf{V=v})s_{i}s_{j}-\sum_{\mathbf{s}}P(\mathbf{S=s})s_{i}s_{j}\right),
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{S}$
\end_inset

 is 
\begin_inset Formula $\mathbf{V}\cup\mathbf{H}$
\end_inset

, and 
\begin_inset Formula $s_{i}$
\end_inset

 is the state of node 
\begin_inset Formula $S_{i}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

.
 The goal is to find a delta for each weight 
\begin_inset Formula $w_{ij}$
\end_inset

, which can be added to the weight, so that the likelihood for the training
 sample increases.
 The derivative of the log-likelihood with respect to a weight 
\begin_inset Formula $w_{ij}$
\end_inset

 multiplied by a learning rate
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
learning rate
\end_layout

\end_inset

 provides such a delta.
 This derivative can be approximated by the difference between two parallel
 Gibbs sampling steps: the 
\emph on
positive phase
\emph default
, where 
\begin_inset Formula $P(\mathbf{S=s}\mid\mathbf{V=v})s_{i}s_{j}$
\end_inset

 is approximated, and the 
\emph on
negative phase
\emph default
, where 
\begin_inset Formula $P(\mathbf{S=s})s_{i}s_{j}$
\end_inset

 is approximated 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

.
 
\end_layout

\begin_layout Paragraph
Positive Phase
\end_layout

\begin_layout Standard
In the positive phase
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
positive phase
\end_layout

\end_inset

 of training a Boltzmann Machine, the visible nodes 
\begin_inset Formula $\mathbf{V}$
\end_inset

 are clamped (i.e.
 their state is held fixed) to their states as they are in the training
 sample 
\begin_inset Formula $\mathbf{v}$
\end_inset

, and then the states of the remaining (i.e.
 hidden) nodes are sampled via Gibbs sampling.
 This means starting in any (for example random) configuration of the hidden
 nodes, repeatedly sampling each remaining variable 
\begin_inset Formula $S$
\end_inset

 from its conditional probability distribution given the states of all other
 variables (i.e.
 
\begin_inset Formula $P(S_{i}=s_{i}\mid\mathbf{S_{j}}=\mathbf{s_{j}}:j\neq i)$
\end_inset

) until the Gibbs sampler reaches equilibrium, and recording the state 
\begin_inset Formula $s_{i}$
\end_inset

 that each remaining variable 
\begin_inset Formula $S_{i}$
\end_inset

 had assumed in equilibrium.
 By repeatedly sampling the 
\begin_inset Formula $s_{i}$
\end_inset

 a few times when the Markov chain is in equilibrium, we determine their
 probability distributions.
 This means the conditional probability distribution of the remaining variables
 
\begin_inset Formula $P(\mathbf{S=s}|\mathbf{V=v})$
\end_inset

 is determined, and therefore the term 
\begin_inset Formula $\sum_{\mathbf{s}}P(\mathbf{S=s}|\mathbf{V=v})s_{i}s_{j}$
\end_inset

 can be determined, which completes the positive phase.
\end_layout

\begin_layout Paragraph
Negative Phase
\end_layout

\begin_layout Standard
In the negative phase
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
negative phase
\end_layout

\end_inset

 no nodes are clamped, and the states of all variables in equilibrium are
 recorded.
 Again, we start in any configuration of the network.
 Then we repeatedly sample from the conditional probability distributions
 
\begin_inset Formula $P(S_{i}=s_{i}\mid S_{j}=s_{j}:j\neq i)$
\end_inset

 for all variables 
\begin_inset Formula $S_{i}$
\end_inset

 until equilibrium, and record the state 
\begin_inset Formula $s_{i}$
\end_inset

 each variable 
\begin_inset Formula $S_{i}$
\end_inset

 had in equilibrium.
 Sampling a few more steps in equilibrium, we can determine their distributions
 
\begin_inset Formula $P(\mathbf{S=s})$
\end_inset

 and therefore the term 
\begin_inset Formula $\sum_{\mathbf{s}}P(\mathbf{S=s})s_{i}s_{j}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Training Iterations
\end_layout

\begin_layout Standard
The derivatives obtained by the positive and negative phases are multiplied
 by the learning rate
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
learning rate
\end_layout

\end_inset

 (a small positive real constant) and added to the current weights.
 Then another training iteration is started.
 This is repeated until the derivatives all converge to zero.
\end_layout

\begin_layout Paragraph
Connections to other Graphical Models
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1993"

\end_inset

 notes that the Boltzmann Machine is a generalization of the Ising model
 of ferromagnetism: 
\begin_inset Quotes eld
\end_inset

Generalized to allow [parameters] to vary from spin to spin, and to allow
 interactions between any two spins, the Ising model becomes the 
\begin_inset Quotes eld
\end_inset

Boltzmann machine
\begin_inset Quotes erd
\end_inset

 of Ackley, Hinton, and Sejnowski.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsection
Restricted Boltzmann Machines
\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Example figure of an RBM.
\end_layout

\end_inset

A Restricted Boltzmann Machine
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Restricted Boltzmann Machine
\end_layout

\end_inset

(RBM
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
RBM
\end_layout

\end_inset

) has a bipartite topology: there are visible nodes and hidden nodes, and
 each node in the visible layer is connected to all hidden nodes by undirected
 edges, but in contrast to general Boltzmann Machines there are no visible-to-vi
sible node connections and no hidden-to-hidden node connections.
 In a Restricted Boltzmann Machine, the visible nodes represent the observable
 features of a training set, while the hidden nodes are feature detectors
 which are computed from the states of all visible nodes.
\end_layout

\begin_layout Standard
As originally proposed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Smolensky1986"

\end_inset

, a Restricted Boltzmann Machine has binary visible and hidden nodes.
 There are extensions to real-valued nodes, however.
\end_layout

\begin_layout Standard
The conditional probabilities are defined analogous to Boltzmann Machines
 (see equation 
\begin_inset CommandInset ref
LatexCommand vref
reference "eq:Boltzmann Machine p(h=1|S)"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(H_{i}=1\mid\mathbf{V}=\mathbf{v})=\sigma\left(\sum_{j}v_{j}w_{ij}-b_{i}\right)
\]

\end_inset


\begin_inset Formula 
\[
P(V_{j}=1\mid\mathbf{H}=\mathbf{h})=\sigma\left(\sum_{i}h_{i}w_{ij}-c_{j}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
When choosing a zero temperature 
\begin_inset Formula $T=0$
\end_inset

, the Restricted Boltzmann Machine becomes deterministic and equivalent
 to a Hopfield network.
\end_layout

\begin_layout Plain Layout
The energy of a hopfield network was defined in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Energy of a Hopfield network"

\end_inset

 
\begin_inset CommandInset ref
LatexCommand vpageref
reference "eq:Energy of a Hopfield network"

\end_inset

 as:
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: repeat energy of equation 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Training Restricted Boltzmann Machines using Contrastive Divergence
\begin_inset CommandInset label
LatexCommand label
name "sub:Training-Restricted-Boltzmann-Machines-using-Contrastive-Divergence"

\end_inset


\end_layout

\begin_layout Standard
Since a Restricted Boltzmann Machine is a restricted form of the more general
 Boltzmann Machine, it can be trained using the training procedure for Boltzmann
 Machines.
 However, there is also a more direct learning procedure called 
\emph on
contrastive divergence
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
contrastive divergence
\end_layout

\end_inset

, where the positive phase is simpler, because the hidden and visible nodes
 are conditionally independent, given the nodes of other type.
 Contrastive divergence is obtained by approximating the derivative of the
 log-likelihood with respect to a weight 
\begin_inset Formula $w_{ij}$
\end_inset

.
\end_layout

\begin_layout Standard
Like for Boltzmann Machines, the goal of training is to find parameters
 such that the the probability of the training data becomes maximal.
 The log-likelihood is 
\begin_inset Formula 
\begin{eqnarray*}
L & = & \log\prod_{\mathbf{v}\in\mathbf{T}}P(\mathbf{V}=\mathbf{v}),
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\mathbf{T}$
\end_inset

 is the set of training data (to be applied to the visible nodes) and its
 derivative with respect to a weight 
\begin_inset Formula $w_{ij}$
\end_inset

 is 
\begin_inset Formula 
\[
\frac{\partial L}{\partial w_{ij}}=\sum_{\mathbf{v}\in\mathbf{T}}\left(P(H_{i}=1\mid\mathbf{V=v})v_{j}-\sum_{\mathbf{v}}P(\mathbf{V=v})P(H_{i}=1\mid\mathbf{v})v_{j}\right)
\]

\end_inset

 (see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "FischerIgel2012"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Formel 29 auf Seite 26
\end_layout

\end_inset

).
\end_layout

\begin_layout Paragraph
Positive Phase
\end_layout

\begin_layout Standard
The positive phase, i.e.
 
\begin_inset Formula $P(H_{i}=1\mid\mathbf{V=v})v_{j}$
\end_inset

 can be computed directly by setting the visible nodes to the training sample,
 and then computing 
\begin_inset Formula $P(H_{i}=1\mid\mathbf{V=v})=\sigma\left(\sum_{j}v_{j}w_{ij}-b_{i}\right)$
\end_inset

.
 Multiplying by 
\begin_inset Formula $v_{j}$
\end_inset

 completes the positive phase of computing the delta for 
\begin_inset Formula $w_{ij}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Negative Phase
\end_layout

\begin_layout Standard
The negative phase 
\begin_inset Formula $\sum_{\mathbf{v}}P(\mathbf{V=v})P(H_{i}=1\mid\mathbf{v})$
\end_inset

 is not as straightforward to compute.
 It may be approximated by running a Gibbs chain until convergence.
 This means initializing the network with any state, and then alternatingly
 computing 
\begin_inset Formula $P(\mathbf{H}\mid\mathbf{V})$
\end_inset

 and 
\begin_inset Formula $P(\mathbf{V}\mid\mathbf{H})$
\end_inset

 until the stationary distribution is reached.
 The number of iterations is 
\begin_inset Formula $k$
\end_inset

 and the gradient computed by contrastive divergence
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
contrastive divergence
\end_layout

\end_inset

 (i.e.
 the difference of positive phase and negative phase) is called 
\begin_inset Formula $CD_{k}$
\end_inset

.
 Often 
\begin_inset Formula $k=1$
\end_inset

 is used at the beginning of training and later 
\begin_inset Formula $k$
\end_inset

 is incremented.
 
\begin_inset Formula $\mathbf{v}$
\end_inset

 and 
\begin_inset Formula $\mathbf{h}$
\end_inset

 are sampled from the stationary distribution and allow computing 
\begin_inset Formula $\sum_{\mathbf{v}}P(\mathbf{V=v})P(H_{i}=1\mid\mathbf{v})v_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
(As 
\begin_inset CommandInset citation
LatexCommand cite
key "BengioDelalleau2007"

\end_inset

 showed, training using contrastive divergence (CD-1) and training using
 reconstruction error as error metric in an autoassociator are linked.
 They showed that the weight learning gradient used with the reconstruction
 error is a more biased approximator of the log-likelihood than the gradient
 computed by 
\begin_inset Formula $CD_{1}$
\end_inset

.)
\end_layout

\begin_layout Subsubsection
Parameters of Training a Restricted Boltzmann Machine
\end_layout

\begin_layout Standard
In addition to the parameters for training a Multilayer Perceptron (see
 section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sub:Parameters-of-Training-a-Multilayer-Perceptron"

\end_inset

), there are the following:
\end_layout

\begin_layout Paragraph
Interpreting the Output of a Node as a Continuous Value
\end_layout

\begin_layout Standard
The output of a node in a Restricted Boltzmann Machine, as orignially proposed,
 is binary (i.e.
 either 0 or 1).
 However the sigmoid activation function outputs continuous values between
 0 and 1.
 This output of the sigmoid activation function is interpreted as the probabilit
y that the node outputs value 1, and 0 otherwise.
 Using this output value directly, without using it to sample from a binomial
 distribution, allows the output to be from the interval 
\begin_inset Formula $\{0,1\}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Linear nodes with independent Gaussian noise
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006"

\end_inset

 proposed a way to extend Restricted Boltzmann Machines with only binary
 values to nodes with real values.
 However, this extension was largely replaced by rectified linear nodes
 because they performed better.
\end_layout

\begin_layout Paragraph
Rectified linear activation function
\begin_inset CommandInset label
LatexCommand label
name "par:Rectified-linear-activation-function"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "NairHinton2010"

\end_inset

 then modified the idea in 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006"

\end_inset

 to rectified linear nodes, in which the sampled output of a node is given
 by 
\begin_inset Formula $\max(0,x+N(0,\sigma(x))$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is the sum of the inputs of the node, 
\begin_inset Formula $\sigma(x)$
\end_inset

 is the sigmoid function, and 
\begin_inset Formula $N(0,\sigma(x))$
\end_inset

 is normally distributed noise with mean 
\begin_inset Formula $0$
\end_inset

 and variance 
\begin_inset Formula $\sigma(x)$
\end_inset

.
 This allows using any positive real value for the random variables of a
 RBM.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Say something about the time taken to train RBMs.
 Training time is not deterministic, since the energy landscape is affected
 by the training samples.
 Training is quadratic in the number of features if a multiple-layer neural
 network is trained and the number of features is gradually reduced.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Deep Belief Networks
\begin_inset CommandInset label
LatexCommand label
name "sub:Deep-Belief-Network"

\end_inset


\end_layout

\begin_layout Paragraph
Structure
\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Belief network
\end_layout

\end_inset

Belief Network is another name for directed graphical model.
 Deep Belief Networks
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset

(DBN
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DBN
\end_layout

\end_inset

) are Belief Networks in which the nodes are organized in layers, and where
 the value of a node in a layer only depends on the value of nodes in the
 layer above.
 There are no loops in Deep Belief Networks.
 The word 
\begin_inset Quotes eld
\end_inset

deep
\begin_inset Quotes erd
\end_inset

 refers to the number of hidden layers of a Deep Belief Network.
 Deep Belief Networks can be seen as the stochastic counterpart of deterministic
 feed-forward networks.
\end_layout

\begin_layout Standard
Because they are directed graphical models, the computation of the values
 of children nodes does not affect the value of parent nodes.
 In contrast to undirected graphical models, this allows generating (drawing
 a sample) from the model in a single pass.
 Thus, a Deep Belief Network can be used in an unsupervised algorithm to
 generate samples distributed like a training data set.
\end_layout

\begin_layout Paragraph
Sigmoid Belief Networks
\end_layout

\begin_layout Standard
Sigmoid Belief Networks were defined by 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

 as a directed graphical model with a sigmoid conditional probability function
 (see section 
\begin_inset CommandInset ref
LatexCommand vref
reference "The-sigmoid-activation-function"

\end_inset

).
 A Sigmoid Belief Network is a Belief Network in which the conditional probabili
ty associated with node 
\begin_inset Formula $N_{i}$
\end_inset

 depends only on previous nodes 
\begin_inset Formula $\mathbf{N_{j}}$
\end_inset

 (where parents must come before children).
 The conditional probbilities can be expressed as a sigmoid function 
\begin_inset Formula 
\begin{eqnarray*}
P(N_{i} & = & 1_{i}\mid\mathbf{N_{j}=n_{j}}:j<i)=\sigma(\sum_{j<i}n_{j}w_{ij}-b_{i}),
\end{eqnarray*}

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Note that Neal defines on page 10, 
\begin_inset Formula $P(S_{i}=s_{i}\mid S_{j}=s_{j}:j<i)=\sigma(s_{i}^{*}\sum_{j<i}s_{j}w_{ij})$
\end_inset

.
 The 
\begin_inset Formula $s_{i}^{*}$
\end_inset

 is defined as 
\begin_inset Formula $2s_{i}-1$
\end_inset

 for 0/1-valued nodes.
 Since I write 
\begin_inset Formula $P(N_{i}=1\mid...)$
\end_inset

 above, I can substitute 
\begin_inset Formula $1$
\end_inset

 where 
\begin_inset Formula $s_{i}^{*}$
\end_inset

 is written on the right side.
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Neal says on the bottom of page 3: 
\begin_inset Quotes eld
\end_inset

"Bias" weights, wi0, from a fictitious unit 0 whose value is aways 1 are
 also assumed to be present.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

 where 
\begin_inset Formula $n_{j}$
\end_inset

 is the binary state of node 
\begin_inset Formula $N_{j}$
\end_inset

, 
\begin_inset Formula $w_{ij}$
\end_inset

 is the directed weight from node 
\begin_inset Formula $N_{j}$
\end_inset

 to node 
\begin_inset Formula $N_{i}$
\end_inset

, and 
\begin_inset Formula $b_{i}$
\end_inset

 is the bias of node 
\begin_inset Formula $N_{i}$
\end_inset

.
 Thus the conditional probabilities of a Sigmoid Belief Network are parameterize
d with the weights and biases.
\end_layout

\begin_layout Paragraph
Arbitrary Modelling Capability
\end_layout

\begin_layout Standard
Both Boltzmann machines and Sigmoid Belief Networks can represent arbitrary
 probability distributions over a set of an arbitrary number of visible
 nodes, provided that a sufficient number of hidden nodes is available
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

It turns out that each of these three networks can represent probability
 distributions over the full set of units that the other two networks cannot.
 With the help of "hidden" units, all these networks can represent arbitrary
 distributions over a set of "visible" units.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection
Training a Deep Sigmoid Belief Network
\end_layout

\begin_layout Standard
Training a Sigmoid Belief Network is the process of finding weights between
 hidden and visible nodes such that the likelihood given the training samples
 becomes maximal.
 Doing gradient ascent means we have to find the derivative of the likelihood
 with respect to a weight.
 As 
\begin_inset CommandInset citation
LatexCommand cite
key "Neal1992"

\end_inset

 shows
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
in equation 27
\end_layout

\end_inset

, it is
\begin_inset Formula 
\[
\frac{\partial L}{\partial w_{ij}}=\sum_{\mathbf{v}\in\mathbf{T}}\sum_{\mathbf{s}}P(\mathbf{S}=\mathbf{s}\mid\mathbf{V}=\mathbf{v})s_{j}\sigma\left(-s_{i}\sum_{k<i}s_{k}w_{ik}\right),
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{S}$
\end_inset

 are the nodes, and 
\begin_inset Formula $\mathbf{V}\subset\mathbf{S}$
\end_inset

 are the arbitrarily chosen visible nodes.
 However, in the Deep Belief Network below, the visible nodes are always
 in the last (bottom) layer.
 In the likelihood we have to evaluate the conditional probability 
\begin_inset Formula $P(\mathbf{S}=\mathbf{s}\mid\mathbf{V}=\mathbf{v})$
\end_inset

.
\end_layout

\begin_layout Standard
Since exact inference of the hidden nodes given the visible nodes is intractable
, we must resort to the approximate Gibbs Sampling.
 This works as shown in section 
\begin_inset CommandInset ref
LatexCommand vref
reference "par:Gibbs-Sampling-in-Markov-Random-Fields"

\end_inset

.
 However, in Gibbs Sampling, all hidden variables (of all layers simultaneously)
 are inferred together (at the same time), and this scales poorly as models
 become larger.
 Therefore another learning algorithm is needed.
\end_layout

\begin_layout Subsubsection
A Fast Learning Algorithm for Deep Belief Networks
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 showed that there is a fast greedy learning algorithm for Deep Belief Networks,
 even with many hidden layers and millions of parameters.
\end_layout

\begin_layout Standard
As mentioned in section 
\begin_inset CommandInset ref
LatexCommand vref
reference "par:Exact-Inference-in-Deep-Belief-Networks-is-Complicated"

\end_inset

, there are computational problems with inferring the hidden variables from
 visible ones.
 The problems would go away if the posterior of the hidden given the visible
 nodes were independent between individual hidden nodes, but this is not
 the case due to explaining away.
 However, 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 came up with a trick: The posterior is equal to the product of prior times
 likelihood.
 If the prior were so that it would cancel the correlations of the likelihood,
 then the product would factor according to the hidden nodes 
\begin_inset Formula $\mathbf{y}$
\end_inset

: 
\begin_inset Formula 
\[
P(\mathbf{y}\mid\mathbf{x})=\prod_{j}P(y_{j}\mid\mathbf{x}),
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{\mathbf{x}}$
\end_inset

 are the visible nodes.
 They showed that such priors exist and have the form
\begin_inset Formula 
\[
P(\mathbf{y})=\frac{1}{C}\exp\left(\log\Omega(\mathbf{y})+\sum\alpha_{j}(y_{j})\right),
\]

\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is a normalization constant, 
\begin_inset Formula $\Omega$
\end_inset

 is a function of the visible states and the 
\begin_inset Formula $\alpha_{j}$
\end_inset

 are functions depending on the visible nodes individually.
 The desired factorial form of the posterior means that all 
\begin_inset Formula $y_{j}$
\end_inset

 must be conditionally independent (given 
\begin_inset Formula $\mathbf{x}$
\end_inset

).
 By the Hammersley-Clifford theorem (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:The-Hammersley-Clifford-theorem-of-Undirected-Graphical-Model"

\end_inset

) these conditions are fulfilled in an undirected graphical model that has
 edges between every hidden and visible variables and edges between all
 visible nodes with a joint probability of the form
\begin_inset Formula 
\begin{equation}
P(\mathbf{x},\mathbf{y})=\frac{1}{C}\exp\left(\sum_{j}\Phi_{j}(\mathbf{x},y_{j})+\beta(\mathbf{x})+\sum_{j}\alpha_{j}(y_{j})\right).\label{eq:DBN-joint-probability-with-dependencies-between-visible}
\end{equation}

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Susi findet, dass das 
\begin_inset Formula $\Phi$
\end_inset

 hier dem Phi aus Hammersley-Clifford zu √§hnlich ist.
\end_layout

\end_inset

For reasons that will become clear later, we also want to get rid of the
 dependencies between the visible nodes.
 The conditional probabilities are then of the form
\begin_inset Formula 
\begin{equation}
P(\mathbf{x}\mid\mathbf{y})=\prod_{i}P(x_{i}\mid\mathbf{y})\label{eq:DBN-x-given-y}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
P(\mathbf{y}\mid\mathbf{x})=\prod_{j}P(y_{j}\mid\mathbf{x}).\label{eq:DBN-y-given-x}
\end{equation}

\end_inset

Also by the Hammersley-Clifford theorem, the joint probability then specializes
 from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DBN-joint-probability-with-dependencies-between-visible"

\end_inset

 to
\begin_inset Formula 
\[
P(\mathbf{x},\mathbf{y})=\frac{1}{C}\exp\left(\sum_{j}\Phi_{j}(\mathbf{x},y_{j})+\sum_{i}\gamma_{i}(x_{i})+\sum_{j}\alpha_{j}(y_{j})\right).
\]

\end_inset

The reason we wanted to have both independencies as encoded by equations
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DBN-x-given-y"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DBN-y-given-x"

\end_inset

 is that these are the (in)dependecies described by a Restricted Boltzmann
 Machine.
 Inference in an RBM works by repeatedly and alternatingly evaluating these
 two conditional probabilities.
 The correctly inferred distribution is obtained once the Markov chain reaches
 equilibrium in iterating.
 However, we can also view this iterative induction as taking place in a
 Deep Belief Network that is infinitely deep, has alternating visible and
 hidden layers and has shared (
\begin_inset Quotes eld
\end_inset

tied
\begin_inset Quotes erd
\end_inset

) weights at all layers.
 The weights matrix between the DBN layers are 
\begin_inset Formula $W$
\end_inset

 from hidden to visible and 
\begin_inset Formula $W^{T}$
\end_inset

 from visible to hidden layers.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: insert graphics like Figure 3 in 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
The Greedy Training Procedure
\end_layout

\begin_layout Standard
This equivalence makes the following greedy training procedure for a Deep
 Belief Network possible.
 Start with a single RBM containing visible nodes 
\begin_inset Formula $\mathbf{V_{0}}$
\end_inset

 and hidden nodes 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 and train it using contrastive divergence to learn the weights 
\begin_inset Formula $W$
\end_inset

.
\end_layout

\begin_layout Standard
Then copy the visible layer together with the weights, turn the copy upside-down
 (i.e.
 transpose the weights), and connect this new layer called 
\begin_inset Formula $\mathbf{V_{1}}$
\end_inset

 to the top of the hidden layer.
 This new 3-layer network is then further modified: The undirected weights
 between 
\begin_inset Formula $\mathbf{V_{0}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 are 
\begin_inset Quotes eld
\end_inset

untied
\begin_inset Quotes erd
\end_inset

 and replaced by directed weights 
\begin_inset Formula $W$
\end_inset

 going from 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 to 
\begin_inset Formula $\mathbf{V_{0}}$
\end_inset

, and directed weights 
\begin_inset Formula $W^{T}$
\end_inset

 going from 
\begin_inset Formula $\mathbf{V_{0}}$
\end_inset

 to 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

.
 The upward weights serve the purpose of inferring the 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 representation of the data, and the downward weights are generative and
 part of the model.
 Place the original RBM (that had been between 
\begin_inset Formula $\mathbf{V_{0}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

) on top of the new layer 
\begin_inset Formula $\mathbf{V_{1}}$
\end_inset

, so that it is now between 
\begin_inset Formula $\mathbf{V_{1}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{1}}$
\end_inset

.
 Up to now this model is equivalent to running an RBM for one more iteration,
 which is implemented by the extra directed layer with shared weights below
 the RBM.
\end_layout

\begin_layout Standard
Infer the hidden layer representation at 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

 (i.e.
 feed the activations of the visible layer 
\begin_inset Formula $\mathbf{V_{0}}$
\end_inset

 through the activation function of 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

).
 Now train the copied RBM between layers 
\begin_inset Formula $\mathbf{V_{1}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{1}}$
\end_inset

 and adapt its weights.
 This greedy step makes the weights different from those between 
\begin_inset Formula $\mathbf{V_{0}}$
\end_inset

, 
\begin_inset Formula $\mathbf{H_{0}}$
\end_inset

, and 
\begin_inset Formula $\mathbf{V_{1}}$
\end_inset

, and thus the upward (inferring) weights between 
\begin_inset Formula $\mathbf{V_{0}}\leftarrow\mathbf{H_{0}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{H_{0}}\leftarrow\mathbf{V_{1}}$
\end_inset

 become incorrect in theory.
 In practice, however, this does not matter that much.
 The gain by training the RBM on top on re-represented data outweighs the
 incorrectness of inference.
 Because the weights of the new RBM before training are initialized to be
 the same as that of the old RBM, additional constrastive divergence steps
 can only improve the model.
\end_layout

\begin_layout Standard
Iteratively adding two new layers between the directed layers of the Deep
 Belief Network and the RBM on top can now continue until the model is sufficien
tly deep.
\end_layout

\begin_layout Paragraph
The RBM Seen as the DBN's Markov Chain in Reverse
\end_layout

\begin_layout Standard
Another way to view the unrolling is to regard the infinitely Deep Belief
 Network as the process implementing the original Markov Chain.
 The Markov Chain implemented by the RBM in the unrolling of the DBN can
 then be regarded as simulating the same Markov Chain run in reverse, and
 the more layers we add to the DBN, the farther back we go in time.
\end_layout

\begin_layout Subsubsection
Deep Belief Networks Interpreted as Feed-forward Neural Networks
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: ist der folgende Paragraph √ºberfl√ºssig und verwirrend?
\end_layout

\end_inset

A trained DBN can be reinterpreted as a feed-forward neural network.
 In particular, the weights and biases of a trained DBN can be transferred
 to a multi-layer feed-forward neural network with the same architecture
 as the DBN, thereby making the stochastic DBN a deterministic neural network.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
TODO: See 
\begin_inset Quotes eld
\end_inset

An Introduction to Restricted Boltzmann Machines
\begin_inset Quotes erd
\end_inset

 by Asja Fischer and Christian Igel.
 Paragraph starting at 
\begin_inset Quotes eld
\end_inset

It is an important property that single as well as stacked RBMs
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

 This process is also called 
\emph on
pre-training
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
pre-training
\end_layout

\end_inset

.
 Furthermore, another neural network can be put on top of the pre-trained
 converted DBN, where the final (output) layer has neurons corresponding
 to variables to be predicted.
 Usually the network consists of only one layer, due to difficulties in
 training freshly initialized multi-layer neural networks.
 The resulting network can then be 
\emph on
fine-tuned
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
fine-tuning
\end_layout

\end_inset

, using standard back-propagation, into a configuration that can predict
 from input variables (input at the bottom of the network) the output variables
 (read off at the top of the network).
\end_layout

\begin_layout Standard
In such a composite structure, the (unsupervisedly trained) DBN weights
 take on the responsibility of re-representing the data so that it is in
 an abstracted form that has for example correlating variables replaced
 by a single variable indicating whether a feature is present in the sample
 or not.
 The (supervisedly trained) weights on top of the network have the responsibilit
y to label the sample, i.e.
 indicate whether the abstracted representation of the sample is of a certain
 form or not.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: HIER WEITERMACHEN.
 Erkl√§ren, wie wake-sleep funktioniert, oder wie ein DBN mittels stacking
 RBMs gelernt werden kann.
 Wie geht das mit den uncoupled upward weights? Warum optimiert das inference
 Network die falsche Kullback-Leibler divergence?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
How Unlabeled Data Was Used in Training
\end_layout

\begin_layout Subsubsection
How Unlabeled Data was Used in Training Autoencoders
\end_layout

\begin_layout Standard
Because autoencoders
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
autoencoder
\end_layout

\end_inset

 are composed of an encoder
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
encoder
\end_layout

\end_inset

 and a decoder
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
decoder
\end_layout

\end_inset

 and the encoder tries to compress the input data and the decoder tries
 to reconstruct the input from its compressed form, training them is unsupervise
d.
 As a general rule only the samples designated as 
\begin_inset Quotes eld
\end_inset

unlabeled
\begin_inset Quotes erd
\end_inset

 were used in training the unsupervised part of an algorithm.
\end_layout

\begin_layout Standard
The decoder network of the trained autoencoder was then thrown away, so
 that only the encoder remained.
 Classification was done on the compressed representation of the input.
 The classifier network was built on top of the encoder (see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:supervised-network-on-top-of-unsupervised"

\end_inset

), and the compressed output layer of the encoder was used as the input
 layer of the classifier.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/progress_report_201411-unsupervised-to-supervised.eps
	width 35page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:supervised-network-on-top-of-unsupervised"

\end_inset

Example scheme showing how the supervised network is trained on top of a
 pre-trained unsupervised network.
 The network receives input from the bottom, and the resulting label is
 read off at the top.
 The unsupervised network can be more than 2 layers deep.
 It is trained first during pre-training.
 Then the supervised network is appended onto the top and trained together
 with the unsupervised network.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The weights of the classifier were initialized randomly, and standard back-propa
gation was used to train the whole network to classify samples.
 This means that the encoder can be modified by back-propagation training,
 but pre-training with the autoencoder initializes it to a configuration
 that 
\begin_inset Quotes eld
\end_inset

knows
\begin_inset Quotes erd
\end_inset

 about the unlabeled samples.
 The learning rate of the second training run (
\emph on
fine-tuning
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
fine-tuning
\end_layout

\end_inset

) should be small enough not to diverge from the unlabeled training configuratio
n in too large steps (per iteration).
\end_layout

\begin_layout Subsubsection
How Unlabeled Data was Used When Pre-training a Restricted Boltzmann Machine
 and Fine-tuning Using Back-propagation
\end_layout

\begin_layout Standard
When using the unsupervised Restricted Boltzmann Machine to find the approximate
 weights of a neural network, the unlabeled data were used to train the
 Restricted Boltzmann Machine.
 The resulting neural network was then extended with the network layer for
 supervised classification, and the complete network was trained using (supervis
ed) back-propagation on the labeled data.
\end_layout

\begin_layout Subsubsection
How Unlabeled Data was Used in Training Deep Belief Networks
\end_layout

\begin_layout Standard
As was said in section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sub:Deep-Belief-Network"

\end_inset

, Deep Belief Networks are an unsupervised algorithm and thus were trained
 using unlabeled data.
 As with Restricted Boltzmann Machines, the resulting network was then extended
 with the classifier layer, and the complete network was fine-tuned using
 back-propagation on the labeled data.
\end_layout

\end_body
\end_document

#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
fix-cm
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section*
Notation
\end_layout

\begin_layout Description
Random
\begin_inset space ~
\end_inset

variable The name of a random variable is written upper-case.
 For example: 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Description
Value
\begin_inset space ~
\end_inset

of
\begin_inset space ~
\end_inset

a
\begin_inset space ~
\end_inset

random
\begin_inset space ~
\end_inset

variable The value of a random variable is written lower-case.
 For example, the value of variable 
\begin_inset Formula $X$
\end_inset

 is written 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Description
Vector Vectors or sets are written in bold font.
 For example, the variable 
\begin_inset Formula $\mathbf{X}$
\end_inset

 could represent the variables 
\begin_inset Formula $\{X_{1},X_{2},X_{3}\}$
\end_inset

.
 And the vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 could mean the values 
\begin_inset Formula $\{x_{1},x_{2},x_{3}\}$
\end_inset

 of the variables 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: At the end of the manuscript, I could add a list of 
\emph on
emphasized 
\emph default
terms introduced, together with the page number they were introduced on.
 Die emphasized Terms suche ich wahrscheinlich am besten im LyX-Quelltext.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Graphical Models
\end_layout

\begin_layout Paragraph
Graphs
\end_layout

\begin_layout Standard
A 
\emph on
graph 
\emph default
is a set 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 of nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

 and edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

.
 An edge 
\begin_inset Formula $\mathbf{E}\ni e=(n_{1},n_{2})$
\end_inset

 consists of a pair of nodes 
\begin_inset Formula $n_{1}$
\end_inset

 and 
\begin_inset Formula $n_{2}$
\end_inset

.
 The edges can be directed or undirected (i.e.
 the pair 
\begin_inset Formula $(n_{1},n_{2})$
\end_inset

 can be ordered or unordered).
 If all edges are directed, then the graph is called a 
\emph on
directed graph
\emph default
; if all edges are undirected, then the graph is called a 
\emph on
undirected graph
\emph default
.
\end_layout

\begin_layout Standard
A 
\emph on
directed acyclic graph 
\emph default
(DAG) is a directed graph that does not contain (directed) cycles.
 This means if we start at any node and only go into the direction of the
 edges, it is impossible to reach a node visited earlier.
\end_layout

\begin_layout Standard
A 
\emph on
clique 
\emph default
in an undirected graph is a subset of nodes 
\begin_inset Formula $\mathbf{N_{C}}\subset\mathbf{N}$
\end_inset

, such that every pair of nodes in the clique 
\begin_inset Formula $n_{1},n_{2}\in\mathbf{N_{C}}$
\end_inset

 has an edge in the graph 
\begin_inset Formula $(n_{1},n_{2})\in\mathbf{E}$
\end_inset

.
 A 
\emph on
maximal clique
\emph default
 is a clique where there is no edge that can be added to it so that the
 resulting subset is still a clique.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: add definition of 
\begin_inset Quotes eld
\end_inset

separate
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A set of nodes 
\begin_inset Formula $\mathbf{N_{A}}\subset\mathbf{N}$
\end_inset

 is 
\emph on
separated 
\emph default
from a set of nodes 
\begin_inset Formula $\mathbf{N_{B}}\subset\mathbf{N}$
\end_inset

 by a set of nodes 
\begin_inset Formula $\mathbf{N_{S}}\subset\mathbf{N}$
\end_inset

, if it is impossible to go (along the edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

 of the graph) from a node 
\begin_inset Formula $N_{1}\in\mathbf{N_{A}}$
\end_inset

 to a node 
\begin_inset Formula $N_{2}\in\mathbf{N_{B}}$
\end_inset

 without passing through any of the nodes in 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Graphical Models
\end_layout

\begin_layout Standard
Graphical Models are an encoding of a joint probability distribution with
 the help of a graph.
\end_layout

\begin_layout Standard
In a graphical model, each node of the graph corresponds to a random variable
 of the joint probability distribution, and the dependencies between the
 random variables are encoded in the edges of the graph.
 In addition to the graph structure, probability functions of the elements
 of the structure have to be defined that are equivalent to the target joint
 probability.
 Let's illustrate this with an example.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: hier muss ein Beispiel mit einer Tabelle einer joint probability distribut
ion stehen.
\end_layout

\begin_layout Plain Layout
TODO: Hier eine Figure mit einem DAG und einem MRF.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the following we will consider only variables with discrete values in
 the joint probability distribution.
\end_layout

\begin_layout Paragraph
Directed and Undirected Graphical Models
\end_layout

\begin_layout Standard
Graphical models can be directed or undirected.
 Graphical models are based on (directed acyclic or undirected) graphs with
 nodes and edges, where each node corresponds to a random variable, and
 the edges encode relations between random variables.
 Directed acyclic graphical models are also called 
\emph on
Bayesian networks
\emph default
, and undirected graphical models are called 
\emph on
Markov random fields
\emph default
.
\end_layout

\begin_layout Standard
(Besides other types of graphs, there is also a unification of Bayesian
 networks and Markov random fields, i.e.
 a graphical model that can have both directed and undirected edges (subject
 to some restrictions of cycles).
 These networks are called 
\emph on
chain graphs
\emph default
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
TODO: reference.
 Referenz zu chain graph ist http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html
 (
\begin_inset Quotes erd
\end_inset

It is possible to have a model with both directed and undirected arcs, which
 is called a chain graph.
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_inset

, or 
\emph on
partially directed acyclic graphs
\emph default
.)
\end_layout

\begin_layout Subparagraph
Undirected Graphical Model
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
I should at some point write that in undirected graphical models, the probabilit
y is factored into potentials (written 
\begin_inset Formula $\phi$
\end_inset

), where each factor corresponds to a clique in the undirected graph.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Assume that we want to encode a joint probability distribution 
\begin_inset Formula $P$
\end_inset

 into an undirected graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

.
 We could use for every random variable a node, and connect all nodes with
 all others.
 This 
\emph on
complete graph 
\emph default
encodes no conditional independencies.
 However, to minimize computation time we normally want to get a minimal
 graph that still encodes the joint probability distribution faithfully.
 What properties does 
\begin_inset Formula $P$
\end_inset

 have to fulfill and what does the minimal undirected graph 
\begin_inset Formula $G$
\end_inset

 look like?
\end_layout

\begin_layout Standard
The 
\emph on
Hammersley-Clifford theorem 
\emph default
(
\begin_inset CommandInset citation
LatexCommand cite
key "HammersleyClifford1971"

\end_inset

) is helpful here.
 It states that when we have a vector of random variables 
\begin_inset Formula $\mathbf{X}=\{X_{1},\dots,X_{n}\}$
\end_inset

, its strictly positive joint probability distribution 
\begin_inset Formula $P(\mathbf{X})$
\end_inset

 with 
\begin_inset Formula $P(\mathbf{x})>0$
\end_inset

 for all possible values 
\begin_inset Formula $\mathbf{x}$
\end_inset

 of 
\series bold

\begin_inset Formula $\mathbf{X}$
\end_inset


\series default
, and an undirected graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 with each node corresponding to a random variable (i.e.
 
\begin_inset Formula $\mathbf{N}=\{X_{1},\dots,X_{n}\}$
\end_inset

), then the following statements are equivalent:
\end_layout

\begin_layout Itemize
\begin_inset Formula $P(\mathbf{X})$
\end_inset

 is a 
\emph on
Gibbs distribution 
\emph default
that factorizes according to the maximal cliques 
\begin_inset Formula $C_{1},\dots,C_{m}$
\end_inset

 in 
\begin_inset Formula $G$
\end_inset

, i.e.
 
\begin_inset Formula $P(\mathbf{X})=\frac{1}{Z}\phi_{1}(\mathbf{C_{1}})\cdot\dots\cdot\phi_{m}(C_{m})$
\end_inset

 where 
\begin_inset Formula $Z$
\end_inset

 is the 
\emph on
partition function
\emph default
, and 
\begin_inset Formula $\phi_{i}(C_{i})$
\end_inset

 are the 
\emph on
potential functions
\emph default
.
\begin_inset Note Note
status open

\begin_layout Plain Layout
On page 21 of the Hammersley-Clifford paper, the authors define the name
 
\begin_inset Quotes eld
\end_inset

light-coloured potential function
\begin_inset Quotes erd
\end_inset

.
 There, they also define 
\begin_inset Quotes eld
\end_inset

Gibbsian ensemble
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
the 
\emph on
local Markov property 
\emph default
holds for the graph 
\begin_inset Formula $G$
\end_inset

 and the joint probability distribution 
\begin_inset Formula $P$
\end_inset

: Given the states of the random variables 
\begin_inset Formula $\mathbf{N_{neighbor}}$
\end_inset

 immediately connected to a node 
\begin_inset Formula $X_{i}$
\end_inset

, 
\begin_inset Formula $X_{i}$
\end_inset

 is conditionally independent from all other nodes 
\begin_inset Formula $\mathbf{N}\backslash\mathbf{N_{neighbor}}$
\end_inset

, i.e.
 
\begin_inset Formula $P(X_{i}|\mathbf{N_{neighbor}})=P(X_{i}|\mathbf{N}\backslash\mathbf{N_{neighbor}})$
\end_inset

, where 
\begin_inset Formula $\mathbf{N}\backslash\mathbf{N_{neighbor}}$
\end_inset

 is the set of nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

 without the neighboring nodes of 
\begin_inset Formula $X_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
the 
\emph on
global Markov property 
\emph default
holds for the graph 
\begin_inset Formula $G$
\end_inset

 and the joint probability distribution 
\begin_inset Formula $P$
\end_inset

: Given any disjoint subsets 
\begin_inset Formula $\mathbf{N_{A}},\mathbf{N_{B}},\mathbf{N_{S}}\subset\mathbf{N}$
\end_inset

 where 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

 separates the nodes 
\begin_inset Formula $\mathbf{N_{A}}$
\end_inset

 from the nodes 
\begin_inset Formula $\mathbf{N_{B}}$
\end_inset

, and given the states of the random variables of 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

, the nodes 
\begin_inset Formula $\mathbf{N_{A}}$
\end_inset

 are conditionally independent of the nodes 
\begin_inset Formula $\mathbf{N_{B}}$
\end_inset

: 
\begin_inset Formula $P(\mathbf{N_{A}}|\mathbf{N_{S}})=$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: look up global Markov property and insert formula here
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: insert graphics illustrating local and global Markov property
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: I should include the result of the Hammersley-Clifford theorem here.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This means that when we have a strictly positive joint probability distribution
 
\begin_inset Formula $P$
\end_inset

, we can construct the minimal undirected graph 
\begin_inset Formula $G$
\end_inset

 representing 
\begin_inset Formula $P$
\end_inset

 by starting from the completely connected graph and deleting the edges
 
\begin_inset Formula $E_{12}=(N_{1},N_{2})$
\end_inset

 whose ends are conditionally independent 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: conditionally independent on what? I think that's the crux of being
 NP; we probably have to enumerate all possible separators...?
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
A Markov random field must fulfill the 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: is there a difference between 
\begin_inset Quotes eld
\end_inset

global Markov propery
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Markov property
\begin_inset Quotes erd
\end_inset

? There is the local and the global Markov property.
\end_layout

\end_inset

Markov property.
 That is, a graph 
\begin_inset Formula $G=(\mathbf{N},\mathbf{E})$
\end_inset

 consisting of a given set of nodes 
\begin_inset Formula $\mathbf{N}$
\end_inset

 and edges 
\begin_inset Formula $\mathbf{E}$
\end_inset

 is not a Markov random field if there are any three sets of nodes not fulfillin
g the Markov property.
 This means that for all sets of nodes 
\begin_inset Formula $\mathbf{N_{A}}\subset\mathbf{N}$
\end_inset

 and 
\begin_inset Formula $\mathbf{N_{B}}\subset\mathbf{N}$
\end_inset

 separated by a third set of nodes 
\begin_inset Formula $\mathbf{N_{S}}\subset\mathbf{N}$
\end_inset

 the corresponding random variables in 
\begin_inset Formula $\mathbf{N_{A}}$
\end_inset

 must be conditionally independent from the random variables in 
\begin_inset Formula $\mathbf{N_{B}}$
\end_inset

 given the random variables in the separator 
\begin_inset Formula $\mathbf{N_{S}}$
\end_inset

.
\end_layout

\begin_layout Subparagraph
Example of an Undirected Graph Model
\end_layout

\begin_layout Standard
For example, if there are the three cliques 
\begin_inset Formula $\mathbf{C_{1}}=\{X_{1},X_{2},X_{3}\}$
\end_inset

, 
\begin_inset Formula $\mathbf{C_{2}}=\{X_{3},X_{4}\}$
\end_inset

, and 
\begin_inset Formula $\mathbf{C_{3}}=\{X_{3},X_{5}\}$
\end_inset

, then the joint probability distribution 
\begin_inset Formula $P$
\end_inset

 (also called Gibbs distribution) can be written as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(X_{1},\dots,X_{5})=\frac{1}{Z}\phi_{1}(\mathbf{C_{1}})\phi_{2}(\mathbf{C_{2}})\phi_{3}(\mathbf{C_{3}})
\]

\end_inset

, where 
\begin_inset Formula $\phi_{1}(\mathbf{C_{1}})=\phi(X_{1},X_{2},X_{3})$
\end_inset

 is the potential function of clique 1 (
\emph on
clique potential
\emph default
), and is a function of the 3 random variables 
\begin_inset Formula $X_{1},X_{2},X_{3}$
\end_inset

 in the clique.
 
\begin_inset Formula $Z$
\end_inset

 is the partition function and must normalize the function so that 
\begin_inset Formula $P$
\end_inset

 is a probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Z=\sum_{X_{1},\dots,X_{n}}\phi_{1}(\mathbf{C_{1}})\phi_{2}(\mathbf{C_{2}})\phi_{3}(\mathbf{C_{3}})
\]

\end_inset


\end_layout

\begin_layout Standard
In practice, 
\begin_inset Formula $\phi_{1}(X_{1},X_{2},X_{3})$
\end_inset

 can be represented by a table that has, for each possible combination of
 states of the three random variables, a positive real number.
 For example, if each of the three random variables has two states, then
 the table (with 
\begin_inset Formula $2^{3}$
\end_inset

 entries) could look like this:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\phi(X_{1}=x_{1},X_{2}=x_{2},X_{3}=x_{3})$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.124
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.553
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.842
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.258
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Paragraph
Generative and Discriminative Models
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: I'm not sure this belongs here, but I should introduce the concept
 of a 
\begin_inset Quotes eld
\end_inset

Generative Model
\begin_inset Quotes erd
\end_inset

 to be able to refer to it later when introducing Deep Belief Networks from
 RBMs.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Bayesian Networks
\end_layout

\begin_layout Standard
As said before, directed acyclic graphical models are also called 
\emph on
Bayesian Networks
\emph default
 or 
\emph on
Belief Networks
\emph default
.
\end_layout

\begin_layout Paragraph
Inference in Graphical Models
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Do 
\begin_inset Formula $\mathbf{K},\mathbf{W},\mathbf{U}$
\end_inset

 have to fulfill some relationship for directed acyclic graphs in the inference
 example below? (For example, does 
\begin_inset Formula $\mathbf{K}$
\end_inset

 have to be a subset of the parents of 
\series bold

\begin_inset Formula $\mathbf{W}$
\end_inset


\series default
?)
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Exact Inference in directed and in undirected graphical models
\end_layout

\begin_layout Standard
A graphical model is a representation of a joint probability distribution.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: I'm not sure I know how 
\begin_inset Quotes eld
\end_inset

Inference
\begin_inset Quotes erd
\end_inset

 is defined.
 I should look it up in a book, e.g.
 
\begin_inset Quotes eld
\end_inset

Probabilistic Graphical Model, by Koller and Friedman
\begin_inset Quotes erd
\end_inset

.
 But that book is not downloadable.
\end_layout

\begin_layout Plain Layout
There is a paper, however: http://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pd
f
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Inference in a graphical model is the task of answering a query about the
 joint probability distribution encoded by the graph.
 However, in the general case this takes exponential time.
 An example will illustrate this.
\end_layout

\begin_layout Standard
For example, we might want to infer the probability of a configuration of
 variables when the values of only some of the variables are known.
 This means that we can partition the variables 
\begin_inset Formula $\mathbf{V}$
\end_inset

 of a graphical model into three disjoint groups:
\end_layout

\begin_layout Enumerate
the known variables 
\begin_inset Formula $\mathbf{K}$
\end_inset

,
\end_layout

\begin_layout Enumerate
the unknown variables 
\begin_inset Formula $\mathbf{W}$
\end_inset

 that we want to know the probability distribution of,
\end_layout

\begin_layout Enumerate
the unknown variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

 that we do not care about.
\end_layout

\begin_layout Standard
Let the known values of 
\begin_inset Formula $\mathbf{K}$
\end_inset

 be written 
\series bold

\begin_inset Formula $\mathbf{k}$
\end_inset


\series default
.
 The unknown values of 
\begin_inset Formula $\mathbf{W}$
\end_inset

 are named 
\series bold

\begin_inset Formula $\mathbf{w}$
\end_inset


\series default
, and the values of 
\begin_inset Formula $\mathbf{U}$
\end_inset

, 
\begin_inset Formula $\mathbf{u}$
\end_inset

.
 
\end_layout

\begin_layout Standard
When we want to find the probability of configuration 
\begin_inset Formula $\mathbf{W}=\mathbf{w}$
\end_inset

, given 
\begin_inset Formula $\mathbf{K}=\mathbf{k}$
\end_inset

, we can first write the query in terms of the joint probability distribution.
 After that we marginalize out the unknown variables 
\begin_inset Formula $\mathbf{U}$
\end_inset

 that we do not care about, and condition on 
\begin_inset Formula $\mathbf{K}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k}) & = & \sum_{\mathbf{U}}P(\mathbf{W}=\mathbf{w},\mathbf{U}=\mathbf{u}|\mathbf{K}=\mathbf{k})\nonumber \\
 & = & \sum_{\mathbf{U}}\frac{P(\mathbf{W}=\mathbf{w},\mathbf{U}=\mathbf{u},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\label{eq:Inference in graphical models}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: write that in the general case, this query is exponential.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the above formula there is a sum over all variables 
\series bold

\begin_inset Formula $\mathbf{U}$
\end_inset


\series default
.
 Writing this out, we obtain:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k})\nonumber \\
= & \sum_{\mathbf{U}}\frac{P(\mathbf{W}=\mathbf{w},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\nonumber \\
= & \sum_{U_{1}}\sum_{U_{2}}\cdots\sum_{U_{n}}\frac{P(\mathbf{W}=\mathbf{w},U_{1}=u_{1},U_{2}=u_{2},\dots,U_{n}=u_{n},\mathbf{K}=\mathbf{k})}{P(\mathbf{K}=\mathbf{k})}\label{eq:Inference in graphical models, written out}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In the general case (if the joint probability cannot be factorized), this
 nested sum needs 
\begin_inset Formula $O(|\mathbf{u}|^{|\mathbf{U}|})=O(|\mathbf{u}|^{n})$
\end_inset

 operations to compute, where 
\begin_inset Formula $|\mathbf{u}|$
\end_inset

 is the number of possible values a variable 
\begin_inset Formula $U_{i}$
\end_inset

 can have (assuming for simplicity that all random variables 
\begin_inset Formula $U_{i}$
\end_inset

 have the same number of possible values 
\begin_inset Formula $|\mathbf{u}|$
\end_inset

) and 
\begin_inset Formula $|\mathbf{U}|$
\end_inset

 is the number of unknown variables 
\begin_inset Formula $U_{i}$
\end_inset

.
 This is because all possible combinations of variable assignments have
 to be considered.
 Thus, run-time is exponential in the number of variables, and therefore
 intractable.
\end_layout

\begin_layout Standard
This can be improved by factorizing the joint probability into independent
 sub-joint-probabilities, if possible.
 For example, if the random variables 
\begin_inset Formula $\mathbf{U}=\{U_{1},U_{2},\dots,U_{7}\}$
\end_inset

 can be partitioned into three pairwise independent cliques 
\begin_inset Formula $\mathbf{C_{1}}=\{U_{1},U_{2}\},$
\end_inset

 
\begin_inset Formula $\mathbf{C_{2}}=\{U_{2},U_{3}\}$
\end_inset

, 
\begin_inset Formula $\mathbf{C_{3}}=\{U_{3},U_{4},U_{5}\}$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Note that the random variables 
\begin_inset Formula $U$
\end_inset

 overlap between clusters, so that the clusters are connected.
\end_layout

\end_inset

, so that 
\begin_inset Formula $P(\mathbf{U})=P(\mathbf{C_{1}})P(\mathbf{C_{2}})P(\mathbf{C_{3}})$
\end_inset

, then above sum can be written as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(\mathbf{W}=\mathbf{w}|\mathbf{K}=\mathbf{k}) & =\\
 & = & \sum_{\mathbf{C_{1}}}P(\mathbf{W}=\mathbf{w},\mathbf{C_{1}}|\mathbf{K}=\mathbf{k})*\\
 &  & \sum_{\mathbf{C_{2}}}P(\mathbf{W}=\mathbf{w},\mathbf{C_{2}}|\mathbf{K}=\mathbf{k})*\\
 &  & \sum_{\mathbf{C_{3}}}P(\mathbf{W}=\mathbf{w},\mathbf{C_{3}}|\mathbf{K}=\mathbf{k})\\
 & = & \prod_{\mathbf{C}\in\{\mathbf{C_{1}},\mathbf{C_{2}},\mathbf{C_{3}}\}}\sum_{\mathbf{C}}P(\mathbf{W}=\mathbf{w},\mathbf{C}|\mathbf{K}=\mathbf{k})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The runtime of this formula is dominated by the largest clique, and so the
 runtime is 
\begin_inset Formula $O(|\mathbf{u}|^{|\mathbf{C_{l}}|})$
\end_inset

, where 
\begin_inset Formula $\mathbf{C_{l}}$
\end_inset

 is the clique with the largest number of variables in it (in our case 
\begin_inset Formula $\mathbf{C_{l}}=\mathbf{C_{3}}$
\end_inset

, because 
\begin_inset Formula $|\mathbf{C_{3}}|=3$
\end_inset

, and 
\begin_inset Formula $|\mathbf{C_{1}}|=|\mathbf{C_{2}}|=2$
\end_inset

).
 This is still an exponential run-time.
 Therefore, in practice, the joint probability is approximated, for example
 by Gibbs sampling.
\end_layout

\begin_layout Paragraph
Gibbs sampling
\end_layout

\begin_layout Standard
Gibbs sampling is a type of Markov Chain Monte Carlo (MCMC) algorithm.
\end_layout

\begin_layout Paragraph
Explaining away
\end_layout

\begin_layout Standard
Directed Acyclic Graphical Models show the phenomenon of 
\begin_inset Quotes eld
\end_inset

explaining away
\begin_inset Quotes erd
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: is there also explaining away in undirected graphs? nope, I don't
 think so.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In a directed graph Explaining away
\end_layout

\begin_layout Section
Artificial Neural Networks
\end_layout

\begin_layout Paragraph
Hopfield Network
\end_layout

\begin_layout Standard
A Hopfield Network is a deterministic recurrent network with nodes 
\begin_inset Note Note
status open

\begin_layout Plain Layout
As this is not a stochastic network, I don't think it makes sense to define
 random variables 
\begin_inset Formula $\mathbf{N}=\{N_{1},\dots,N_{m}\}$
\end_inset

.
\end_layout

\end_inset

, whose states 
\begin_inset Formula $\mathbf{n}=\{n_{1},n_{m}\}$
\end_inset

 are binary, i.e.
 
\begin_inset Formula $n_{i}\in\{0,1\}$
\end_inset

 for all nodes 
\begin_inset Formula $i$
\end_inset

.
 Each node has a connection with all others (but not itself).
 The connection from node 
\begin_inset Formula $N_{i}$
\end_inset

 to node 
\begin_inset Formula $N_{j}$
\end_inset

 is directed and has a real weight 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

.
 (As there is no connection from node 
\begin_inset Formula $i$
\end_inset

 to node 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $w_{ii}=0$
\end_inset

 for all nodes 
\begin_inset Formula $i$
\end_inset

.) There is also a real-valued bias 
\begin_inset Formula $b_{i}\in\mathbb{R}$
\end_inset

 for each node 
\begin_inset Formula $i$
\end_inset

.
 (This means each node has 
\begin_inset Formula $m-1$
\end_inset

 outgoing connections and 
\begin_inset Formula $m-1$
\end_inset

 incoming connections.)
\end_layout

\begin_layout Standard
The network is updated iteratively, this means the states of time step 
\begin_inset Formula $t$
\end_inset

 depend only on the states at time step 
\begin_inset Formula $t-1$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
n_{j}^{(t)}=f(\sum_{i\in\{1,\dots,m\}\land i\neq j}n_{i}^{(t-1)}w_{ij}+b_{j})
\]

\end_inset


\begin_inset Newline newline
\end_inset

The activation function 
\begin_inset Formula $f$
\end_inset

 is a stepping function
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: there must be a better word instead of 
\begin_inset Quotes eld
\end_inset

stepping function
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

, with negative values mapped to 0, and positive values mapped to 1: 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: stimmt die Definition von 
\begin_inset Formula $f$
\end_inset

 überhaupt?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(x)=\begin{cases}
0 & x\leq0\\
1 & x>0
\end{cases}
\]

\end_inset


\begin_inset Newline newline
\end_inset

While this updating rule was described as synchronous (i.e.
 all nodes are updated at the same time), the network can also be updated
 asynchronously (i.e.
 at each time step a node is picked at random and its state is updated.).
 Although a hopfield network is recurrent, Hopfield 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: citation.
 TODO: stimmt das überhaupt? In 
\begin_inset Quotes eld
\end_inset

Neural networks and physical systems with emergent collective computational
 abilities
\begin_inset Quotes erd
\end_inset

, Hopfield writes: 
\begin_inset Quotes eld
\end_inset

A simple cycle also occurred occasionally
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The third behavior seen was chaotic wandering in a small region of state
 space.
\begin_inset Quotes erd
\end_inset

 TODO: Ausserdem steht in den beiden Hopfield-Papers nichts über einen Beweis,
 dass das Netzwerk konvergiert.
\end_layout

\end_inset

 proved that the updating rule converges to a (local) minimum.
\end_layout

\begin_layout Standard
This can be also viewed in terms of an energy: Each state 
\begin_inset Formula $\mathbf{N}=\mathbf{n}$
\end_inset

 has an energy 
\begin_inset Formula $E$
\end_inset

 associated with it: 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: complete
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E=\sum\label{eq:Energy of a Hopfield network}
\end{equation}

\end_inset


\begin_inset Newline newline
\end_inset

As time progresses, 
\begin_inset Formula $E$
\end_inset

 becomes smaller and smaller, i.e.
 
\begin_inset Formula $E^{(t)}\leq E^{(t-1)}$
\end_inset

.
\end_layout

\begin_layout Standard
Training a Hopfield network is the task of finding weights 
\begin_inset Formula $w_{ij}$
\end_inset

 and biases 
\begin_inset Formula $b_{i}$
\end_inset

, so that desirable states (training patterns) have a low energy and undesirable
 states have a high energy.
 After training, a Hopfield network can be initialized with a distorted
 pattern (i.e.
 state), and updated iteratively until its state doesn't change anymore.
 This stationary state should then be equal to a similar pattern used in
 training.
 Due to this property Hopfield networks can be used as 
\emph on
associative memory
\emph default
.
\end_layout

\begin_layout Paragraph
Multilayer Perceptron
\end_layout

\begin_layout Standard
A Multilayer Perceptron belongs to the class of feedforward neural networks.
 These are in contrast to recurrent networks, which contain a directed cycle
 of neural connections.
\end_layout

\begin_layout Paragraph
Multilayer feedforward networks as universal function approximators
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HornikWhite1989"

\end_inset

 found that artificial feedforward neural networks with as few as one hidden
 layer are capable of modeling virtually any function within a given error,
 provided the following conditions are met:
\end_layout

\begin_layout Itemize
The activation function must be a 
\begin_inset Quotes eld
\end_inset

squashing
\begin_inset Quotes erd
\end_inset

 function: A squashing function 
\begin_inset Formula $s(x)$
\end_inset

 must be non-decreasing, 
\begin_inset Formula $\lim_{x\rightarrow\infty}s(x)=1$
\end_inset

 and 
\begin_inset Formula $\lim_{x\rightarrow-\infty}s(x)=0$
\end_inset

.
\end_layout

\begin_layout Itemize
Sufficiently many hidden nodes must be available.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HornikWhite1989"

\end_inset

 also note that 
\begin_inset Quotes eld
\end_inset

This [result] implies that any lack of success in applications must arise
 from inadequate learning, insufficient numbers of hidden units or the lack
 of a deterministic relationship between input and target.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Section
Restricted Boltzmann Machines
\end_layout

\begin_layout Paragraph
Boltzmann Machines
\end_layout

\begin_layout Standard
What are Boltzmann Machines? 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Example figure of a BM.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Inference in Boltzmann Machines
\end_layout

\begin_layout Standard
How is inference complicated in general Boltzmann Machines?
\end_layout

\begin_layout Paragraph
Restricted Boltzmann Machines
\end_layout

\begin_layout Standard
What are Restricted Boltzmann Machines? 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Example figure of an RBM.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A Restricted Boltzmann Machine has a bipartite topology: there are visible
 nodes and hidden nodes, and each node in the visible layer is connected
 (via an undirected edge) to all hidden nodes, but there are no visible-to-visib
le node connections and no hidden-to-hidden node connections.
\end_layout

\begin_layout Standard
As originally proposed by Hinton 
\begin_inset Note Note
status open

\begin_layout Plain Layout
and ...
 the guy who named it 
\begin_inset Quotes eld
\end_inset

Harmonium
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

, a Restricted Boltzmann Machine has binary visible and hidden nodes.
 There are extensions to real-valued nodes, however.
\end_layout

\begin_layout Paragraph
Analogy to Hopfield Networks
\end_layout

\begin_layout Standard
A Restricted Boltzmann Machine is a stochastic version of a Hopfield network.
 When choosing a zero temperature 
\begin_inset Formula $T=0$
\end_inset

, the Restricted Boltzmann Machine becomes deterministic and equivalent
 to a Hopfield network.
\end_layout

\begin_layout Standard
The energy of a hopfield network was defined in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Energy of a Hopfield network"

\end_inset

 (
\begin_inset CommandInset ref
LatexCommand vpageref
reference "eq:Energy of a Hopfield network"

\end_inset

) as:
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: repeat energy of equation 
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Explaining away in RBMs
\end_layout

\begin_layout Standard
There could be the problem of 
\begin_inset Quotes eld
\end_inset

explaining away
\begin_inset Quotes erd
\end_inset

 in multi-layer RBMs.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: https://www.quora.com/Why-does-the-phenomenon-of-explaining-away-make-infere
nce-difficult-in-directed-belief-nets?share=1
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Difficulties in training multi-layer neural networks
\end_layout

\begin_layout Standard
Training a feed-forward neural network with more than 1 hidden layer using
 back-propagation is difficult and usually does not succeed.
 This is probably due to the many local minima (
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
TODO: visualise or describe in words the many bumps created by adding many
 weighted sigmoid functions.
\end_layout

\end_inset

) of the implicitly optimized energy function during back-propagation.
\end_layout

\begin_layout Paragraph
RBMs can be interpreted as feed-forward neural networks
\end_layout

\begin_layout Standard
A trained stacked RBM can be reinterpreted as a feed-forward neural network.
 In particular, the weights and biases of a trained stacked RBM can be transferr
ed to a multi-layer feed-forward neural network with the same architecture
 as the stacked RBM, thereby making the stochastic RBM a deterministic neural
 network.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
TODO: See 
\begin_inset Quotes eld
\end_inset

An Introduction to Restricted Boltzmann Machines
\begin_inset Quotes erd
\end_inset

 by Asja Fischer and Christian Igel.
 Paragraph starting at 
\begin_inset Quotes eld
\end_inset

It is an important property that single as well as stacked RBMs
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

 This process is also called 
\emph on
pre-training
\emph default
.
 Furthermore, another neural network (usually only one layer, due to the
 training difficulties of multi-layer neural networks) can be put on top
 of the pre-trained converted RBM, where the final (output) layer has neurons
 corresponding to variables to be predicted.
 The resulting network can be then be fine-tuned, using standard back-propagatio
n, into a configuration that can predict from input variables (input at
 the bottom of the network) the output variables (read off at the top of
 the network).
\end_layout

\begin_layout Paragraph
Learning Rule in Restricted Boltzmann Machines
\end_layout

\begin_layout Standard
Usually, the learning rule includes a 
\begin_inset Quotes eld
\end_inset

momentum
\begin_inset Quotes erd
\end_inset

 term.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Formel hinschreiben
\end_layout

\end_inset

 This term works like a low-pass filter and reduces oscillations during
 learning by smoothing the weight and bias deltas added to the parameters
 of the network.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: maybe the following paragraph should go into Results, not Introduction.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Because the momentum term includes a coefficient that describes the fraction
 of the weights deltas in the previous time step to be added to the current
 weight deltas, it adds another meta-parameter to training.
 Sometimes the coefficient is even changed during training, usually gradually
 increased during the early steps of training to its final value.
 This is to prevent the 
\begin_inset Quotes eld
\end_inset

explosion
\begin_inset Quotes erd
\end_inset

 of the model during training, which happens when the training does not
 converge, and can be caused by a too high momentum coefficient.
\end_layout

\begin_layout Subsection
Parameters of a Restricted Boltzmann Machine
\end_layout

\begin_layout Paragraph
Activation function
\end_layout

\begin_layout Standard
The hidden and visible nodes are a function of the sum of their inputs.
 The function that maps the sum of the inputs of a node to its value is
 called the 
\emph on
activation function
\emph default
.
 There are several activations functions:
\end_layout

\begin_layout Paragraph
Sigmoid activation function
\end_layout

\begin_layout Standard
This is a fairly standard activation function, often used in neural networks.
 It has the property that it is almost linear for inputs around zero, tends
 to 1 as its inputs go to positive infinity and to 0 as inputs go to negative
 infinity.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Kurve einer sigmoid-Funktion hier einfügen.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As the values of a Restricted Boltzmann Machine, as orignially proposed,
 are binary (i.e.
 either 0 or 1), the output of the sigmoid activation function is used as
 a probability that the output of the node assumes value 1.
 In this way the output is sampled from 
\begin_inset Formula $\{0,1\}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Linear units with independent Gaussian noise
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006"

\end_inset

 proposed a way to extend Restricted Boltzmann Machines with only binary
 values to nodes with real values.
\end_layout

\begin_layout Paragraph
Rectified linear activation function
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "NairHinton2010"

\end_inset

 then modified the idea in 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006"

\end_inset

 to rectified linear units, in which the sampled output of a unit is given
 by 
\begin_inset Formula $max(0,x+N(0,\sigma(x))$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is the sum of the inputs of the unit, 
\begin_inset Formula $\sigma(x)$
\end_inset

 is the variance of the input
\end_layout

\begin_layout Subsection
Regularizations of Restricted Boltzmann Machines
\end_layout

\begin_layout Paragraph
L1 Weight Decay
\end_layout

\begin_layout Paragraph
L2 Weight Decay
\end_layout

\begin_layout Standard
As 
\begin_inset CommandInset citation
LatexCommand cite
key "FischerIgel2012"

\end_inset

 note, adding an L2 weight decay term to the corresponds to assuming a zero-mean
 Gaussian prior on the parameters.
\end_layout

\begin_layout Paragraph
Sparsity
\end_layout

\begin_layout Paragraph
Dropout
\end_layout

\begin_layout Standard
Dropout is usually combined with weight normalization.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: I think weight normalization is described a bit in the dropout paper.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Weight normalization
\end_layout

\begin_layout Section
Motivation for using RBMs on genetic data
\end_layout

\begin_layout Standard
There were successes in visual object/face recognition by RBMs.
 There is a certain similarity of visual data and genetic data like high
 correlation of neighboring pixels and certain genes.
\end_layout

\begin_layout Section
Bibliography
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: make the citations look like 
\begin_inset Quotes eld
\end_inset

[FischerIgel2012]
\begin_inset Quotes erd
\end_inset

, not 
\begin_inset Quotes eld
\end_inset

[1]
\begin_inset Quotes erd
\end_inset

.
 This is probably done using a Bibtex style file?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "zusammenfassung"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document

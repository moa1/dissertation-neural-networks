#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
DONE: maybe replace 
\begin_inset Quotes eld
\end_inset

neuronal network
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Quotes eld
\end_inset

neural network
\begin_inset Quotes erd
\end_inset

, which is the correct term according to dict.cc.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This is supposed to be a high-level description of neural networks, to be
 used in the thesis before the mathematical description of neural networks
 (currently 
\begin_inset Quotes eld
\end_inset

methods.lyx
\begin_inset Quotes erd
\end_inset

).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Was meint Claudio mit 
\begin_inset Quotes eld
\end_inset

Überhaupt gehören die beiden am 23.5.
 per Mail angesprochenen Themen definitiv beide in die Intro.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Personalized Medicine
\end_layout

\begin_layout Standard
A medium-term goal of medicine is 
\begin_inset Quotes eld
\end_inset

personalized medicine
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
personalized medicine
\end_layout

\end_inset


\begin_inset Quotes erd
\end_inset

, whose goal is to provide custom-tailored health care on an individual
 basis.
 For example, a standard treatment for breast cancer is chemotherapy, but
 not all patients profit from this treatment.
 The event that a patient has no sign of breast cancer after reductive surgery
 followed by chemotherapy is called 
\emph on
pathologic complete response
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
pathologic complete response
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
pCR
\end_layout

\end_inset

, and the opposite event that the patient still has cancerous tissue after
 this procedure is called 
\emph on
residual disease
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
residual disease
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
RD
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Suppose there were a predictor that could tell the physicist how likely
 a patient is to benefit from chemotherapy.
 If the prediction for a certain patient was such that complete response
 to chemotherapy was unlikely, chemotherapy could be replaced by another
 therapy.
\end_layout

\begin_layout Standard
The goal of this work is to contribute to such a predictor.
 The input to the predictor is the molecular expression data, i.e.
 measures of the number of RNA copies of specific genes present in the cancer
 tissue.
 These gene expression measurements are ususally measured using microarrays
 or next generation sequencing.
 An artificial neural network then processes this data.
 The prediction is output by the network in the form of a number between
 0 and 1.
 Here, 0 means the patient is predicted with absolute certainty to have
 residual disease, 1 means the patient is predicted with absolute certainty
 to have pathologic complete response, and a number in-between is interpreted
 as the probability for pathologic complete response.
\end_layout

\begin_layout Standard
The study of neural networks in biology prompted the development of artificial
 neural networks as models of biological neural networks.
 After an introduction to biological and artificial neural networks we will
 give an overview of the relevant topics of machine learning and then introduce
 the own work done in this manuscript.
\end_layout

\begin_layout Section
Biological and Artificial Neural Networks
\end_layout

\begin_layout Standard
Artificial neural networks
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
artificial neural networks
\end_layout

\end_inset

 are mathematical constructs, designed to imitate the signal processing
 capabilities of real neurons, found in nearly all animals.
 Neurons can be connected to form complex neural networks.
 Like their biological counterparts, artificial neural networks consist
 of simpler building blocks, the neurons.
\end_layout

\begin_layout Subsection
Neurons As Basic Signal Processing Units
\end_layout

\begin_layout Standard
The biological neurons are defined (according to the neuron doctrine 
\begin_inset CommandInset citation
LatexCommand cite
key "BullockDouglas2005"

\end_inset

) as the smallest units whose state change may be called signal processing,
 so they are the basic signal processing units.
 They have multiple inputs at dendrites, and multiple outputs at axon terminals
 
\begin_inset CommandInset citation
LatexCommand cite
key "ByrneDafny1997"

\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Schematic-image-of-biological-neurons"

\end_inset

 gives a schematic overview of these elements.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/neurons.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Schematic-image-of-biological-neurons"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Schematic image of three biological neurons.
\end_layout

\end_inset

Schematic image of three biological neurons.
 A: neuron body B: nucleus C: dendrite D: synapse E: axon projecting from
 a distant neuron.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In most real neurons, the signal transmission and processing is facilitated
 by alternating small electric (action) potentials (along the axons) and
 chemical transmissions (at chemical synapses between axon and dendrite).
 The electric potential is transmitted along the dendrites of a neuron,
 and flows to the axon of the neuron, where it can lead to the release of
 neurotransmitters stored in the axon terminals into the synaptic cleft.
 The released neurotransmitters are detected by receptors and cause ion
 channels in the adjacent dendrites of other neurons to open, which changes
 their membrane potential.
 See figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Schema-of-chemical-synapse"

\end_inset

 for a depiction of axon, synaptic cleft, and dendrite.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/synapse.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Schema-of-chemical-synapse"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Schema of a chemical synapse.
\end_layout

\end_inset

Schema of a chemical synapse.
 The signal is transmitted from the axon terminal (left) to the dendrite
 (right).
 Grey: membranes of neurons.
 Green and blue: ion transporters maintain intracellular ion concentrations.
 Red: neurotransmitter is stored inside the cell in vesicles and emitted
 into the synaptic cleft upon an electric potential arriving at the axon
 terminal.
 Purple: receptors signal to the inside of the cell the absence or presence
 of neurotransmitter on the outside of the cell.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Action Potentials, Their Propagation, and Chemical Synapses 
\end_layout

\begin_layout Standard
The action potentials are realized by cells in the form of different ion
 concentrations inside and outside the cell.
 These ion gradients are maintained in the resting state by the 
\begin_inset Formula $Na^{+}/K^{+}$
\end_inset

-ATPases that pump 3 
\begin_inset Formula $Na^{+}$
\end_inset

 ions out of and 2 
\begin_inset Formula $K^{+}$
\end_inset

 ions into the cell for every ATP molecule 
\begin_inset CommandInset citation
LatexCommand cite
key "LodishZipursky2000"

\end_inset

.
 Because ions are charged, there is an electric potential between the outside
 and inside of the cell.
 The resting potential is between 
\begin_inset Formula $-80$
\end_inset

mV and 
\begin_inset Formula $-40$
\end_inset

mV, depending on the type of neuron.
 The electric potential becoming more positive is called depolarization,
 and the opposite hyperpolarization.
\end_layout

\begin_layout Standard
The propagation of the action potentials along dendrites is realized by
 the opening and closing of ion channels.
 Once depolarization of an adjacent region of a neuron causes the electric
 potential between the inside and outside of a 
\begin_inset Formula $Na^{+}$
\end_inset

 ion channel to reach a critical value, the ion channel opens, causing further
 depolarization in adjacent regions of the neuron.
 This positive feedback loop continues until all 
\begin_inset Formula $Na^{+}$
\end_inset

 channels are open.
 At the peak of depolarization, 
\begin_inset Formula $K^{+}$
\end_inset

 ion channels open, causing hyperpolarization, and the potential returns
 to the resting  potential.
 This makes the action potential travel along the neuron.
 Once it has reached an axon terminal, it causes neurotransmitter release.
\end_layout

\begin_layout Standard
Neurotransmitters binding to receptors
\begin_inset Note Note
status open

\begin_layout Plain Layout
for example, AMPA and NMDA receptors
\end_layout

\end_inset

 present on the outside of the neuron's membrane cause ion channels to open,
 and the ions flow into or out of the cell to achieve equilibrium of ion
 concentration.
 The type of ion channel being opened upon binding of a neurotransmitter
 can cause either depolarization or hyperpolarization of the dendrite, depending
 on the charge of the ion, and whether the resting concentration of the
 ion is higher intracellular or extracellular.
 If a critical threshold of depolarization is reached, the 
\begin_inset Formula $Na^{+}$
\end_inset

 ion channels will open, and an action potential 
\begin_inset Quotes eld
\end_inset

spike
\begin_inset Quotes erd
\end_inset

 is generated as described above.
\end_layout

\begin_layout Subsubsection
Encoding of Information in Action Potentials
\end_layout

\begin_layout Standard
The presence of a critical threshold suggests that it is not the 
\begin_inset Quotes eld
\end_inset

analog
\begin_inset Quotes erd
\end_inset

 electric potential, but the 
\begin_inset Quotes eld
\end_inset

digital
\begin_inset Quotes erd
\end_inset

 spike that carries the information from one neuron to the next.
 For example, the strength muscles are innervated with, is encoded in the
 number of action potentials per time delivered by the muscle neuron to
 the muscle fiber.
 However, some neurons involved in perception directly transmit information
 in the fluctuations of neurotransmitter released.
 This analog mode of transmission allows more information to be transmitted
 per time.
 Sub\SpecialChar \-
threshold emission of neurotransmitter also seems to modulate subsequent
 action potentials, allowing for a mixture of analog and digital information
 transmission 
\begin_inset CommandInset citation
LatexCommand cite
key "DebanneRama2013"

\end_inset

.
 
\end_layout

\begin_layout Standard
Examples for neural networks that have been partly decoded are the eye (visual
 system) and the nose (olfactory system).
\end_layout

\begin_layout Subsection
Examples of Biological Neural Networks
\end_layout

\begin_layout Subsubsection
The Eye, a Visual System
\end_layout

\begin_layout Standard
In the eye, specialized cells called rods and cones detect light
\begin_inset CommandInset citation
LatexCommand cite
key "Biochemistry2002,Kolb2003"

\end_inset

.
 Rods are more sensitive to dim light, while the three types of cones react
 to bright light only but can differentiate between colors.
 Both rods and cones release the neurotransmitter glutamate continuously
 into the synaptic cleft, but when hit by light, suspend this emission for
 the duration of the light.
 This is implemented by the cell by a long pathway.
\end_layout

\begin_layout Standard
Specifically, light elicits a transformation of cis-rhodopsin to trans-rhodopsin
, which presents on its surface a G protein binding site.
 The G protein transducin binds to the activated rhodopsin, and in this
 process GDP acquires a phosphate group to form GTP.
 The 
\begin_inset Formula $\alpha$
\end_inset

-subunit of transducin activates a cGMP phosphodiesterase, which in turn
 hydrolyzes cGMP to GMP.
 The reduction in the concentration of cGMP causes cGMP-gated ion channels
 to close.
 This in turn hyperpolarizes the photosensitive cell, causing glutamate
 to be released into the synaptic cleft at a slower rate.
 This long pathway between cis-rhodopsin and glutamate release inhibition
 facilitates an amplification of the signal at every step, which allows
 rod cells to signal a spike in response to it being hit by a single photon.
\end_layout

\begin_layout Standard
The area that elicits a response in the cell upon being illuminated is called
 the 
\emph on
receptive field
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
receptive field
\end_layout

\end_inset

, and is just as large as the top of the photoreceptor for rods and cones.
 The released glutamate binds to receptors present on the outside of bipolar
 cells, and, depending on the type of bipolar cell, cause either an action
 potential to be generated when the photoreceptor is lit and the surrounding
 area is dark (
\emph on
ON 
\emph default
bipolar cell), or when the photoreceptor is dark against a bright background
 (
\emph on
OFF 
\emph default
bipolar cell).
 Another type of cell, the horizontal cell integrates signals from surrounding
 cone cells, and feed their signal back to the cones, or directly to bipolar
 cells.
 This enhances contrast.
 The signal from several bipolar cells is fed into a ganglion cell, which
 therefore has a larger receptive field than its connected bipolar cells.
 ON bipolar cells only excite ON ganglion cells, and OFF bipolar cells excite
 only OFF ganglion cells.
 Finally, in primates, there are more than a million nerve fibers from ganglion
 cells to the visual cortex of the brain.
 Altogether, the basic cell types are, depending on the species, 1 to 4
 types of horizontal cells, 11 types of bipolar cells, 22 to 30 types of
 amacrine cells, and 20 types of ganglion cells.
 Among those cell types' known functions are integration of a large number
 of rods to provide sight in little light, brightness-dependent size regulation
 of the receptive field of amacrine cells, and an additional photoreceptor
 distinct from rods and cones
\begin_inset CommandInset citation
LatexCommand cite
key "Kolb2003"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier ein Bild einfügen, das links ein biologisches neuronales Netz
 zeigt, und rechts ein artifizielles.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Odor Sensing in the Olfactory System
\end_layout

\begin_layout Standard
The olfactory system of mammals and insects contains neurons that detect
 odor molecules, called glomeruli
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

.
 In humans, there are about 
\begin_inset Formula $500$
\end_inset

 different types of glomeruli [1], but it is hypothesized that a human can
 perceive around 
\begin_inset Formula $10,000$
\end_inset

 different odors.
 Each 
\begin_inset Quotes eld
\end_inset

atomic
\begin_inset Quotes erd
\end_inset

 odor consisting of a few (
\begin_inset Formula $<100$
\end_inset

) molecular species excites one or more glomeruli, and the compression requires
 each glomerulus to signal the presence of one or more than one odor.
 The excitation pattern of multiple glomeruli must be resolved in the olfactory
 neuronal system so that a low-dimensional vector of (
\begin_inset Formula $\approx500$
\end_inset

) glomeruli activations is decompressed to a high-dimensional representation
 of (
\begin_inset Formula $\approx10,000$
\end_inset

) odors in the brain.
\end_layout

\begin_layout Standard
Each glomerulus is connected to one or more Kenyon Cells in insects.
 It is assumed that the activation of a Kenyon cell signals to the insect
 nervous system the presence of one specific odor.
 (In the mammalian brain, a single odor is represented by neurons in the
 olfactory cortex.) Experiments show that the circuit connecting glomeruli
 to Kenyon cells is feed-forward
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
feed-forward neural network
\end_layout

\end_inset

 only, i.e.
 without recurrent connections (loops).
 The structure of a feed-forward compressed sensing circuitry is of interest,
 because standard compressed sensing circuits are recurrent dynamic systems
 that converge to one of their attractor states.
 In addition to quick decoding of odors, experimental evidence shows that
 the biological compressed sensing circuitry is robust to noise, i.e.
 to spurious neuronal spikes in glomeruli, or noise due to experimental
 inhibition
\begin_inset Foot
status open

\begin_layout Plain Layout
Also experimental modifications of glomeruli have been made so that half
 of all glomeruli always express only one type of receptor.
\end_layout

\end_inset

 of glomeruli.
\end_layout

\begin_layout Standard
The theoretical work of 
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

 proposed that a feed-forward architecture could facilitate odor decoding
 simply by implementing a logical AND
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

That is, odor component i will be detected as present if all glomeruli that
 feed signals to node i in the reconstruction layer are activated.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
 They suggest that in the neuronal AND-circuit a specific odor's Kenyon
 cell is activated when at least (for example) 
\begin_inset Formula $80\%$
\end_inset

 of the glomeruli with receptors to this odor are active.
 On a cellular level, this could be realized by connecting the glomeruli
 associated with an odor with the odor's Kenyon cell, and a threshold at
 the Kenyon cell's input.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The following is from 
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

's discussion section.
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

 showed that the optimal connectivity between glomeruli and Kenyon cells
 
\begin_inset Formula $p_{m}$
\end_inset

 only depends on the sparseness (i.e.
 number of molecular species in an odor) 
\begin_inset Formula $K$
\end_inset

: 
\begin_inset Formula $p_{m}=1/(K+1)$
\end_inset

.
\end_layout

\end_inset

A prediction of 
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

 is that the number of glomeruli activated by a single odor should be close
 to the number of glomeruli that are connected to a Kenyon cell.
 They postulated further that the validity of their feed-forward model can
 be tested by measuring the odor sparseness
\begin_inset Foot
status open

\begin_layout Plain Layout
Odor sparseness is the average number of different molecular species in
 an odor.
\end_layout

\end_inset

 in the environment of an animal species and comparing it to its average
 number of connections from glomeruli to Kenyon cells.
\end_layout

\begin_layout Standard
For example, in 
\emph on
Drosophila
\emph default
, about 
\begin_inset Formula $9\%$
\end_inset

 of the glomeruli are excited by an odorant, and the connectivity rate between
 glomeruli and Kenyon Cells is between 
\begin_inset Formula $6.5\%$
\end_inset

 and 
\begin_inset Formula $12.5\%$
\end_inset

.
 In the locust, a projection neuron (the equivalent to a glomerulus) is
 activated by half of the odorants and the connectivity rate is about 
\begin_inset Formula $50\%$
\end_inset

.
 This is in agreement with the proposed model.
\end_layout

\begin_layout Standard
The model also predicts that species with sparse connectivity have better
 odor perception of complex odor mixtures.
 On the other hand, species with dense connectivity should have better olfactory
 performance in detecting simple odor mixtures.
\end_layout

\begin_layout Subsection
Artificial Neurons as Simple Models of Biological Neurons
\begin_inset CommandInset label
LatexCommand label
name "sub:Artificial-Neurons-as-Simple-Models-of-Biological-Neurons"

\end_inset


\end_layout

\begin_layout Standard
The machinery facilitating propagation and transmission of information in
 and between biological neurons is highly simplified in artificial neurons
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
artificial neuron signal processing
\end_layout

\end_inset

.
 Signal processing of a real neuron is modelled in an artificial neuron
 as a mathematical function that has multiple input variables, computes
 a value according to the function formula and its parameters and outputs
 its computed value to multiple neurons, which use it as an input variable.
 Herein, the processes of neurotransmitter release, de- and hyperpolarization,
 and propagation of the action potential are abstracted away into discrete
 time steps.
\end_layout

\begin_layout Standard
Each artificial neuron's function is evaluated once per time step.
 Often, the 
\emph on
sigmoid
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sigmoid function
\end_layout

\end_inset


\emph on
 
\emph default
function is used to describe the output of an artificial neuron, the so-called
 
\emph on
activation
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
activation of an artificial neuron
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
o_{i}=\sigma(v_{i})=\frac{1}{1+\exp(-v_{i})},\label{eq:sigmoid-function}
\end{equation}

\end_inset

where 
\begin_inset Formula $v_{i}\in\mathbb{R}$
\end_inset

 is the accumulated input to neuron 
\begin_inset Formula $i$
\end_inset

, and 
\begin_inset Formula $o_{i}\in[0;1]$
\end_inset

 is the activation of neuron 
\begin_inset Formula $i$
\end_inset

.
 See the left panel of figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sigmoid-function"

\end_inset

 for a plot of the sigmoid function.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/plot-sigmoid.pdf
	lyxscale 50
	width 45col%

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename images/neuronal-network-example.dia
	width 45col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:sigmoid-function"

\end_inset

Left: The sigmoid function 
\begin_inset Formula $\sigma$
\end_inset

.
 Right: Schema of the computational steps in an artificial neural feed-forward
 network
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
feed-forward neural network
\end_layout

\end_inset

 from input layer to output layer.
 The 
\begin_inset Quotes eld
\end_inset

+
\begin_inset Quotes erd
\end_inset

 nodes accumulate their input values, and the 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $\sigma$
\end_inset


\begin_inset Quotes erd
\end_inset

 nodes compute the output of a neuron, to be used as input for the next
 layer.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The effect of an incoming axon onto a neuron, that is, the different types
 of receptors that can be present on the outside of a real dendrite, and
 the effected de- or hyperpolarization of the dendrite are abstracted away
 by using real-numbered weights.
 Weights are parameters to the mathematical function describing the conversion
 of outputs of neurons to the single input of the next connected neuron.
 Usually the input 
\begin_inset Formula $v_{i}$
\end_inset

 of neuron 
\begin_inset Formula $i$
\end_inset

 is computed from the outputs of its connected neurons 
\begin_inset Formula $\mathbf{c_{i}}$
\end_inset

 as in
\begin_inset Formula 
\begin{equation}
v_{i}=b_{i}+\sum_{j\in\mathbf{c_{i}}}o_{j}w_{ij},\label{eq:input-to-a-neuron}
\end{equation}

\end_inset

where 
\begin_inset Formula $b_{i}\in\mbox{\mathbb{R}}$
\end_inset

 is the so-called 
\emph on
bias
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
bias of an artificial neuron
\end_layout

\end_inset


\emph on
 
\emph default
of neuron 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $\mathbf{c_{i}}$
\end_inset

 is the vector of indices of its in-going connected neurons, 
\begin_inset Formula $o_{j}\in\mathbb{R}$
\end_inset

 is the activation of the connected neuron 
\begin_inset Formula $j$
\end_inset

, and 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

 is the weight of the connection going out of neuron 
\begin_inset Formula $j$
\end_inset

 and into neuron 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Standard
In an neural network the neurons are often arranged in layers.
 See the right panel of figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sigmoid-function"

\end_inset

 for an example of the structure of an artificial neural network.
\end_layout

\begin_layout Subsection
Learning in Biological Neural Networks
\end_layout

\begin_layout Standard
Nervous systems do not only process signals, but they also learn, that means
 that they adapt their signal processing over time.
 One reason for this is an organism's need for a change in behavior, as
 response to a changing environment.
\end_layout

\begin_layout Standard
In biological neuronal systems, this is possible by altering existing synapses
 (for example by exchanging the receptors on the surface of dendrites),
 or by creating and abandoning existing synapses (i.e.
 connecting the axon terminals of a neuron to different neurons).
 There are several known cellular mechanisms for that, among them LTP, LTD,
 and PTP
\begin_inset Note Note
status open

\begin_layout Plain Layout
post-tetanic potentiation
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "BermudezFederico2007"

\end_inset

.
 Strengthening of the synaptic link (that occurs within minutes and remains
 after hours and up to weeks in the hippocampus of mammals) is called long-term
 potentiation (LTP), while its weakening is called long-term depression
 (LTD).
 LTP is induced by associativity of connected neurons, that means, when
 a neuron contributes to the depolarization in a directly connected neuron,
 the efficiency of that connection will be strengthened
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe (after reading) cite 
\begin_inset CommandInset citation
LatexCommand cite
key "GalanGalizia2006"

\end_inset


\end_layout

\end_inset

.
 The molecular mechanisms responsible for this phenomenon are not yet completely
 understood.
 It is known that they differ between brain regions, and also between types
 of synapses in the same brain region.
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier evtl.
 weitermachen mit Erklären und Zitieren der 
\begin_inset Quotes eld
\end_inset

Hebbian learning rule
\begin_inset Quotes erd
\end_inset

: die synapse zwischen zwei benachbarten neuronen wird gestärkt, wenn beide
 neuronen gleichzeitig aktiv sind.? Ein Einblick in molekulare Abläufe wäre
 wünschenswert.
\end_layout

\end_inset

 The cellular mechanisms controlling these processes, and their interplay
 in larger neuron ensembles are a field of active research 
\begin_inset CommandInset citation
LatexCommand cite
key "BermudezFederico2007"

\end_inset

.
\end_layout

\begin_layout Standard
An unproven hypothesis is that learning is local
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
locality of Hebbian learning rule
\end_layout

\end_inset

, which means that changes at a synapse only depend on the directly connected
 neurons, but not on other distantly-connected neurons.
 This type of local learning is called 
\emph on
Hebbian learning
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Hebbian learning
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
The learning in biological neuronal systems happens seemingly automatically,
 for example, migratory birds learn and remember travel paths around the
 globe, without an apparent teacher.
 Ultimately the goals of learning are determined by an interplay of evolution
 and the environment.
 
\end_layout

\begin_layout Subsection
Back-propagation for Training Artificial Neural Networks
\begin_inset CommandInset label
LatexCommand label
name "sub:Back-propagation"

\end_inset


\end_layout

\begin_layout Standard
The nearest analog to learning in biological neural networks is 
\emph on
training 
\emph default
artificial neural networks
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
training of artificial neural networks
\end_layout

\end_inset

.
 Here, the goal is explicitly set by humans by providing training data sets.
 One training sample consists of a vector of real numbers called 
\emph on
input patterns
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
input pattern
\end_layout

\end_inset

 and a corresponding vector of real numbers of desired 
\emph on
output patterns
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
output pattern
\end_layout

\end_inset

, also called 
\emph on
labels
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
label
\end_layout

\end_inset

.
 For every input pattern in the training data set an output pattern is defined
 that the learner should compute from the input pattern.
 Learning is hereby facilitated by changing the parameters of the artificial
 neural network.
\end_layout

\begin_layout Standard
Back-propagation is a supervised training procedure for artificial neural
 networks 
\begin_inset CommandInset citation
LatexCommand cite
key "RumelhartWilliams1988"

\end_inset

.
 It learns from labeled training samples.
 The parameters of the network, the weights and biases, are adapted using
 
\emph on
gradient descent
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
gradient descent
\end_layout

\end_inset

.
 The basic idea is to set the neurons in the input layer to the input pattern,
 compute the activations of neurons in the network, compute the total error
 observed in the output layer using the difference between actual and desired
 output, then determine how much each neuron was 
\begin_inset Quotes eld
\end_inset

responsible
\begin_inset Quotes erd
\end_inset

 for the total error in the network, and finally use it to adapt the weights
 and biases.
 This procedure is then repeated until the network is fully trained.
\end_layout

\begin_layout Standard
Let the supervised training patterns be indexed by 
\begin_inset Formula $p$
\end_inset

, 
\begin_inset Formula $x_{i,p}$
\end_inset

 the activation of a neuron 
\begin_inset Formula $i$
\end_inset

 in the input layer for training pattern 
\begin_inset Formula $p$
\end_inset

, and 
\begin_inset Formula $y_{k,p}$
\end_inset

 the desired activation of neuron 
\begin_inset Formula $k$
\end_inset

 in the output layer for this training pattern.
\end_layout

\begin_layout Paragraph
Forward Pass
\end_layout

\begin_layout Standard
The supervised training procedure first performs the 
\emph on
forward pass
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
forward pass
\end_layout

\end_inset

: it sets activations 
\begin_inset Formula $o_{i}$
\end_inset

 in the input layer according to the input pattern 
\begin_inset Formula $x_{i,p}$
\end_inset

 to be learned, computes the activations 
\begin_inset Formula $o_{j}$
\end_inset

 of the hidden layers and the output layer 
\begin_inset Formula $o_{k}$
\end_inset

.
\end_layout

\begin_layout Standard
(In the following, index 
\begin_inset Formula $k$
\end_inset

 is used for a neuron in the output layer, and indices 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $i$
\end_inset

 for a neuron in a hidden layer or the output layer.
 If the network has only one hidden layer, then index 
\begin_inset Formula $i$
\end_inset

 refers to a neuron in the input layer.)
\end_layout

\begin_layout Standard
Each input neuron's output 
\begin_inset Formula $o_{i}$
\end_inset

 is set to a training input: 
\begin_inset Formula 
\begin{equation}
o_{i}=x_{i,p}.
\end{equation}

\end_inset

The input 
\begin_inset Formula $v_{j}$
\end_inset

 to a hidden or output neuron is computed from the sum of the connected
 neurons' outputs in the layer above (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Artificial-Neurons-as-Simple-Models-of-Biological-Neurons"

\end_inset

 ):
\begin_inset Formula 
\begin{equation}
v_{j}=b_{j}+\sum_{i\in\mathbf{c_{j}}}o_{i}w_{ji},
\end{equation}

\end_inset

 where 
\begin_inset Formula $b_{j}$
\end_inset

 is the bias, 
\begin_inset Formula $o_{i}$
\end_inset

 is the output of a neuron in the layer above, and 
\begin_inset Formula $w_{ji}$
\end_inset

 is the weight of the connection from neuron 
\begin_inset Formula $i$
\end_inset

 to neuron 
\begin_inset Formula $j$
\end_inset

.
 The input 
\begin_inset Formula $v_{j}$
\end_inset

 is then scaled by the sigmoid function to produce a neuron's output 
\begin_inset Formula $o_{j}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
o_{j}=\sigma(v_{j})=\frac{1}{1+\exp(-v_{j})}.\label{eq:sigmoid-function-2}
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Backward Pass for the Output Layer
\end_layout

\begin_layout Standard
While in the forward pass the information flowed from input to output layer,
 in the 
\emph on
backward pass
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
backward pass
\end_layout

\end_inset

, the information flows from output to input layer, adjusting the weights
 and biases on the way.
 See figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Data-flow-in-back-propagation"

\end_inset

 for an illustration of the two data flow directions.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/introduction-backpropagation-data-flow.dia
	width 34col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Data-flow-in-back-propagation"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Data flow in training a neural network using back-propagation.
\end_layout

\end_inset

Data flow in training a neural network with three layers using back-propagation.
 The input layer is at the top, the hidden layer in the middle, and the
 output layer at the bottom.
 The black arrows denote information flow during the forward pass.
 Each black arrow is associated with a weight from tail to head.
 The grey arrows denote information flow during the backward pass, which
 propagates errors in the reverse direction.
\end_layout

\end_inset


\end_layout

\end_inset

The training procedure computes the total error 
\begin_inset Formula $E_{total}$
\end_inset

 of the network, which is defined as the squared sum of differences between
 actual output 
\begin_inset Formula $o_{k}$
\end_inset

 and desired output 
\begin_inset Formula $y_{k}$
\end_inset

 over all training patterns:
\begin_inset Formula 
\begin{eqnarray}
E_{total} & = & \sum_{p}\sum_{k}\frac{1}{2}(o_{k,p}-y_{k,p})^{2}\\
 & = & \sum_{p}E,\nonumber 
\end{eqnarray}

\end_inset

 where 
\begin_inset Formula $E$
\end_inset

 is the error of one training pattern.
 We further split 
\begin_inset Formula $E$
\end_inset

 into a sum of errors of individual neurons 
\begin_inset Formula $E_{k}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray}
E & = & \sum_{k}\frac{1}{2}(o_{k}-y_{k})^{2}\\
 & = & \sum_{k}E_{k},\nonumber 
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $E_{k}=\frac{1}{2}(o_{k}-y_{k})^{2}$
\end_inset

.
 To compute the contribution of weight 
\begin_inset Formula $w_{kj}$
\end_inset

 to the error, the error 
\begin_inset Formula $E$
\end_inset

 is then differentiated with respect to a weight 
\begin_inset Formula $w_{kj}$
\end_inset

 for a connection from neuron 
\begin_inset Formula $j$
\end_inset

 in the last hidden layer to neuron 
\begin_inset Formula $k$
\end_inset

 in the output layer: 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial E}{\partial w_{kj}} & = & \frac{\partial E}{\partial o_{k}}\cdot\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial w_{kj}}\nonumber \\
 & = & \frac{\partial\sum_{k}E_{k}}{\partial o_{k}}\cdot\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial w_{kj}}\nonumber \\
 & = & \frac{\partial E_{k}}{\partial o_{k}}\cdot\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial w_{kj}}\label{eq:backpropagation-error-wrt-weight-output-layer}\\
 & = & (o_{k}-y_{k})\cdot o_{k}(1-o_{k})\cdot o_{j},\nonumber 
\end{eqnarray}

\end_inset

 which uses that the derivative of the sigmoid function 
\begin_inset Formula $o_{k}=\frac{1}{1+\exp(-v_{k})}$
\end_inset

 is 
\begin_inset Formula $\frac{\partial o_{k}}{\partial v_{k}}=o_{k}(1-o_{k})$
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Derivative of the sigmoid function:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial o_{j}}{\partial v_{j}} & = & \frac{\partial}{\partial v_{j}}\frac{1}{1+\exp(-v_{j})}\\
 & = & -\frac{\frac{\partial}{\partial v_{j}}1+\exp(-v_{j})}{(1+\exp(-v_{j}))^{2}}\\
 & = & -\frac{\frac{\partial}{\partial v_{j}}\exp(-v_{j})}{(1+\exp(-v_{j}))^{2}}\\
 & = & -\frac{\exp(-v_{j})\frac{\partial(-v_{j})}{\partial v_{j}}}{(1+\exp(-v_{j}))^{2}}\\
 & = & -\frac{-\exp(-v_{j})}{(1+\exp(-v_{j}))^{2}}\\
 & = & \frac{\exp(-v_{j})}{(1+\exp(-v_{j}))^{2}}\\
 & = & \frac{\exp(-v_{j})}{(1+\exp(-v_{j}))^{2}}\frac{1-o_{j}(v_{j})}{1-o_{j}(v_{j})}\\
 & = & \frac{\exp(-v_{j})}{(1+\exp(-v_{j}))^{2}}\frac{1-o_{j}(v_{j})}{1-\frac{1}{1+\exp(-v_{j})}}\\
 & = & \frac{\exp(-v_{j})}{(1+\exp(-v_{j}))^{2}}\frac{1-o_{j}(v_{j})}{\frac{(1+\exp(-v_{j}))-1}{1+\exp(-v_{j})}}\\
 & = & \frac{\exp(-v_{j})}{(1+\exp(-v_{j}))^{2}}\frac{1-o_{j}(v_{j})}{\frac{\exp(-v_{j})}{1+\exp(-v_{j})}}\\
 & = & \frac{\exp(-v_{j})}{\frac{\exp(-v_{j})}{1+\exp(-v_{j})}(1+\exp(-v_{j}))^{2}}\left(1-o_{j}(v_{j})\right)\\
 & = & \frac{\exp(-v_{j})}{\frac{\exp(-v_{j})}{1+\exp(-v_{j})}(1+\exp(-v_{j}))^{2}}\left(1-o_{j}(v_{j})\right)\\
 & = & \frac{\exp(-v_{j})}{\exp(-v_{j})(1+\exp(-v_{j}))}\left(1-o_{j}(v_{j})\right)\\
 & = & \frac{1}{1+\exp(-v_{j})}\left(1-o_{j}(v_{j})\right)\\
 & = & o_{j}(v_{j})\left(1-o_{j}(v_{j})\right)
\end{eqnarray*}

\end_inset


\end_layout

\end_inset

.
 The derivative of the error with respect to 
\begin_inset Formula $b_{k}$
\end_inset

 is
\begin_inset Formula 
\begin{equation}
\frac{\partial E}{\partial b_{k}}=\frac{\partial E}{\partial o_{k}}\cdot\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial b_{k}}=(o_{k}-y_{k})\cdot o_{k}(1-o_{k})\cdot1.
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Backward Pass for the Other Layers
\end_layout

\begin_layout Standard
To perform gradient descent, we also need to update the weights and biases
 for the remaining connections between hidden layers, and from the input
 layer to the first hidden layer.
 The derivative of the error with respect to the weight 
\begin_inset Formula $w_{ji}$
\end_inset

 of the connection from neuron 
\begin_inset Formula $i$
\end_inset

 in a layer to neuron 
\begin_inset Formula $j$
\end_inset

 in the layer below (for example, from neuron 
\begin_inset Formula $i$
\end_inset

 in the second last hidden layer to neuron 
\begin_inset Formula $j$
\end_inset

 in the last hidden layer) is
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial E}{\partial w_{ji}} & = & \frac{\partial E}{\partial o_{j}}\cdot\frac{\partial o_{j}}{\partial v_{j}}\cdot\frac{\partial v_{j}}{\partial w_{ji}}\label{eq:backpropagation-error-wrt-weight-other-layers}\\
 & = & \frac{\partial E}{\partial o_{j}}\cdot o_{j}(1-o_{j})\cdot o_{i},\nonumber 
\end{eqnarray}

\end_inset

 where
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
this was 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E}{\partial o_{j}} & = & \frac{\partial}{\partial o_{j}}\sum_{k}E_{k}\\
 & = & \frac{\partial}{\partial o_{j}}\sum_{k}E_{k}(o_{k}(v_{k}(o_{j})))\\
 & = & \sum_{k}\frac{\partial}{\partial o_{j}}E_{k}(o_{k}(v_{k}(o_{j})))\\
 & = & \sum_{k}\frac{\partial}{\partial v_{k}}E_{k}(o_{k}(v_{k}))\cdot\frac{\partial v_{k}}{\partial o_{j}}\\
 & = & \sum_{k}\frac{\partial}{\partial o_{k}}E_{k}(o_{k})\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial o_{j}}\\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial o_{j}}\\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\cdot w_{kj},
\end{eqnarray*}

\end_inset

, but I shortened it, which has the side-effect that it looks cleaner.
\end_layout

\end_inset

 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial E}{\partial o_{j}} & = & \frac{\partial}{\partial o_{j}}\sum_{k}E_{k}\nonumber \\
 & = & \sum_{k}\frac{\partial}{\partial o_{j}}E_{k}\nonumber \\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial o_{j}}\nonumber \\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\cdot w_{kj},\label{eq:backpropagation-error-wrt-output-other-layers}
\end{eqnarray}

\end_inset

and neuron 
\begin_inset Formula $k$
\end_inset

 is in the layer below the layer that neuron 
\begin_inset Formula $j$
\end_inset

 is in (in our example neuron 
\begin_inset Formula $k$
\end_inset

 is in the output layer).
 We take the value for 
\begin_inset Formula $\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-error-wrt-output-other-layers"

\end_inset

 above from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-error-wrt-weight-output-layer"

\end_inset

 when neuron 
\begin_inset Formula $k$
\end_inset

 is in the output layer or equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-error-wrt-weight-other-layers"

\end_inset

 when neuron 
\begin_inset Formula $k$
\end_inset

 is in a hidden layer.
 (Neuron 
\begin_inset Formula $k$
\end_inset

 is named 
\begin_inset Formula $j$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:backpropagation-error-wrt-weight-other-layers"

\end_inset

.) In our example (for neuron 
\begin_inset Formula $j$
\end_inset

 in the last hidden layer), we take 
\begin_inset Formula $\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}$
\end_inset

 from the output layer: 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial E}{\partial o_{j}} & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\cdot\frac{\partial o_{k}}{\partial v_{k}}\cdot w_{kj}\\
 & = & \sum_{k}(o_{k}-y_{k})\cdot o_{k}(1-o_{k})\cdot w_{kj}.\nonumber 
\end{eqnarray}

\end_inset

Analogously, the derivative of 
\begin_inset Formula $E$
\end_inset

 with respect to 
\begin_inset Formula $b_{j}$
\end_inset

 is
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Derivative of the error with respect to the bias.
\end_layout

\begin_layout Plain Layout
Use the following definitions:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\begin{eqnarray*}
v_{j} & = & b_{j}+\sum_{i\in\mathbf{c_{j}}}o_{i}w_{ji}\\
o_{j} & = & \sigma(v_{j})=\frac{1}{1+\exp(-v_{j})}.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Plain Layout
The derivative of the error with respect to the bias of node 
\begin_inset Formula $j$
\end_inset

 in the last hidden layer:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E}{\partial b_{j}} & = & \frac{\partial E}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\frac{\partial v_{k}}{\partial o_{j}}\frac{\partial o_{j}}{\partial v_{j}}\frac{\partial v_{j}}{\partial b_{j}}\\
 & = & \frac{\partial\sum_{k}E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\frac{\partial v_{k}}{\partial o_{j}}\frac{\partial o_{j}}{\partial v_{j}}\frac{\partial v_{j}}{\partial b_{j}}\\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\frac{\partial v_{k}}{\partial o_{j}}\frac{\partial o_{j}}{\partial v_{j}}\frac{\partial v_{j}}{\partial b_{j}}\\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\frac{\partial(b_{k}+\sum_{j\in\mathbf{c_{k}}}o_{j}w_{kj})}{\partial o_{j}}\frac{\partial o_{j}}{\partial v_{j}}\frac{\partial v_{j}}{\partial b_{j}}\\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}w_{kj}\frac{\partial o_{j}}{\partial v_{j}}\frac{\partial v_{j}}{\partial b_{j}}\\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}w_{kj}o_{j}(v_{j})(1-o_{j}(v_{j}))\frac{\partial v_{j}}{\partial b_{j}}\\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}w_{kj}o_{j}(v_{j})(1-o_{j}(v_{j}))\cdot1
\end{eqnarray*}

\end_inset


\end_layout

\end_inset

 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial E}{\partial b_{j}} & = & \frac{\partial E}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}\cdot\frac{\partial v_{k}}{\partial o_{j}}\cdot\frac{\partial o_{j}}{\partial v_{j}}\cdot\frac{\partial v_{j}}{\partial b_{j}}\\
 & = & \sum_{k}\frac{\partial E_{k}}{\partial o_{k}}\frac{\partial o_{k}}{\partial v_{k}}w_{kj}\cdot o_{j}(1-o_{j})\cdot1.\nonumber 
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The error for each node 
\begin_inset Formula $\frac{\partial E_{k}}{\partial o_{k}}\cdot\frac{\partial o_{k}}{\partial v_{k}}$
\end_inset

 is thus back-propagated in reverse input direction through the hidden layers,
 until all derivatives have been determined.
\end_layout

\begin_layout Subparagraph
Updating rule
\end_layout

\begin_layout Standard
After having computed the derivatives of the error with respect to the parameter
s of the network, we can perform gradient descent and integrate these derivative
s over the training patterns 
\begin_inset Formula $p$
\end_inset

.
 We update each weight 
\begin_inset Formula $w$
\end_inset

 and bias 
\begin_inset Formula $b$
\end_inset

 using the learning rate
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
learning rate
\end_layout

\end_inset

 
\begin_inset Formula $\epsilon$
\end_inset

, a small positive number:
\begin_inset Formula 
\begin{eqnarray}
\Delta w & = & -\epsilon\sum_{p}\frac{\partial E}{\partial w}^{(p)}\\
\Delta b & = & -\epsilon\sum_{p}\frac{\partial E}{\partial b}^{(p)},\nonumber 
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $\frac{\partial E}{\partial w}^{(p)}$
\end_inset

 or 
\begin_inset Formula $\frac{\partial E}{\partial b}^{(p)}$
\end_inset

 are the derivatives of the error with respect to a weight or bias, when
 the input layer was set to training pattern 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Standard
Alternatingly performing forward and backward pass and updating the weights
 and biases, until the error over all training patterns 
\begin_inset Formula $E_{total}$
\end_inset

 is small enough, forms the complete back-propagation training procedure
 of a neural network.
\end_layout

\begin_layout Paragraph
Limits of Backpropagation
\end_layout

\begin_layout Standard
The purpose of error back-propagation is to adjust the weights of the artificial
 neural network following the gradient, such that when the current input
 pattern pair is presented to the network, its computed output pattern gets
 closer and closer to the desired output pattern.
 However, back-propagation is not able to train networks with more than
 one or two hidden layers, because it is a gradient descent method and can
 get stuck in poor local optima, and the error surface gets more rugged
 the more hidden neurons and layers there are 
\begin_inset CommandInset citation
LatexCommand cite
key "GoriTesi1992"

\end_inset

.
 Having artificial neural networks with more than one hidden layer is desirable,
 because they can perform the same computation with less total number of
 hidden nodes compared to a network with less hidden layers.
 A network with one hidden layer less needs up to an exponential factor
 more hidden nodes 
\begin_inset CommandInset citation
LatexCommand cite
key "Hastad1987"

\end_inset

.
\end_layout

\begin_layout Section
Introduction to Machine Learning
\end_layout

\begin_layout Subsection
Supervised and Unsupervised Machine Learning
\end_layout

\begin_layout Standard
In machine learning, there are two major types of learning: 
\emph on
supervised
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
supervised machine learning
\end_layout

\end_inset

 and 
\emph on
unsupervised learning
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
unsupervised machine learning
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Barber2012"

\end_inset

.
 Both methods process training data sets that are in matrix form: for example,
 in expression data, the rows usually denote different genes or transcripts,
 and each column represents an independently measured sample.
 (Note that in the general machine learning literature, usually the data
 matrix is transposed: the columns denote the features, and the rows the
 samples.) Samples usually are tissue, blood samples, or cell line, and differ
 in their biological background (e.g.
 cell type, gene knock-out or knock-in, cell cycle phase) or treatment (e.g.
 drugs applied).
\end_layout

\begin_layout Standard
In supervised learning, for every input pattern in the data set an output
 pattern is defined that the learner should compute from the input pattern.
 Herein, both input and output pattern can be one- or multi-dimensional
 vectors.
 The goal of supervised learning is to infer a function that maps from the
 space of input patterns to the space of output patterns.
 The output patterns are also called 
\emph on
labels
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
labels
\end_layout

\end_inset

.
 (The fact that for every input pattern there is a defined output pattern
 is termed 
\begin_inset Quotes eld
\end_inset

the input data is labeled
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Standard
In unsupervised learning, there is only an input data set and the goal is
 to find its compact description.
 The output of an unsupervised learning algorithm is the underlying structure
 of the data according to the algorithm's objective function.
 The objective of an unsupervised learning algorithm can range from dimensionali
ty reduction to data re-representation to latent variable modelling.
\end_layout

\begin_layout Standard
Examples of supervised learning algorithms are (linear or logistic) regression,
 k-Nearest Neighbor (k-NN) regression, support vector machines (SVMs), and
 backpropagation neural networks.
 Examples for unsupervised learning algorithms are (hierarchical or kNN)
 clustering, self-organising maps (SOMs), principal component analysis (PCA),
 and Restricted Boltzmann Machines (RBMs).
\end_layout

\begin_layout Standard
A goal of both supervised and unsupervised learning is that the learned
 rules should generalize well, i.e.
 previously unseen data should be characterized correctly.
 The samples are therefore split into training, validation and test data
 sets.
 The 
\emph on
training data set
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
training data set
\end_layout

\end_inset

 is used to train a machine learning algorithm.
 Some machine learning algorithms have meta-parameters, i.e.
 parameters that are needed for the algorithm, but that we are not really
 interested in.
 (An example is the number of hidden neurons in an artificial neural network.)
 The meta-parameters are optimized using the 
\emph on
validation data set
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
validation data set
\end_layout

\end_inset

.
 At the end of training, the performance of the machine learning algorithm
 must be evaluated on previously unseen samples, the 
\emph on
testing data set
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
testing data set
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
Semi-supervised Machine Learning
\end_layout

\begin_layout Standard
An intermediate form between supervised and unsupervised machine learning
 is semi-supervised learning
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
semi-supervised learning
\end_layout

\end_inset

.
 In contrast to supervised machine learning, which has for every input pattern
 a target output pattern, semi-supervised learning does not need a target
 output pattern for every input pattern.
 However, in contrast to fully unsupervised machine learning, it does need
 some labeled input data sets.
 A common objective of semi-supervised machine learning algorithms is to
 find underlying structure in all input data sets and then use the known
 labels to assign labels to unlabeled input samples.
 This assumes that samples close in the (high-dimensional) input space have
 the same label.
 Another assumption is that samples distant to each other have different
 labels.
 An example of a possible improvement of semi-supervised classification
 over supervised classification is given in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Illustration-of-semi-supervised-learning"

\end_inset

 (adapted from 
\begin_inset CommandInset citation
LatexCommand cite
key "Joachims1999a"

\end_inset

).
 In the figure, the two-dimensional samples are either labeled orange or
 blue, or unlabeled, and the task is to (1) find a straight line that separates
 samples with the two colors and (2) color the unlabeled samples.
 Supervised learning alone does not take into account the unlabeled samples,
 while semi-supervised learning recognizes that there are two clouds of
 samples, separated by a gap, where the labeled samples from each color
 are on different sides of the gap.
 It then draws the separating line in the middle of the gap, and colors
 the unlabeled samples on the side with the orange samples orange, and the
 unlabed samples on the other side blue.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
A
\begin_inset Graphics
	filename images/semi-supervised-advantage1.dia
	lyxscale 30
	width 30col%

\end_inset

 B
\begin_inset Graphics
	filename images/semi-supervised-advantage2.dia
	lyxscale 30
	width 30col%

\end_inset

 C
\begin_inset Graphics
	filename images/semi-supervised-advantage3.dia
	lyxscale 30
	width 30col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Illustration-of-semi-supervised-learning"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Illustration of semi-supervised learning in two dimensions.
\end_layout

\end_inset

Illustration of semi-supervised learning in two dimensions.
 Each axis is a dimension.
 Circles are samples; filled circles are labeled samples; unfilled circles
 are unlabeled samples.
 The blue circles are samples with label 1, the orange circles are samples
 with label 2.
 A: Supervised SVM learning produces a maximum-margin classifier.
 B: Supervised learning ignores and probably mis-classifies some unlabeled
 samples (the red crossed-out samples).
 C: Semi-supervised learning regards densities of unlabeled samples and
 may give better results than supervised learning on the labeled samples
 alone.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are two types of semi-supervised learning: transductive and inductive
 semi-supervised learning.
 The goal of transductive semi-supervised learning
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
transductive semi-supervised learning
\end_layout

\end_inset

 is to predict the class labels of a pre-specified list of unlabeled input
 patterns, while the goal of inductive
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
inductive semi-supervised learning
\end_layout

\end_inset

 semi-supervised learning is to find a universal rule mapping from the space
 of input patterns to class labels, which could be applied to classify unknown,
 future input patterns.
 In case unknown, future input patterns are to be classified using transductive
 semi-supervised machine learning, the whole model may have to be re-evaluated.
\end_layout

\begin_layout Subsubsection
Example Scenarios for Semi-supervised Learning
\end_layout

\begin_layout Standard
An advantage of semi-supervised learning over supervised learning is that
 it does not need labels for all input patterns, because labels are often
 time-consuming or costly to acquire.
 For example, the Gene Expression Omnibus data base (GEO) 
\begin_inset CommandInset citation
LatexCommand cite
key "BarrettSoboleva2013"

\end_inset

 contains 41,379 expression data sets that were uploaded between Jan 1st,
 2000, and August 31st, 2013.
 Many of these are potentially usable as unlabeled data sets in semi-supervised
 learning.
\end_layout

\begin_layout Standard
However in practice, many machine learning algorithms require samples to
 be independently and identically distributed
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
independetly and identically distributed
\end_layout

\end_inset

 (iid
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
iid
\end_layout

\end_inset

).
 In an ideal world, GEO samples could be assumed to be identically distributed
 within a data set.
 Unfortunately even within the same GEO data set there often is systematic
 variation between samples, called the 
\emph on
batch-effect
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
batch-effect
\end_layout

\end_inset

, caused, for example, by different sample handling or conditions at measurement
 time.
 Hence one either has to use samples from one data set only, or, if one
 wants to use samples from different GEO data sets simultaneously, correct
 for a possible batch-effect manually, or use an algorithm that has some
 built-in mechanism to make such a correction.
\end_layout

\begin_layout Standard
One machine learning algorithm with such a built-in mechanism is
\emph on
 deep learning
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
deep learning
\end_layout

\end_inset

, as employed in 
\emph on
Deep Belief Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset


\emph on
 (DBNs
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DBN
\end_layout

\end_inset


\emph on
)
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

.
 This machine-learning algorithm (which is unsupervised, but can be used
 for supervised and semi-supervised learning) can learn from images of objects
 or faces, where the objects or faces are in different lighting conditions
 or are viewed from different angles (
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006,KrizhevskyHinton2012,KarpathyFei-Fei2014"

\end_inset

).
 In this setting, the batch effect would be the lighting condition or viewing
 angle.
 Such results seem to imply that DBNs are able to abstract the images, which
 are given as vectors of pixels, into encodings of relevant features and
 compute a classifier on these abstract features.
 This can be seen as a form of batch-effect correction.
\end_layout

\begin_layout Standard
An extreme form of batch-effect correction is when all training data are
 in batch 1 and all test data in batch 2.
 Are neural networks able to handle such a situation? For example, suppose
 that batch 1 are all face portraits (frontal view), batch 2 are half profile
 faces (30 degree angle view) of -- not necessarily the same -- people,
 and the task is to match faces seen from both viewing angles to the same
 person.
 During training, semi-supervised learning would have access to the unlabeled
 faces from both viewing angles, but all the half profiles would be unlabeled.
 We are not aware of a study of that setting using artificial neural networks,
 but a similar setting was studied in cognitive psychology, where humans
 were the learners 
\begin_inset CommandInset citation
LatexCommand cite
key "BobakBate2015"

\end_inset

.
 The participants of the study were asked to match a given half profile
 to one out of 10 portraits, or indicate that there is no matching face.
 The accuracy of average people was 81%, while people who have extraordinary
 face recognition abilities, so-called 
\begin_inset Quotes eld
\end_inset

super-recognizers
\begin_inset Quotes erd
\end_inset

, had 94% accuracy.
 When there was no matching face, average people correctly rejected the
 face in 65% of all cases, and super-recognizers did so in 92%.
 For humans, the unsupervised training would consist in seeing people's
 faces from different angles during normal day-to-day activity.
 However, one could argue that this training is unfair, because humans had
 access to the label of many half profiles, since they often have seen a
 person's portrait just moments before seeing the half profiles.
\end_layout

\begin_layout Subsection
Deep Learning
\end_layout

\begin_layout Standard
Deep learning
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
deep learning
\end_layout

\end_inset

 is a term used in artificial neural networks with several hidden layers.
 The advantage of a deep network over a network with just one hidden layer
 is that it can model a problem more compactly using less hidden neurons
 in total, because it has more than one intermediate computation step.
\end_layout

\begin_layout Standard

\emph on
Autoencoders
\emph default
 and 
\emph on
Deep Belief Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset


\emph on
 
\emph default
(
\emph on
DBNs
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DBN
\end_layout

\end_inset

) overcame the limitation of only a few hidden layers 
\begin_inset CommandInset citation
LatexCommand cite
key "BengioLarochelle2007,HintonTeh2006"

\end_inset

.
 Here we give brief overviews over both types of artificial neural network.
\end_layout

\begin_layout Paragraph
Autoencoder
\end_layout

\begin_layout Standard
An autoencoder is a network with more than one hidden layer, whose training
 is unsupervised and its objective is to reconstruct the input in the output
 layer.
 An autoencoder starts as a three-layer network composed of input layer,
 hidden layer, and output layer.
 This network is trained using back-propagation.
 The middle hidden layer is then copied and new hidden layer is inserted
 between both copies, forming a five-layer-network.
 The newly added parameters of this network are the same in number compared
 to a three-layer network, which can be trained using back-propagation.
 Hence, training the five-layer-network using back-propagation also works,
 and adding a new hidden layer in the middle can be repeated.
\end_layout

\begin_layout Paragraph
Deep Belief Network
\end_layout

\begin_layout Standard
Training a Deep Belief Network is separated into a pre-training phase and
 a fine-tuning phase.
 The pre-training phase is unsupervised.
 It starts with a network consisting of one input layer and a hidden layer.
 This pair of  layers is called a 
\emph on
Restricted Boltzmann Machine
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Restricted Boltzmann Machine
\end_layout

\end_inset

 (RBM
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
RBM
\end_layout

\end_inset

).
 There is an accompanying unsupervised training procedure called 
\emph on
contrastive divergence
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
contrastive divergence
\end_layout

\end_inset

 which finds weights between these layers.
 After training the RBM, hidden layers are iteratively added on top of the
 RBM, and the new weights between layers are initialized and pre-trained
 using contrastive divergence.
 While the pre-training phase is unsupervised, the fine-tuning phase can
 be unsupervised as well as supervised.
 After training, the multiple-hidden-layer-network forms a generative artificial
 neural network called a Deep Belief Network.
\end_layout

\begin_layout Section
Overview of Own and Related Work
\end_layout

\begin_layout Standard
We used autoencoders, Deep Belief Networks, and Transductive Support Vector
 Machines on expression data to predict whether breast cancer patients will
 show pathologic complete response to chemotherapy or residual disease.
 The expression data are high-dimensional (
\begin_inset Formula $\approx22,000$
\end_inset

 genes) and we use a relatively large data set (
\begin_inset Formula $\approx500$
\end_inset

 patients).
\end_layout

\begin_layout Subsection
Motivations for Using Deep Belief Networks on Transcriptomic Data
\end_layout

\begin_layout Standard
The motivations for using Deep Belief Networks on transcriptomic data come
 from those networks' successes when used on image data.
 In the hand-written digit classification and graphical object recognition
 data sets on which the deep artificial neural networks were developed,
 they are among the best-performing predictors.
\end_layout

\begin_layout Subsubsection
The ImageNet Large Scale Visual Recognition Challenge
\end_layout

\begin_layout Standard
An example for the success of deep neural network is image classification
 in the ImageNet Large Scale Visual Recognition Challenge 
\begin_inset CommandInset citation
LatexCommand cite
key "RussakovskyFeiFei2015"

\end_inset

.
 It is a yearly contest, wherein participants are given around 1.2 million
 training images.
 Each training image is labeled with one of 
\begin_inset Formula $1,000$
\end_inset

 possible object categories describing the main object appearing in the
 image, for example 
\begin_inset Quotes eld
\end_inset

trumpet
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

butterfly
\begin_inset Quotes erd
\end_inset

.
 After training an image classification algorithm, each contestant must
 compute up to 
\begin_inset Formula $5$
\end_inset

 labels for each of 
\begin_inset Formula $100,000$
\end_inset

 test images.
 Each test image has a single label, which is kept hidden by the contest
 organizer.
 A test image is scored as correctly classified if the correct label appears
 in the (up to 
\begin_inset Formula $5$
\end_inset

) labels submitted by the contestant.
 (Up to 5 labels may be submitted because for example a street scene may
 contain, besides the correct 
\begin_inset Quotes eld
\end_inset

car
\begin_inset Quotes erd
\end_inset

 label also street signs and drivers.) Finally, the accuracy of a contestant
 is computed as the average fraction of correctly classified test images.
\end_layout

\begin_layout Standard
There have been notable improvements in the accuracy of the winning contestant,
 starting in 2012.
 In the last years, all top contestants have moved to using deep neural
 networks.
 See table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:ILSVRC-Test-set-accuracies"

\end_inset

 for the winning contestants between 2010 and 2014.
 Significant differences before and after 2012 are the usage of neural networks
 directly on the image pixel data, and not using pre-computed image features
 in a supervised learning algorithm like a Support Vector Machine.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Year
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Winner
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Technique
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2010
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NEC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
71.8%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SIFT and LBP image features classified by SVM
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2011
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XRCE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
74.2%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
image signatures classified by one-vs-all SVMs
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2012
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SuperVision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
83.6%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
deep convolutional neural network
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2013
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Clarifai
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
88.3%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
deep convolutional neural networks averaged
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2014
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
GoogLeNet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
93.3%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
deep convolutional neural network
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:ILSVRC-Test-set-accuracies"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Test set accuracies and techniques of winning contestants in the ImageNet
 Large Scale Visual Recognition Challenge from 2010-2014.
\end_layout

\end_inset

Test set accuracies and techniques of winning contestants in the ImageNet
 Large Scale Visual Recognition Challenge from 2010-2014.
 Column 
\begin_inset Quotes eld
\end_inset

Year
\begin_inset Quotes erd
\end_inset

 is the year of the contest, 
\begin_inset Quotes eld
\end_inset

Winner
\begin_inset Quotes erd
\end_inset

 the team name of the winner of this year, 
\begin_inset Quotes eld
\end_inset

Accuracy
\begin_inset Quotes erd
\end_inset

 the test set accuracy of the winner's submission, and 
\begin_inset Quotes eld
\end_inset

Technique
\begin_inset Quotes erd
\end_inset

 a summary of the winner's algorithm technique.
 SVM, support vector machine.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Highly Correlated Inputs
\end_layout

\begin_layout Standard
We will now discuss similarities between image classification and expression
 data classification.
\end_layout

\begin_layout Standard
Both underlying distributions – of images and of expression data – have
 many correlated features.
 For images, adjacent pixels often display the same object and have therefore
 correlated values.
 In some face recognition tasks for example, the faces are scaled and translated
 so that the centers of both eyes and mouth are aligned in different faces.
 There will be highly correlated pixels for areas of the image where the
 cheeks and lips usually are.
 If you use the pixels of the whole image as input to the neural network,
 the corresponding input nodes will be highly correlated as well.
\end_layout

\begin_layout Standard
For transcriptomic data, one almost always observes many correlated genes.
 The correlations can be due to many genes being regulated by the same transcrip
tion factor 
\begin_inset CommandInset citation
LatexCommand cite
key "TornowMewes2003,KlebanovYakovlev2007"

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: add some statistics of correlations, e.g.
 in the breast_cancer data set.
 for example, find a group of genes that are all correlated in all samples
 with a correlation coefficient larger than 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Plain Layout
TODO: add some citations for reasons of high correlation in expression data.
\end_layout

\begin_layout Plain Layout
TODO: read TornowMewes2003, KlebanovYakovlev2007.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Deep Belief Networks Find Correlated Nodes
\end_layout

\begin_layout Standard
Deep Belief Networks can group correlated input nodes.
 The network can do this by increasing the weights from the correlated group
 to a single hidden node, and decreasing the weights from the group to all
 other hidden nodes.
 The hidden node becomes the representative of the correlated group of input
 nodes.
 This is a form of abstraction and dimensionality reduction.
 The hidden node will only be active if many of its highly-weighted input
 nodes are active and only few of its negatively-weighted input nodes are
 active.
 Repeated application of this principle of abstraction in deeper and deeper
 hidden layers allows the Deep Belief Network to form more and more abstract
 representations of its input.
\end_layout

\begin_layout Standard
In face recognition for example, an abstract representation might have a
 single value for the size of the lips.
 In expression data, a single node in an abstract representation might encode
 the activity of a gene module.
\end_layout

\begin_layout Subsubsection
Transductive Support Vector Machine
\end_layout

\begin_layout Standard
We compare the artificial neural network approach with another, older, and
 established semi-supervised method, the Transductive Support Vector Machine
 (TSVM) 
\begin_inset CommandInset citation
LatexCommand cite
key "Joachims1999"

\end_inset

.
 Despite the name, it supports transductive as well as inductive learning.
 A standard Support Vector Machine searches for a decision boundary such
 that the margin between samples with differing labels is maximal (see figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Illustration-of-semi-supervised-learning"

\end_inset

).
 The TSVM seeks a labeling of the unlabeled samples so that the decision
 boundary has the maximal margin between all samples with differing classes.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TSVM ist in Methods, section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Transductive-Support-Vector-Machines"

\end_inset

 erklärt.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Previous Work: Gene Expression Inference With Deep Learning
\begin_inset CommandInset label
LatexCommand label
name "sub:Previous-Work:Gene-Expression-Inference-with-Deep-Learning"

\end_inset


\end_layout

\begin_layout Standard
Very recently, 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

 published work on compressing expression data into fewer dimensions on
 a large scale using deep learning.
 The motivation for this work was that principal component analysis found
 that 
\begin_inset Formula $943$
\end_inset

 
\begin_inset Quotes eld
\end_inset

landmark
\begin_inset Quotes erd
\end_inset

 genes can capture about 80% of the information in the CMAP data set.
 This prompted the development of the 
\begin_inset Quotes eld
\end_inset

L1000 Luminex bead technology
\begin_inset Quotes erd
\end_inset

, which measures the expression of these 
\begin_inset Formula $943$
\end_inset

 genes at a low cost, and computationally infers the remaining 
\begin_inset Formula $\approx21,000$
\end_inset

 genes.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

 worked on improving this computational inference step.
\end_layout

\begin_layout Standard
Input data were all 
\begin_inset Formula $\approx111,000$
\end_inset

 genome-wide expression profiles from the GEO database of Affymetrix microarrays
, which were partitioned into training, validation, and testing data sets.
 For each sample, the same subset of 
\begin_inset Formula $943$
\end_inset

 landmark genes was chosen and 
\begin_inset Formula $9,520$
\end_inset

 other genes were predicted from the landmark genes.
 This is different from our work, because we classified breast cancer samples
 using gene expression levels, while 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

 did regression of gene expression levels using other gene expression levels.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

's artificial neural network architecture had between 
\begin_inset Formula $1$
\end_inset

 and 
\begin_inset Formula $3$
\end_inset

 hidden layers with either 
\begin_inset Formula $3,000$
\end_inset

, 
\begin_inset Formula $6,000$
\end_inset

, or 
\begin_inset Formula $9,000$
\end_inset

 nodes.
 It had 943 input expression values (one for each landmark gene), and a
 total of 
\begin_inset Formula $9,520$
\end_inset

 output expression values (one for each gene to be predicted).
 In addition to the (non-linear) neural network, they evaluated linear regressio
n with no regularization, L1-, and L2-regularization.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

 also evaluated k-Nearest Neighbor 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
k-Nearest Neighbor
\end_layout

\end_inset

(kNN
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
kNN
\end_layout

\end_inset

).
 During training, they determined a number, 
\begin_inset Formula $k$
\end_inset

, of landmark genes with expression value closest to each target gene 
\begin_inset Formula $i$
\end_inset

 (let's call this set of genes 
\begin_inset Formula $knn_{i}$
\end_inset

) in the training data set.
 During testing, they predicted the expression value of the target gene
 
\begin_inset Formula $i$
\end_inset

 as the average of the gene's 
\begin_inset Formula $knn_{i}$
\end_inset

 expression values in the testing data set.
 The optimal 
\begin_inset Formula $k$
\end_inset

 (number of genes to average over) was chosen based on a validation data
 set.
\end_layout

\begin_layout Standard
The input values were quantile normalized expression values between 4 and
 15.
 The models were ranked according to the average prediction errors over
 all 9,520 target genes.
\end_layout

\begin_layout Standard
k-Nearest Neighbor performed worst, with an average prediction error of
 
\begin_inset Formula $0.5866$
\end_inset

.
 Linear regression without regularization and with L2-regularization both
 had an average prediction error of 
\begin_inset Formula $0.3784$
\end_inset

.
 Linear regression with L1-regularization had an average prediction error
 of 
\begin_inset Formula $0.3782$
\end_inset

.
 As the three linear regression models performed about equally well, regularizat
ion did not improve linear regression.
 The neural network-based average prediction errors were between 
\begin_inset Formula $0.3421$
\end_inset

 and 
\begin_inset Formula $0.3204$
\end_inset

, with the network having 
\begin_inset Formula $3$
\end_inset

 hidden layers of size 
\begin_inset Formula $9,000$
\end_inset

 and 
\begin_inset Formula $10\%$
\end_inset

 dropout rate performing best.
 (
\emph on
Dropout 
\emph default
is a regularization technique for neural networks, explained in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Dropout"

\end_inset

.)
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Rainer meint, ich sollte dropout vorher erklären.
\end_layout

\end_inset

 Because the input expression values were between 
\begin_inset Formula $4$
\end_inset

 and 
\begin_inset Formula $15$
\end_inset

, an average prediction error of 
\begin_inset Formula $0.3204$
\end_inset

 implies an average error of about 
\begin_inset Formula $3\%$
\end_inset

 on the GEO dataset.
\end_layout

\begin_layout Standard
In another dataset, 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

 again predicted 
\begin_inset Formula $9,520$
\end_inset

 genes from the 
\begin_inset Formula $943$
\end_inset

 landmark genes, but used the GEO dataset for training, the 
\begin_inset Formula $1,000$
\end_inset

 Genomes data for validation 
\begin_inset CommandInset citation
LatexCommand cite
key "LappalainenPedro2013"

\end_inset

, and GTEx data for testing 
\begin_inset CommandInset citation
LatexCommand cite
key "ArdlieLek2015"

\end_inset

.
 Learning in this data set is harder since the training, validation, and
 testing data sets are measured using different expression measurement technolog
ies, and therefore prone to the batch-effect.
 Nevertheless, the performance ranking of the methods was the same, but
 worse than the data set without batch-effect.
 KNN scored worst, with an average prediction error of 
\begin_inset Formula $0.6520$
\end_inset

.
 Linear regression with L1-regularization had an average prediction error
 of 
\begin_inset Formula $0.5667$
\end_inset

.
 Linear regression without regularization and with L2-regularization both
 had an average prediction error of 
\begin_inset Formula $0.4702$
\end_inset

.
 The artificial neural networks all scored consistently better than KNN
 and linear regression, with the artificial neural network with 
\begin_inset Formula $2$
\end_inset

 hidden layers of size 
\begin_inset Formula $9,000$
\end_inset

, and 
\begin_inset Formula $25\%$
\end_inset

 dropout rate having the lowest prediction error of 
\begin_inset Formula $0.4393$
\end_inset

 (which is equivalent to a relative error of 
\begin_inset Formula $4\%$
\end_inset

).
 On the validation data set, the average prediction error was 
\begin_inset Formula $0.7467$
\end_inset

, which is a relative error of 
\begin_inset Formula $6.8\%$
\end_inset

.
 This shows that artificial neural networks are capable of processing input
 from multiple sources, with an acceptable gain in error.
 We had a similar result, in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Unsupervised-Reconstruction-of-Expression-Values"

\end_inset

.
\end_layout

\end_body
\end_document

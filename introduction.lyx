#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: maybe replace 
\begin_inset Quotes eld
\end_inset

neuronal network
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Quotes eld
\end_inset

neural network
\begin_inset Quotes erd
\end_inset

, which is the correct term according to dict.cc.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This is supposed to be a high-level description of neuronal networks, to
 be used in the thesis before the mathematical description of neuronal networks
 (currently 
\begin_inset Quotes eld
\end_inset

methods.lyx
\begin_inset Quotes erd
\end_inset

).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Was meint Claudio mit 
\begin_inset Quotes eld
\end_inset

Überhaupt gehören die beiden am 23.5.
 per Mail angesprochenen Themen definitiv beide in die Intro.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Personalized Medicine
\end_layout

\begin_layout Standard
A medium-term goal of medicine is 
\begin_inset Quotes eld
\end_inset

personalized medicine
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
personalized medicine
\end_layout

\end_inset


\begin_inset Quotes erd
\end_inset

, whose goal is to provide custom-tailored health care on an individual
 basis.
 For example, a standard treatment for breast cancer is chemotherapy, but
 not all patients profit from this treatment.
 The event that a patient has no sign of breast cancer after reductive surgery
 followed by chemotherapy is called 
\emph on
pathologic complete response
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
pathologic complete response
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
pCR
\end_layout

\end_inset

, and the opposite event that the patient still has cancerous tissue after
 this procedure is called 
\emph on
residual disease
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
residual disease
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
RD
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Suppose there were a predictor that could tell the physicist how likely
 a patient is to benefit from chemotherapy.
 If the prediction for a certain patient was such that complete response
 to chemotherapy was unlikely, chemotherapy could be replaced by another
 therapy.
\end_layout

\begin_layout Standard
The goal of this work is to contribute to such a predictor.
 The input to the predictor is the molecular expression data, i.e.
 measures of the number of RNA copies of specific genes present in the cancer
 tissue.
 An artificial neuronal network then processes this data.
 The prediction is output by the network in the form of a number between
 0 and 1.
 Here, 0 means the patient is predicted with absolute certainty to have
 residual disease, 1 means the patient is predicted with absolute certainty
 to have pathologic complete response, and a number in-between is interpreted
 as the probability for pathologic complete response.
\end_layout

\begin_layout Standard
The study of neuronal networks in biology prompted the development of artificial
 neuronal networks.
 After an introduction to biological and artificial neuronal networks we
 will give an overview of machine learning and then introduce the own work
 done in this manuscript.
\end_layout

\begin_layout Section
Biological and Artificial Neuronal Networks
\end_layout

\begin_layout Standard
Artificial neuronal networks
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
artificial neuronal networks
\end_layout

\end_inset

 are mathematical constructs, designed to imitate the signal processing
 capabilities of real neurons, found in nearly all animals.
 Like their biological counterparts, artificial neuronal networks consist
 of simpler building blocks, the neurons.
\end_layout

\begin_layout Subsection
Neurons As Basic Signal Processing Units
\end_layout

\begin_layout Standard
The biological neurons are defined (according to the neuron doctrine 
\begin_inset CommandInset citation
LatexCommand cite
key "BullockDouglas2005"

\end_inset

) as the smallest units whose state change may be called signal processing,
 so they are the basic signal processing units.
 They have multiple inputs at dendrites, and multiple outputs at axon terminals
 
\begin_inset CommandInset citation
LatexCommand cite
key "ByrneDafny1997"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/neurons.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Schematic image of three biological neurons.
 A: neuron body B: nucleus C: dendrite D: synapse E: axon projecting from
 a distant neuron.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In most real neurons, the signal transmission and processing is facilitated
 by alternating small electric (action) potentials (along the axons) and
 chemical transmissions (at chemical synapses between axon and dendrite).
 The electric potential is transmitted along the dendrites of a neuron,
 and flows to the axon of the neuron, where it can lead to the release of
 neurotransmitters stored in the axon terminals into the synaptic cleft.
 The released neurotransmitters are detected by receptors and cause ion
 channels in the adjacent dendrites of other neurons to open, which changes
 their membrane potential.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/synapse.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Schema of a chemical synapse.
 The signal is transmitted from the axon terminal (left) to the dendrite
 (right).
 Grey: membranes of neurons.
 Green and blue: ion transporters maintain intracellular ion concentrations.
 Red: neurotransmitter is stored inside the cell in vesicles and emitted
 into the synaptic cleft upon an electric potential arriving at the axon
 terminal.
 Purple: receptors signal to the inside of the cell the absence or presence
 of neurotransmitter on the outside of the cell.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Action Potentials, Their Propagation, and Chemical Synapses 
\end_layout

\begin_layout Standard
The action potentials are realized by cells in the form of different ion
 concentrations inside and outside the cell.
 These ion gradients are maintained in the resting state by the 
\begin_inset Formula $Na^{+}/K^{+}$
\end_inset

-ATPases that pump 3 
\begin_inset Formula $Na^{+}$
\end_inset

 ions out of and 2 
\begin_inset Formula $K^{+}$
\end_inset

 ions into the cell for every ATP molecule 
\begin_inset CommandInset citation
LatexCommand cite
key "LodishZipursky2000"

\end_inset

.
 Because ions are charged, there is an electric potential between the outside
 and inside of the cell.
 The resting potential is between -80mV and -40mV, depending on the type
 of neuron.
 The electric potential becoming more positive is called depolarization,
 and the opposite hyperpolarization.
\end_layout

\begin_layout Standard
The propagation of the action potentials along dendrites is realized by
 the opening and closing of ion channels.
 Once depolarization of an adjacent region of a neuron causes the electric
 potential between the inside and outside of a 
\begin_inset Formula $Na^{+}$
\end_inset

 ion channel to reach a critical value, the ion channel opens, causing further
 depolarization in adjacent regions of the neuron.
 This positive feedback loop continues until all 
\begin_inset Formula $Na^{+}$
\end_inset

 channels are open.
 At the peak of depolarization, 
\begin_inset Formula $K^{+}$
\end_inset

 ion channels open, causing hyperpolarization, and the potential returns
 to the resting  potential.
 This makes the action potential travel along the neuron.
 Once it has reached an axon terminal, it causes neurotransmitter release.
\end_layout

\begin_layout Standard
Neurotransmitters binding to receptors
\begin_inset Note Note
status open

\begin_layout Plain Layout
for example, AMPA and NMDA receptors
\end_layout

\end_inset

 present on the outside of the neuron's membrane cause ion channels to open,
 and the ions flow into or out of the cell to achieve equilibrium of ion
 concentration.
 The type of ion channel being opened upon binding of a neurotransmitter
 can cause either depolarization or hyperpolarization of the dendrite, depending
 on the charge of the ion, and whether the resting concentration of the
 ion is higher intracellular or extracellular.
 If a critical threshold of depolarization is reached, the 
\begin_inset Formula $Na^{+}$
\end_inset

 ion channels will open, and an action potential 
\begin_inset Quotes eld
\end_inset

spike
\begin_inset Quotes erd
\end_inset

 is generated as described above.
\end_layout

\begin_layout Subsubsection
Encoding of Information in Action Potentials
\end_layout

\begin_layout Standard
The presence of a critical threshold suggests that it is not the 
\begin_inset Quotes eld
\end_inset

analogue
\begin_inset Quotes erd
\end_inset

 electric potential, but the 
\begin_inset Quotes eld
\end_inset

digital
\begin_inset Quotes erd
\end_inset

 spike that carries the information from one neuron to the next.
 For example, the strength muscles are innervated with, is encoded in the
 number of action potentials per time delivered by the muscle neuron to
 the muscle fiber.
 However, some neurons involved in perception directly transmit information
 in the fluctuations of neurotransmitter released.
 This analogue mode of transmission allows more information to be transmitted
 per time.
 Sub\SpecialChar \-
threshold emission of neurotransmitter also seems to modulate subsequent
 action potentials, allowing for a mixture of analogue and digital information
 transmission 
\begin_inset CommandInset citation
LatexCommand cite
key "DebanneRama2013"

\end_inset

.
 
\end_layout

\begin_layout Standard
Examples for neuronal networks that have been partly decoded are the eye
 (visual system) and the nose (olfactory system).
\end_layout

\begin_layout Subsection
Examples of Biological Neuronal Networks
\end_layout

\begin_layout Subsubsection
The Eye, a Visual System
\end_layout

\begin_layout Standard
In the eye, specialized cells called rods and cones detect light
\begin_inset CommandInset citation
LatexCommand cite
key "Biochemistry2002,Kolb2003"

\end_inset

.
 Rods are more sensitive to dim light, while the three types of cones react
 to bright light only but can differentiate between colors.
 Both rods and cones release the neurotransmitter glutamate continuously
 into the synaptic cleft, but when hit by light, suspend this emission for
 the duration of the light.
 This is implemented by the cell by a long pathway.
\end_layout

\begin_layout Standard
Specifically, light elicits a transformation of cis-rhodopsin to trans-rhodopsin
, which presents on its surface a G protein binding site.
 The G protein transducin binds to the activated rhodopsin, and in this
 process GDP acquires a phosphate group to form GTP.
 The 
\begin_inset Formula $\alpha$
\end_inset

-subunit of transducin activates a cGMP phosphodiesterase, which in turn
 hydrolyzes cGMP to GMP.
 The reduction in the concentration of cGMP causes cGMP-gated ion channels
 to close.
 This in turn hyperpolarizes the photosensitive cell, causing glutamate
 to be released into the synaptic cleft at a slower rate.
 This long pathway between cis-rhodopsin and glutamate release inhibition
 facilitates an amplification of the signal at every step, which allows
 rod cells to signal a spike in response to it being hit by a single photon.
\end_layout

\begin_layout Standard
The area that elicits a response in the cell upon being illuminated is called
 the 
\emph on
receptive field
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
receptive field
\end_layout

\end_inset

, and is just as large as the top of the photoreceptor for rods and cones.
 The released glutamate binds to receptors present on the outside of bipolar
 cells, and, depending on the type of bipolar cell, cause either an action
 potential to be generated when the photoreceptor is lit and the surrounding
 area is dark (
\emph on
ON 
\emph default
bipolar cell), or when the photoreceptor is dark against a bright background
 (
\emph on
OFF 
\emph default
bipolar cell).
 Another type of cell, the horizontal cell integrates signals from surrounding
 cone cells, and feed their signal back to the cones, or directly to bipolar
 cells.
 This enhances contrast.
 The signal from several bipolar cells is fed into a ganglion cell, which
 therefore has a larger receptive field than its connected bipolar cells.
 ON bipolar cells only excite ON ganglion cells, and OFF bipolar cells excite
 only OFF ganglion cells.
 Finally, in primates, there are more than a million nerve fibers from ganglion
 cells to the visual cortex of the brain.
 Altogether, the basic cell types are, depending on the species, 1 to 4
 types of horizontal cells, 11 types of bipolar cells, 22 to 30 types of
 amacrine cells, and 20 types of ganglion cells.
 Among those cell types' known functions are integration of a large number
 of rods to provide sight in little light, brightness-dependent size regulation
 of the receptive field of amacrine cells, and an additional photoreceptor
 distinct from rods and cones
\begin_inset CommandInset citation
LatexCommand cite
key "Kolb2003"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier ein Bild einfügen, das links ein biologisches neuronales Netz
 zeigt, und rechts ein artifizielles.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Odor Sensing in the Olfactory System
\end_layout

\begin_layout Standard
The olfactory system of mammals and insects contains neurons that detect
 odor molecules, called glomeruli
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

.
 In humans, there are about 500 different types of glomeruli [1], but it
 is hypothesized that a human can perceive around 10,000 different odors.
 Each 
\begin_inset Quotes eld
\end_inset

atomic
\begin_inset Quotes erd
\end_inset

 odor consisting of a few (<100) molecular species excites one or more glomeruli
, and the compression requires each glomerulus to signal the presence of
 one or more than one odor.
 The excitation pattern of multiple glomeruli must be resolved in the olfactory
 neuronal system so that a low-dimensional vector of (
\begin_inset Formula $\approx$
\end_inset

500) glomeruli activations is decompressed to a high-dimensional representation
 of (
\begin_inset Formula $\approx$
\end_inset

10,000) odors in the brain.
\end_layout

\begin_layout Standard
Each glomerulus is connected to one or more Kenyon Cells in insects.
 It is assumed that the activation of a Kenyon cell signals to the insect
 nervous system the presence of one specific odor.
 (In the mammalian brain, a single odor is represented by neurons in the
 olfactory cortex.) Experiments show that the circuit connecting glomeruli
 to Kenyon cells is feed-forward only, i.e.
 without recurrent connections (loops).
 The structure of a feed-forward compressed sensing circuitry is of interest,
 because standard compressed sensing circuits are recurrent dynamic systems
 that converge to one of their attractor states.
 In addition to quick decoding of odors, experimental evidence shows that
 the biological compressed sensing circuitry is robust to noise, i.e.
 to spurious neuronal spikes in glomeruli, or noise due to experimental
 inhibition
\begin_inset Foot
status open

\begin_layout Plain Layout
Also experimental modifications of glomeruli have been made so that half
 of all glomeruli always express only one type of receptor.
\end_layout

\end_inset

 of glomeruli.
\end_layout

\begin_layout Standard
The theoretical work of 
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

 proposed that a feed-forward architecture could facilitate odor decoding
 simply by implementing a logical AND
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

That is, odor component i will be detected as present if all glomeruli that
 feed signals to node i in the reconstruction layer are activated.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
 They suggest that in the neuronal AND-circuit a specific odor's Kenyon
 cell is activated when at least (for example) 80% of the glomeruli with
 receptors to this odor are active.
 On a cellular level, this could be realized by connecting the glomeruli
 associated with an odor with the odor's Kenyon cell, and a threshold at
 the Kenyon cell's input.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The following is from 
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

's discussion section.
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

 showed that the optimal connectivity between glomeruli and Kenyon cells
 
\begin_inset Formula $p_{m}$
\end_inset

 only depends on the sparseness (i.e.
 number of molecular species in an odor) 
\begin_inset Formula $K$
\end_inset

: 
\begin_inset Formula $p_{m}=1/(K+1)$
\end_inset

.
\end_layout

\end_inset

A prediction of 
\begin_inset CommandInset citation
LatexCommand cite
key "ZhangSharpee2016"

\end_inset

 is that the number of glomeruli activated by a single odor should be close
 to the number of glomeruli that are connected to a Kenyon cell.
 They postulated further that the validity of their feed-forward model can
 be tested by measuring the odor sparseness
\begin_inset Foot
status open

\begin_layout Plain Layout
Odor sparseness is the average number of different molecular species in
 an odor.
\end_layout

\end_inset

 in the environment of an animal species and comparing it to its average
 number of connections from glomeruli to Kenyon cells.
\end_layout

\begin_layout Standard
For example, in 
\emph on
Drosophila
\emph default
, about 9% of the glomeruli are excited by an odorant, and the connectivity
 rate between glomeruli and Kenyon Cells is between 6.5% and 12.5%.
 In the locust, a projection neuron (the equivalent to a glomerulus) is
 activated by half of the odorants and the connectivity rate is about 50%.
 This is in agreement with the proposed model.
\end_layout

\begin_layout Standard
The model also predicts that species with sparse connectivity have better
 odor perception of complex odor mixtures.
 On the other hand, species with dense connectivity should have better olfactory
 performance in detecting simple odor mixtures.
\end_layout

\begin_layout Subsection
Artificial Neurons as Simple Models of Biological Neurons
\end_layout

\begin_layout Standard
The machinery facilitating propagation and transmission of information in
 and between biological neurons is highly simplified in artificial neurons
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
artificial neuron signal processing
\end_layout

\end_inset

.
 Signal processing of a real neuron is modelled in an artificial neuron
 as a mathematical function that has multiple input variables, computes
 a value according to the function formula and its parameters and outputs
 its computed value to multiple neurons, which use it as an input variable.
 Herein, the processes of neurotransmitter release, de- and hyperpolarization,
 and propagation of the action potential are abstracted away into discrete
 time steps.
\end_layout

\begin_layout Standard
Each artificial neuron's function is evaluated once per time step.
 Often, the 
\emph on
sigmoid
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sigmoid function
\end_layout

\end_inset


\emph on
 
\emph default
function is used to describe the output of an artificial neuron, the so-called
 
\emph on
activation
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
activation of an artificial neuron
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
o_{i}=\sigma(v_{i})=\frac{1}{1+\exp(-v_{i})}
\]

\end_inset

where 
\begin_inset Formula $v_{i}\in\mathbb{R}$
\end_inset

 is the accumulated input to neuron 
\begin_inset Formula $i$
\end_inset

, and 
\begin_inset Formula $o_{i}=\sigma(v_{i})\in[0;1]$
\end_inset

 is the activation of neuron 
\begin_inset Formula $i$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier eine Figure mit dem Plot von 
\begin_inset Formula $\sigma(x)$
\end_inset

 einfügen.
\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename images/plot-sigmoid.pdf
	lyxscale 50
	width 45col%

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename images/neuronal-network-example.dia
	width 45col%

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:sigmoid-function"

\end_inset

Left: The sigmoid function 
\begin_inset Formula $\sigma$
\end_inset

.
 Right: Schema of the computational steps in a neuronal feed-forward network
 from input layer to output layer.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The effect of an incoming axon onto a neuron, that is, the different types
 of receptors that can be present on the outside of a real dendrite, and
 the effected de- or hyperpolarization of the dendrite are abstracted away
 by using real-numbered weights.
 Weights are parameters to the mathematical function describing the conversion
 of outputs of neurons to the single input of the next connected neuron.
 Usually the following formula is used to describe the computation of the
 input 
\begin_inset Formula $v_{i}$
\end_inset

 of neuron 
\begin_inset Formula $i$
\end_inset

 from the outputs of its connected neurons 
\begin_inset Formula $\mathbf{c_{i}}$
\end_inset

:
\begin_inset Formula 
\[
v_{i}=-b_{i}-\sum_{j\in\mathbf{c_{i}}}o_{j}w_{ij}
\]

\end_inset

where 
\begin_inset Formula $b_{i}\in\mbox{\mathbb{R}}$
\end_inset

 is the so-called 
\emph on
bias
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
bias of an artificial neuron
\end_layout

\end_inset


\emph on
 
\emph default
of neuron 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $\mathbf{c_{i}}$
\end_inset

 is the vector of indices of its in-going connected neurons, 
\begin_inset Formula $o_{j}\in\mathbb{R}$
\end_inset

 is the activation of the connected neuron 
\begin_inset Formula $j$
\end_inset

, and 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

 is the weight of the connection going out of neuron 
\begin_inset Formula $j$
\end_inset

 and into neuron 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Subsection
Learning
\end_layout

\begin_layout Standard
Nervous systems do not only process signals, but they also learn, that means
 that they adapt their signal processing over time.
 One reason for this is an organism's need for a change in behavior, as
 response to a changing environment.
\end_layout

\begin_layout Standard
In biological neuronal systems, this is possible by altering existing synapses
 (for example by exchanging the receptors on the surface of dendrites),
 or by creating and abandoning existing synapses (i.e.
 connecting the axon terminals of a neuron to different neurons).
 There are several known cellular mechanisms for that, among them LTP, LTD,
 and PTP
\begin_inset Note Note
status open

\begin_layout Plain Layout
post-tetanic potentiation
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "BermudezFederico2007"

\end_inset

.
 Strengthening of the synaptic link (that occurs within minutes and remains
 after hours and up to weeks in the hippocampus of mammals) is called long-term
 potentiation (LTP), while its weakening is called long-term depression
 (LTD).
 LTP is induced by associativity of connected neurons, that means, when
 a neuron contributes to the depolarization in a directly connected neuron,
 the efficiency of that connection will be strengthened
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe (after reading) cite 
\begin_inset CommandInset citation
LatexCommand cite
key "GalanGalizia2006"

\end_inset


\end_layout

\end_inset

.
 The rule for this strengthening is also called the 
\emph on
Hebbian learning rule
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Hebbian learning rule
\end_layout

\end_inset

.
 The molecular mechanisms responsible for this phenomenon are not yet completely
 understood.
 It is known that they differ between brain regions, and also between types
 of synapses in the same brain region.
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier evtl.
 weitermachen mit Erklären und Zitieren der 
\begin_inset Quotes eld
\end_inset

Hebbian learning rule
\begin_inset Quotes erd
\end_inset

: die synapse zwischen zwei benachbarten neuronen wird gestärkt, wenn beide
 neuronen gleichzeitig aktiv sind.? Ein Einblick in molekulare Abläufe wäre
 wünschenswert.
\end_layout

\end_inset

 The cellular mechanisms controlling these processes, and their interplay
 in larger neuron ensembles are a field of active research
\begin_inset CommandInset citation
LatexCommand cite
key "BermudezFederico2007"

\end_inset

.
\end_layout

\begin_layout Paragraph
The Hebbian learning rule is local
\end_layout

\begin_layout Standard
Of note in the Hebbian learning rule is that it is local
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
locality of Hebbian learning rule
\end_layout

\end_inset

, which means that changes at a synapse only depend on the directly connected
 neurons, but not on other distantly-connected neurons.
\end_layout

\begin_layout Section
Introduction to Machine Learning
\end_layout

\begin_layout Subsection
Supervised and Unsupervised Machine Learning
\end_layout

\begin_layout Standard
In artificial neuronal networks, the goals of learning are defined by humans.
 In machine learning in general, there are two major types of learning:
 
\emph on
supervised
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
supervised machine learning
\end_layout

\end_inset

 and 
\emph on
unsupervised learning
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
unsupervised machine learning
\end_layout

\end_inset

.
 Both methods process data sets that are in matrix form: for example, in
 expression data, the rows usually denote different genes or transcripts,
 and each column represents an independently measured sample.
 (Note that in the general machine learning literature, usually the data
 matrix is transposed: the columns denote the features, and the rows the
 samples.) Samples usually are tissue, blood samples, or cell line, and differ
 in their biological background (e.g.
 cell type, gene knock-out or knock-in, cell cycle phase) or treatment (e.g.
 drugs applied).
\end_layout

\begin_layout Standard
In supervised learning, for every input pattern in the data set there is
 defined an output pattern that the learner should compute from the input
 pattern.
 Herein, both input and output pattern can be one- or multi-dimensional
 vectors.
 The goal of supervised learning is to infer a function that maps from the
 space of input patterns to the space of output patterns.
 The output patterns are also called 
\emph on
labels
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
labels
\end_layout

\end_inset

, and one says that  
\begin_inset Quotes eld
\end_inset

the input data is labeled
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
In unsupervised learning, there are only input data and the goal is to find
 interesting underlying patterns in them.
 Unsupervised learning algorithms usually compute a mapping from input space
 to output space, where both spaces can differ in their dimensionality.
 This can be used for example for dimensionality reduction, or data re-represent
ation and abstraction.
\end_layout

\begin_layout Standard
Examples of supervised learning algorithms are (linear or logistic) regression,
 k-Nearest Neighbor (k-NN) regression, support vector machines (SVMs), backpropa
gation neuronal networks, Deep Belief Networks (DBNs).
 Examples for unsupervised learning algorithms are (hierarchical or kNN)
 clustering, self-organising maps (SOMs), principal component analysis (PCA),
 Restricted Boltzmann Machines (RBMs).
\end_layout

\begin_layout Standard
The samples are split into training, validation and test data sets.
 The training samples
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
training data set
\end_layout

\end_inset

 are used to train a machine learning algorithms.
 Some machine learning algorithms have meta-parameters, which are optimized
 using the validation data set
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
validation data set
\end_layout

\end_inset

.
 At the end of all training, the performance of the machine learning algorithm
 must be evaluated on previously unseen samples, the testing data set
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
testing data set
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
Semi-supervised Machine Learning
\end_layout

\begin_layout Standard
An intermediate form of these two machine learning modes is semi-supervised
 learning
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
semi-supervised learning
\end_layout

\end_inset

.
 In contrast to supervised machine learning, which has for every input pattern
 a corresponding target output pattern, semi-supervised learning does not
 need a target output pattern for every input pattern.
 However, in contrast to fully unsupervised machine learning, it does need
 some labeled input data sets.
 Semi-supervised machine learning algorithms often try to find underlying
 structure in all input data sets and then use the known labels to assign
 probable labels to the found structure (or just the unlabeled input samples).
 This assumes that samples close in the (high-dimensional) input space probably
 have the same label.
 Another assumption is that samples distant from each other probably have
 different labels.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
A
\begin_inset Graphics
	filename images/summer_school_2014-intuitive2-semi_supervised_learning.eps
	lyxscale 30
	width 30col%

\end_inset

 B
\begin_inset Graphics
	filename images/summer_school_2014-intuitive4-semi_supervised_learning.eps
	lyxscale 30
	width 30col%

\end_inset

 C
\begin_inset Graphics
	filename images/summer_school_2014-intuitive5-semi_supervised_learning.eps
	lyxscale 30
	width 30col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Illustration-of-semi-supervised-learning"

\end_inset

Illustration of semi-supervised learning in two dimensions.
 Each axis is a dimension.
 Circles are samples; filled circles are labeled samples; unfilled circles
 are unlabeled samples.
 The blue circles are samples with label 1, the orange circles are samples
 with label 2.
 A: Supervised SVM learning produces a maximum-margin classifier.
 B: Supervised learning ignores and probably mis-classifies some unlabeled
 samples (the red crossed-out samples).
 C: Semi-supervised learning regards densities of unlabeled samples and
 may give better results than supervised learning on the labeled samples
 alone.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are two types of semi-supervised learning: transductive and inductive
 semi-supervised learning.
 The goal of transductive semi-supervised learning
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
transductive semi-supervised learning
\end_layout

\end_inset

 is to predict the class labels of a pre-specified list of unlabeled input
 patterns, while the goal of inductive
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
inductive semi-supervised learning
\end_layout

\end_inset

 semi-supervised learning is to find a universal rule mapping from the space
 of input patterns to class labels, which could be applied to classify unknown,
 future input patterns.
 In case unknown, future input patterns are to be classified using transductive
 semi-supervised machine learning, the whole model may have to be re-evaluated.
\end_layout

\begin_layout Subsubsection
Properties of Semi-supervised Learning
\end_layout

\begin_layout Standard
An advantage of semi-supervised learning over supervised learning is that
 it does not need labels for all input patterns, because labels are often
 time-consuming or costly to acquire.
 For example, the Gene Expression Omnibus data base (GEO) 
\begin_inset CommandInset citation
LatexCommand cite
key "BarrettSoboleva2013"

\end_inset

 contains 41,379 expression data sets that were uploaded between Jan 1st,
 2000, and August 31st, 2013.
 Many of these are potentially usable as unlabeled data sets in semi-supervised
 learning.
\end_layout

\begin_layout Standard
However in practice, many machine learning algorithms require samples to
 be independetly and identically distributed
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
independetly and identically distributed
\end_layout

\end_inset

 (iid
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
iid
\end_layout

\end_inset

).
 In an ideal world, GEO samples could be assumed to be identically distributed
 within a data set.
 Unfortunately even within the same GEO data set there often is systematic
 variation between samples, called the 
\emph on
batch-effect
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
batch-effect
\end_layout

\end_inset

, caused, for example, by different sample handling or conditions at measurement
 time.
 This means one either has to use samples from one data set only, or, if
 one wants to use samples from different GEO data sets simultaneously, correct
 for a possible batch-effect manually, or use an algorithm that has some
 built-in mechanism to make such a correction.
\end_layout

\begin_layout Standard
One machine learning algorithm with such a built-in mechanism is
\emph on
 deep learning
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
deep learning
\end_layout

\end_inset

, as employed in 
\emph on
Deep Belief Networks
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset


\emph on
 (DBNs
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DBN
\end_layout

\end_inset


\emph on
)
\emph default
 that have been described by 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

.
 This machine-learning algorithm (which can be used unsupervisedly, supervisedly
, as well as semi-supervisedly) was shown to be able to learn from images
 of objects or faces, where the objects or faces are in different lighting
 conditions or are viewed from different angles (
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006,KrizhevskyHinton2012,KarpathyFei-Fei2014"

\end_inset

).
 In this setting, the batch effect would be the lightning condition or viewing
 angle.
 Such results seem to imply that DBNs are able to abstract the images, which
 are given as vectors of pixels, into encodings of relevant features and
 compute a classifier on these abstract features.
 This can be seen as a form of batch-effect correction.
\end_layout

\begin_layout Standard
Before we motivate using DBNs on expression data, we first introduce deep
 learning.
 Deep learning
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
deep learning
\end_layout

\end_inset

 is a term used in conjunction with artificial neuronal networks which have
 several hidden layers.
 The advantage of a deep network over a network with just one hidden layer
 is that it can model a problem more compactly using less hidden neurons
 in total, because it has more than one intermediate computation step.
 Before deep learning there was back-propagation, which has problems coping
 with more than one hidden layer.
\end_layout

\begin_layout Subsection
Back-propagation
\end_layout

\begin_layout Standard
Before deep learning, supervised artificial neuronal networks were usually
 trained using back-propagation.
 Back-propagation learns from labeled samples the weights and biases of
 an artificial neuronal network 
\begin_inset CommandInset citation
LatexCommand cite
key "RumelhartWilliams1988"

\end_inset

.
 This supervised training procedure sets input activations in the input
 layer according to the input pattern to be learned, computes the activations
 of the hidden layers and the output layer, and then computes the error
 at each node in the output layer by subtracting the computed activation
 from the target activation as given by the desired output pattern for the
 particular input pattern that was fed into the network.
 The error for each node is then back-propagated in reverse input direction
 to the hidden layers and finally to the input layer.
 The purpose of error back-propagation is to be able to adjust the weights
 of the artificial neuronal network following the gradient, such that when
 the current input pattern pair is presented to the network, its computed
 output pattern gets closer and closer to the desired output pattern.
 However, back-propagation was not able to train networks with more than
 one or two hidden layers, because it got stuck in poor local energy optima.
 Having artificial neuronal networks with more than one hidden layer is
 desirable from a computational power standpoint.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/introduction-backpropagation-data-flow.dia
	width 34col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Data flow in training a neuronal network with three layers using back-propagatio
n.
 The input layer is at the bottom, the hidden layer in the middle, and the
 output layer at the top.
 The black arrows denote information flow about the activation state from
 the node at the arrow tail to the node at the arrow head.
 Each arrow is associated with a weight from tail to head.
 The orange arrows denote information flow during error back-propagation.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Networks with Multiple Hidden Layers
\end_layout

\begin_layout Subsubsection
Autoassociator, Autoencoder
\begin_inset CommandInset label
LatexCommand label
name "sub:Autoassociator-introduction"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: vielleicht sollte die Beschreibung eines autoassociators hier kürzer
 sein, und diese lange Beschreibung stattdessen in den Methods-Teil, da
 sonst die Beschreibung von Deep learning danach sehr kurz wirkt.
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier sollte ein Bild von einem Auto-associator stehen.
\end_layout

\end_inset

An algorithm that can train an artificial neuronal network with more than
 one hidden layer is the auto-associator
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
auto-associator
\end_layout

\end_inset

, or autoencoder
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
autoencoder
\end_layout

\end_inset

.
 The algorithm is unsupervised and iteratively constructs deeper and deeper
 networks.
 Its essential idea is the construction of an encoder network and its anti-symme
tric counterpart, the decoder network.
 The encoder starts in the first iteration as a network that consists of
 the input layer and one hidden layer on top.
 The (overlapping) decoder network consists of the very same hidden layer
 and the output layer on top, which must have the same dimensions as the
 input layer.
 Training an autoencoder slowly adds internal layers to en- and decoder.
\end_layout

\begin_layout Standard
Because the network size increases iteratively from only 3 layers, to 5
 layers, to 7 layers, and so on, and only 2 weight layers are initialized
 randomly in each iteration, back-propagation is often able to find parameters
 for a network that is not stuck in a poor local optimum.
\end_layout

\begin_layout Subsubsection
Deep Learning
\end_layout

\begin_layout Standard
Deep learning
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
deep learning
\end_layout

\end_inset

, as demonstrated by 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 also overcame the limitation of only a few hidden layers.
 It is separated into a pre-training phase and a fine-tuning phase.
 The pre-training phase is unsupervised.
 In this phase, hidden layers are iteratively added on top of the input
 layer of the network, and the weights between layers are initialized and
 pre-trained like 
\emph on
Restricted Boltzmann Machines
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Restricted Boltzmann Machine
\end_layout

\end_inset

 (RBMs
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
RBM
\end_layout

\end_inset

).
 Without this step, fine-tuning a multiple-hidden-layer network usually
 gets stuck in a local error minimum.
 The pre-training algorithm is called
\emph on
 contrastive divergence
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
contrastive divergence
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
In unsupervised pre-training, unlabeled training samples can be used in
 learning the weights between layers.
 While the pre-training phase is unsupervised, the fine-tuning phase can
 be unsupervised as well as supervised.
 After training, the multiple-hidden-layer-network forms a generative artificial
 neuronal network called a 
\emph on
Deep Belief Network
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset


\emph on
 
\emph default
(DBN
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DBN
\end_layout

\end_inset

).
\end_layout

\begin_layout Section
Overview of Own and Related Work
\end_layout

\begin_layout Standard
In the work that is described later, we used auto-associators, Deep Belief
 Networks, and TSVMs on expression data to predict whether breast cancer
 patients will show pathologic complete response to chemotherapy or residual
 disease.
 The expression data are high-dimensional (
\begin_inset Formula $\approx$
\end_inset

22,000 genes) and we use a relatively large data set (
\begin_inset Formula $\approx$
\end_inset

500 patients).
\end_layout

\begin_layout Standard
Here we give an overview of the motivation, other applicable methods, and
 other related work.
\end_layout

\begin_layout Subsection
Motivations for Using Deep Belief Networks on Transcriptomic Data
\end_layout

\begin_layout Standard
The motivations for using Deep Belief Networks on transcriptomic data come
 from those networks' successes when used on image data.
 In the hand-written digit classification and graphical object recognition
 data sets on which the deep artificial neuronal networks were developed,
 they are among the best-performing predictors.
\end_layout

\begin_layout Subsubsection
The ImageNet Large Scale Visual Recognition Challenge
\end_layout

\begin_layout Standard
This shall be exemplified by image classification in the ImageNet Large
 Scale Visual Recognition Challenge 
\begin_inset CommandInset citation
LatexCommand cite
key "RussakovskyFeiFei2015"

\end_inset

.
 It is a yearly contest, wherein participants are given around 1.2 million
 training images.
 Each training image is labeled with one of 1.000 possible object categories
 describing the main object appearing in the image, for example 
\begin_inset Quotes eld
\end_inset

trumpet
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

butterfly
\begin_inset Quotes erd
\end_inset

.
 After training an image classification algorithm, each contestant must
 compute up to 5 labels for each of 100.000 test images.
 Each test image has a label, which is kept hidden by the contest organizer.
 A test image is scored as correctly classified if the correct label appears
 in the (up to 5) labels submitted by the contestant.
 Finally, the accuracy of a contestant is computed as the average fraction
 of correctly classified test images.
\end_layout

\begin_layout Standard
There have been dramatic improvements in the accuracy of the winning contestant,
 starting in 2012.
 In the last years, all top contestants have moved to using deep neuronal
 networks.
 See table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:ILSVRC-Test-set-accuracies"

\end_inset

 for the winning contestants between 2010 and 2014.
 Significant differences before and after 2012 are the usage of neuronal
 networks directly on the image pixel data, and not using pre-computed features
 in a supervised learning algorithm like a Support Vector Machine.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Year
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Winner
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Technique
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2010
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NEC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
71.8%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SIFT and LBP image features classified by SVM
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2011
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
XRCE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
74.2%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
image signatures classified by one-vs-all SVMs
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2012
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SuperVision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
83.6%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
deep convolutional neuronal network
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2013
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Clarifai
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
88.3%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
deep convolutional neuronal networks averaged
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2014
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
GoogLeNet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
93.3%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
deep convolutional neuronal network
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:ILSVRC-Test-set-accuracies"

\end_inset

Test set accuracies and techniques of winning contestants in the ImageNet
 Large Scale Visual Recognition Challenge from 2010-2014.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Highly Correlated Inputs
\end_layout

\begin_layout Standard
We will now discuss similarities between image classification and expression
 data classification.
\end_layout

\begin_layout Standard
Both underlying distributions – of images and of expression data – often
 have many correlated dimensions.
 For images, adjacent pixels often display the same object and have therefore
 correlated values.
 In some face recognition tasks for example, the faces are scaled and translated
 so that the centers of both eyes and mouth are aligned in different faces.
 There will be highly correlated pixels for areas of the image where the
 cheeks and lips usually are.
 If you use the pixels of the whole image as input to the neural network,
 the corresponding input nodes will be highly correlated as well.
\end_layout

\begin_layout Standard
For transcriptomic data, one almost always observes many correlated genes.
 The correlations can be due to many genes being regulated by the same transcrip
tion factor 
\begin_inset CommandInset citation
LatexCommand cite
key "TornowMewes2003,KlebanovYakovlev2007"

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: add some statistics of correlations, e.g.
 in the breast_cancer data set.
 for example, find a group of genes that are all correlated in all samples
 with a correlation coefficient larger than 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Plain Layout
TODO: add some citations for reasons of high correlation in expression data.
\end_layout

\begin_layout Plain Layout
TODO: read TornowMewes2003, KlebanovYakovlev2007.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Deep Belief Networks Find Correlated Nodes
\end_layout

\begin_layout Standard
Deep Belief Networks can group correlated input nodes by increasing their
 weights to a single hidden node, and decreasing the weights to all other
 hidden nodes.
 This is a form of abstraction (and dimensionality reduction), since in
 this way, the many correlated input nodes are grouped into one hidden node.
 The hidden node will only be active if many of its highly-weighted input
 nodes are active and only few of its negatively-weighted input nodes are
 active.
 Repeated application of this principle of abstraction allows the Deep Belief
 Network to form more and more abstract representations of its input.
\end_layout

\begin_layout Standard
In face recognition for example, an abstract representation might have a
 single value for the size of the lips.
 In expression data, a single node in an abstract representation might encode
 the activity of a gene module.
\end_layout

\begin_layout Subsubsection
Transductive Support Vector Machine
\end_layout

\begin_layout Standard
We compare our artificial neuronal network approach with another, older,
 and established semi-supervised method, the Transductive Support Vector
 Machine (TSVM) 
\begin_inset CommandInset citation
LatexCommand cite
key "Joachims1999"

\end_inset

.
 Despite the name, it supports transductive as well as inductive learning.
 A standard Support Vector Machine searches for a decision boundary such
 that the margin between samples with differing labels is maximal (see figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Illustration-of-semi-supervised-learning"

\end_inset

).
 The TSVM seeks a labeling of the unlabeled samples so that the decision
 boundary has the maximal margin between all samples with differing classes.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: TSVM verstehen und dann besser erklären.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Previous Work: Gene Expression Inference With Deep Learning
\begin_inset CommandInset label
LatexCommand label
name "sub:Previous-Work:Gene-Expression-Inference-with-Deep-Learning"

\end_inset


\end_layout

\begin_layout Standard
Very recently, 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

 published work on compressing expression data into fewer dimensions on
 a large scale using deep learning.
 Input data were all 
\begin_inset Formula $\approx$
\end_inset

111,000 genome-wide expression profiles from the GEO database of Affymetrix
 microarrays, which were partitioned into training, validation, and testing
 data sets.
 For each sample, the same subset of 943 "landmark" genes was chosen and
 9,520 other genes were predicted from the landmark genes.
\end_layout

\begin_layout Standard
Their artificial neuronal network architecture had between 1 and 3 hidden
 layers with either 3,000, 6,000, or 9,000 nodes.
 It had 943 input expression values (one for each landmark gene), and a
 total of 9,520 output expression values (one for each gene to be predicted).
\end_layout

\begin_layout Standard
In addition to the (non-linear) neuronal network, they tried linear regression
 with no regularization, L1-, and L2-regularization.
\end_layout

\begin_layout Standard
They also tried k-Nearest Neighbor 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
k-Nearest Neighbor
\end_layout

\end_inset

(kNN
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
kNN
\end_layout

\end_inset

).
 In it, during training, they determined a number, 
\begin_inset Formula $k$
\end_inset

, of landmark genes with expression value closest to each target gene 
\begin_inset Formula $i$
\end_inset

 (let's call this set of genes 
\begin_inset Formula $knn_{i}$
\end_inset

) in the training data set.
 During testing, they predicted the expression value of the target gene
 
\begin_inset Formula $i$
\end_inset

 as the average of the gene's 
\begin_inset Formula $knn_{i}$
\end_inset

 expression values in the testing data set.
 The optimal 
\begin_inset Formula $k$
\end_inset

 (number of genes to average over) was chosen based on a validation data
 set.
\end_layout

\begin_layout Standard
The input values were quantile normalized expression values between 4 and
 15.
 The models were ranked according to the average prediction errors over
 all 9,520 target genes.
\end_layout

\begin_layout Standard
k-Nearest Neighbor performed worst, with an average prediction error of
 0.5866.
 The three linear regression models performed about equally well, with average
 prediction errors of either 0.3784 or 0.3782, which means that regularization
 did not improve linear regression.
 The neuronal network-based average prediction errors were between 0.3421
 and 0.3204, with the network having 3 hidden layers of size 9000, and 10%
 dropout rate performing best.
 Because the input expression values were between 4 and 15, an average predictio
n error of 0.3204 implies an average error of about 3% on the GEO dataset.
\end_layout

\begin_layout Standard
In another dataset, 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

 used the GEO dataset for training, the 1000 Genomes data for validation
 
\begin_inset CommandInset citation
LatexCommand cite
key "LappalainenPedro2013"

\end_inset

, and GTEx data for testing 
\begin_inset CommandInset citation
LatexCommand cite
key "ArdlieLek2015"

\end_inset

.
\end_layout

\begin_layout Standard
Learning in this data set is harder since the input is measured in different
 ways, and therefore prone to the batch-effect.
 Nevertheless, the performance ranking of the methods was the same, but
 worse than the data set without batch-effect.
 KNN scored worst, with an average prediction error of 0.6520.
 Linear regression with L1-regularization had an average prediction error
 of 0.5667.
 Linear regression without regularization and with L2-regularization had
 an average prediction error of 0.4702.
 The artificial neuronal networks all scored consistently better than KNN
 and linear regression, with the artificial neuronal network with 2 hidden
 layers of size 9000, and 25% dropout rate having the lowest prediction
 error of 0.4393 (which is equivalent to a relative error of 4%).
 On the validation data set of this data set, the average prediction error
 was 0.7467, which is a relative error of 6.8%.
 This shows that Artificial Neuronal Networks are capable of processing
 input from multiple sources, with an acceptable gain in error.
 We also had a similar result, in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Unsupervised-Reconstruction-of-Expression-Values"

\end_inset

.
\end_layout

\end_body
\end_document

#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
We tried finding evidence that adding unlabeled data to semi-supervised
 training is beneficial in deep neuronal networks, when predicting breast
 cancer recurrence after chemotherapy.
\end_layout

\begin_layout Paragraph
Using Tumour Expression Data Directly to Classify Recurrence
\end_layout

\begin_layout Standard
We predicted recurrence directly from expression data.
 Papers that use neuronal networks to classify cancerous tissue are for
 example 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenHuang2002"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "ErcalMoss1994"

\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "SharafTsokos2015"

\end_inset

, Sharaf and Tsokos predict from 4 input variables the survival time by
 training a neuronal network on 69,000 patients
\begin_inset Note Note
status open

\begin_layout Plain Layout
see Figure 1
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
We selected the GSE25055 and GSE25065 data sets because they are among the
 largest labeled cancer data sets in GEO that come from a single source.
 Like in 
\begin_inset CommandInset citation
LatexCommand cite
key "HatzisSymmans2011"

\end_inset

, the larger data set GSE25055 was used for training a classifier, and the
 independently measured data set GSE25065 was used for testing the classifier.
 We used artificial neuronal networks as classifiers by predicting the class
 1 probability of the samples.
\end_layout

\begin_layout Paragraph
Neuronal Networks are Attractive
\end_layout

\begin_layout Standard
In our view, neuronal networks have attractive properties: they are non-linear,
 they can be used generatively, their implementations are often modular
 (e.g.
 regularizations and network parameters), in prediction they often show
 better accuracies than normal SVMs, and deep neuronal networks are among
 the best predictors in several fields.
 As 
\begin_inset CommandInset citation
LatexCommand cite
key "BiganzoliMarubini1998"

\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
on page 3
\end_layout

\end_inset

 put it: 
\begin_inset Quotes eld
\end_inset

Feed forward ANNs are strictly equivalent to non-linear multivariate regression
 methods.
\begin_inset Quotes erd
\end_inset

 They can be used unsupervisedly as well as supervisedly.
 For example, to obtain a probability for a class like done in 
\begin_inset CommandInset citation
LatexCommand cite
key "AppelSpang2011"

\end_inset

 is straightforward, because the network outputs class probabilities anyways.
 Last, but not least, artificial neuronal networks are supposed to be plausible
 models of the real biological neuronal networks, which are the control
 centers of many animals, and were shaped by evolution during millions of
 years.
 However, neuronal networks' versatility can also be a disadvantage: it
 is often not clear what component or parameter to modify to achieve a desired
 result.
\end_layout

\begin_layout Paragraph
Different Network Configurations
\end_layout

\begin_layout Standard
We tried several different approaches: we tried different pre-training algorithm
s, autoencoders Restricted Boltzmann Machines, and Deep Belief Networks.
 There seemed to be only little difference in test set accuracy between
 these pre-training algorithms.
 However, some authors have reported that autoencoders are harder to train
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: cite a paper that reported this
\end_layout

\end_inset

.
 In general, reconstruction error during pre-training converged well for
 our cases.
 Sometimes one has to wait a few iterations for the network to travel through
 configurations that do not seem to change the resulting reconstruction
 error.
\end_layout

\begin_layout Paragraph
Different Normalizations
\end_layout

\begin_layout Standard
We tried using different normalizations of the raw expression data: RMA
 and MAS5 normalization, logarithmizing, COMBAT batch effect correction,
 ZCA whitening.
 We observed the effect of normalization on reconstruction error during
 pre-training as well as on classification accuracy.
 The reconstruction error plots seem to depend on the normalization used.
 MAS5 normalized data seem to have lower reconstruction error than RMA normalize
d data.
 The effect of different normalizations on accuracy were the other way around:
 Of all normalizations tested, RMA with no additional pre-processing yielded
 the best accuracies in all data sets tested.
 Of note is that a low reconstruction error rate does not imply a good accuracy
 on the test set: Although the neuronal nets using MAS5 normalized data
 had a lower reconstruction error than their RMA counterparts, MAS5 yielded
 a lower accuracy than RMA.
\end_layout

\begin_layout Paragraph
Deep Networks
\end_layout

\begin_layout Standard
We also tried deeper networks with more than one hidden layer.
 Using these did not always improve accuracy.
 However, using more than a few hidden layers was not systematically investigate
d due to limits in computation time.
\end_layout

\begin_layout Paragraph
Model Selection
\end_layout

\begin_layout Standard
We always selected the iteration/model of the neuronal network that had
 the highest validation set accuracy.
 There was the problem that very few samples lead to very few different
 possible accuracy values.
 To be able to choose the best model among candidates that have only few
 possible accuracies, we smoothed the accuracies over time/iterations.
 We deem this justified because the network's state of parameters at a specific
 iteration is closest to its state of parameters at the closest iterations.
 The artificial neuronal networks change only little every iteration.
\end_layout

\begin_layout Paragraph
Compared Methods: Neuronal Networks, SVM and TSVM
\end_layout

\begin_layout Standard
We compared SVM and TSVM to the artificial neuronal networks.
 TSVM is a semi-supervised version of the normal supervised SVM, and the
 artificial neuronal networks were used with (semi-supervised) and without
 (supervised) pre-training.
 We compared the semi-supervised and the supervised versions of both neuronal
 network and Support Vector Machine.
\end_layout

\begin_layout Paragraph
Does Semi-supervised Learning Lead to Better Neuronal Network Classifiers?
\end_layout

\begin_layout Standard
We used an increasing number of unlabeled samples during pre-training to
 find out whether this has an effect on the accuracy achieved during fine-tuning.
 In terms of 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhu2005"

\end_inset

, we are learning an efficient coding of the domain from unlabeled data
 and then perform supervised learning on the coded samples (see their chapter
 8).
 The approach is similar to that of 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

, where they used the representation of a sample at the deepest hidden layer
 to classify the sample, using a supervised algorithm
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Siehe ihre Seite 7 links unten 
\begin_inset Quotes eld
\end_inset

To dissect the nonlinear contribution, we took a relatively simple approach
 by focusing on the representation (activations) from the last hidden layer.
 Each of the hidden units in that layer can be viewed as a feature generated
 through some nonlinear transformation of the landmark genes.
 We then studied whether a linear regression based on these nonlinear features
 can achieve better performance than a linear regression based solely on
 the landmark genes.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset

.
 Like 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhu2005"

\end_inset

, we also noticed that during data set creation for semi-supervised learning,
 one has to constrain the class proportions (in our case to 50% for class
 0 and 50% for class 1), otherwise the semi-supervised training often fails
 with predictions biased in favor of the larger class.
 Only the very last approaches tried (data set breast_cancer_15) showed
 a (partially) significant benefit between those networks trained using
 more unlabeled samples over those trained using less unlabeled samples.
 There are three differences between these approaches and the ones before:
 First, we used less hidden layer neurons.
 Second, we used all 22,283 genes as input instead of only the 500 most
 variable genes.
 Third, we tested on GSE25055 instead of GSE25065.
 The benefit using unlabeled samples is not due to the first point, since
 TSVM was not influenced by this point, but also showed the benefit.
 Whether the benefit is due to the second or third point remains to be seen.
\end_layout

\begin_layout Paragraph
Differences Between Object Recognition Tasks and Cancer Recurrence Prediction
\end_layout

\begin_layout Standard
In different settings this approach worked well 
\begin_inset CommandInset citation
LatexCommand cite
key "ErhanBengio2010"

\end_inset

.
 There are some differences between those fields and the scenario of microarray
 expression data: The dimensionality of microarrays is usually higher than
 that of images or phonemes.
 Expression data sets differ from these data sets in mainly two aspects:
 their dimensionality is higher (~1000 pixels versus ~20,000 genes), but
 the training set sizes are several magnitudes smaller (thousands to tenthousand
s versus tens to hundreds).
 For example, in the image classification task in the ImageNet Large Scale
 Visual Recognition Challenge 
\begin_inset CommandInset citation
LatexCommand cite
key "RussakovskyFeiFei2015"

\end_inset

, there are an average of 
\begin_inset Formula $\approx$
\end_inset

1,200 images per class, while in the GSE25055 data set, there are 249 class
 0 and 57 class 1 samples.
\end_layout

\begin_layout Standard
The fact that there is a much smaller number of labeled and unlabeled samples
 in expression data than in image recognition is probably due to prices.
 The price of microarrays (and RNASeq) is magnitudes larger (hundreds of
 Euros or dollars) than the price to take and label pictures of hand-written
 digits or objects (images are ubiquitous on the internet, and manual or
 computer-assisted labeling is relatively cheap).
\end_layout

\begin_layout Standard
The difficulty of the learning task seems higher than that of recognizing
 digits, where accuracies of 96% can be achieved by nearest neighbor classifiers.
 However, it may be comparable to the object recognition tasks, that had
 accuracies around 70% using SVMs in the ImageNet Large Scale Visual Recognition
 Challenge.
\end_layout

\begin_layout Paragraph
Outlook: More output
\end_layout

\begin_layout Standard
More sophisticated prediction schemes can be imagined so that the prediction
 made produces more than one number: For example, in addition to the severity
 of the disease after therapy (a number between 0 and 1), the number of
 metastases (a natural number) could be trained and predicted.
 Such a prediction is not made in this work, because only little clinical
 data was recorded in GSE25055 and GSE25065.
 However, in the neuronal network it would be straightforward to predict
 such two numbers by adding an additional (properly scaled) number to the
 output layer in the training set.
\end_layout

\begin_layout Paragraph
Outlook: Multiplying of Training Samples
\end_layout

\begin_layout Standard
In most applications of deep learning there is some algorithm involved to
 multiply the number of available training data sets.
 For example, in image classification the training images are usually translated
 by pixel or subpixel shifts, or small non-linear deformations are applied
 using a warped mesh.
 This has the effect that a local feature of an input training image (for
 example, a red pixel on green background) is present in different input
 pixels in the transformed training images.
 This allows producing a large number of similar training images from an
 input training set.
 The neuronal network is thereby forced to learn the property of a feature
 regardless of its position in the image.
 Nevertheless the position of the feature will probably vary in 
\begin_inset Quotes eld
\end_inset

real
\begin_inset Quotes erd
\end_inset

 (not modified) images as well.
\end_layout

\begin_layout Standard
Having such a transformation for expression data would be very useful, not
 only for classification using neuronal networks, but also other machine
 learning algorithms.
 However, it is not at all clear what a pre-processing equivalent to the
 local image deformations could look like for mRNA abundance.
 Straightforward application of the image deformation scheme would provide
 the neuronal network with input for a gene in the dimension of a maybe
 completely unrelated gene.
 (Note that adding some sort of noise onto the expression levels would be
 equivalent to adding noise to the image, which is not equivalent to shifting
 the image.) A possible approach could be to look for gene modules that consist
 of redundant genes, and permute their expression values among the redundacy
 group.
 This would require knowledge about gene modules in advance.
 Maybe one could increase the number of input samples by creating additional
 samples by composing them of random subsets of other input samples.
 For example, take gene 1-1000 from sample 1, gene 1001-2001 from samples
 2, and so on.
 Or maybe even better use gene modules as learned by an RBM (see the 
\emph on
hub features 
\emph default
in Figure S3 of 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenXie2015"

\end_inset

).
 There seem to be hub networks, that have outgoing connections to many output
 layer genes, with many of the hub networks having either positive (i.e.
 that positively affect the expression of a target gene) or negative outgoing
 weights, but not both.
\end_layout

\begin_layout Standard
The algorithm would work like this: Determine for each 
\emph on
measured
\emph default
 input sample the gene hub activation (by training an RBM or unsupervised
 DBN on all input samples).
 This results in a vector of numbers, one vector for each input sample;
 each number stands for one gene hub activation.
 Permute the gene hub activations between the learned representations, but
 only use representations from samples that have the same class label.
 Due to the number of permutations this creates a large number of labeled
 training samples (each training sample is labeled like the measured samples
 used in the training sample generation).
 Use these generated training samples as input for a supervised DBN.
\end_layout

\begin_layout Paragraph
In Summary
\end_layout

\begin_layout Standard
To sum up, we tried different training parameters, different network architectur
es (with many free parameters relative to training data set size, and with
 few free parameters), different normalizations, different data set compositions.
 Only a small improvement in testing set accuracy was achieved by using
 additional unlabeled data during training.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Itemize
The accuracy of the classifier in the GSE25055-paper is 65% on the testing
 data.
 I should compare that to the accuracies that my classifiers achieve.
 (Of note should be however, that they did not optimize for accuracy, but
 for another measure.)
\end_layout

\begin_deeper
\begin_layout Itemize
also calculate and tabulate sensitivity, selectivity, Diagnostic likelihood
 ratios
\end_layout

\end_deeper
\begin_layout Itemize
Our work differs from the ".../zusammenfassung/applications/Bioinformatics-2016-Che
n-bioinformatics_btw074-supplementary.pdf" paper in that they use purely
 supervised learning, and we try semi-supervised learning.
\end_layout

\end_inset


\end_layout

\end_body
\end_document

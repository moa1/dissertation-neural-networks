#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This is supposed to be a high-level description of neuronal networks, to
 be used in the thesis before the mathematical description of neuronal networks
 (currently 
\begin_inset Quotes eld
\end_inset

methods.lyx
\begin_inset Quotes erd
\end_inset

).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Insert sections and paragraphs into the following text and title them
 appropriately.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Artificial neuronal networks
\end_layout

\end_inset

Artificial neuronal networks are mathematical constructs, designed to imitate
 the signal processing capabilities of real neurons, found in nearly all
 animals.
 Like their biological counterparts, artificial neuronal networks consist
 of simpler building blocks, the neurons.
\end_layout

\begin_layout Paragraph
Neurons as basic signal processing units
\end_layout

\begin_layout Standard
The real, biological neurons are defined (according to the neuron doctrine
\begin_inset CommandInset citation
LatexCommand cite
key "BullockDouglas2005"

\end_inset

) as the smallest units whose state change may be called signal processing,
 so they are the basic signal processing units.
 They have multiple inputs (dendrites) and multiple outputs (axons)
\begin_inset CommandInset citation
LatexCommand cite
key "ByrneDafny1997"

\end_inset

.
 In most real neurons, the signal transmission and processing is facilitated
 by alternating small electric (action) potentials (along the axons) and
 chemical transmissions (at chemical synapses between axon and dendrite).
 The electric potential is transmitted along the dendrites of a neuron,
 and flows to the axon of the neuron, which can lead to the release of neurotran
smitters in the axon terminals.
 The neurotransmitters cause ion channels in the adjacent dendrites of other
 neurons to open, which changes their membrane potential.
\end_layout

\begin_layout Paragraph
Action potentials, their propagation, and chemical synapses 
\end_layout

\begin_layout Standard
The action potentials are realized by cells in the form of different ion
 concentrations inside and outside the cell.
 These ion gradients are maintained in the resting state by 
\begin_inset Formula $Na^{+}/K^{+}$
\end_inset

-ATPases that pump 3 
\begin_inset Formula $Na^{+}$
\end_inset

 ions out of and 2 
\begin_inset Formula $K^{+}$
\end_inset

 ions into the cell for every ATP molecule
\begin_inset CommandInset citation
LatexCommand cite
key "LodishZipursky2000"

\end_inset

.
 Because ions are charged, there is an electric potential between the outside
 and inside of the cell.
 (The resting potential is between -80mV and -40mV, depending on the type
 of neuron.) The electric potential becoming more positive is called depolarizati
on, and the opposite hyperpolarization.
\end_layout

\begin_layout Standard
The propagation of the action potentials along dendrites is realized by
 the opening and closing of ion channels.
 Once depolarization of an adjacent region of a neuron causes the electric
 potential between the inside and outside of a 
\begin_inset Formula $Na^{+}$
\end_inset

 ion channel to reach a critical value, the ion channel opens, causing further
 depolarization in adjacent regions of the neuron.
 This positive feedback loop continues until all 
\begin_inset Formula $Na^{+}$
\end_inset

 channels are open.
 At the peak of depolarization, 
\begin_inset Formula $K^{+}$
\end_inset

 ion channels open, causing hyperpolarization, and the potential returns
 to the resting  potential.
 This makes the action potential travel along the neuron.
 Once it has reached an axon terminal, it causes neurotransmitter release.
\end_layout

\begin_layout Standard
Neurotransmitters binding to receptors
\begin_inset Note Note
status open

\begin_layout Plain Layout
for example, AMPA and NMDA receptors
\end_layout

\end_inset

 present on the outside of the neuron's membrane cause ion channels to open,
 and the ions flow into or out of the cell to achieve equilibrium of ion
 concentration.
 The type of ion channel being opened upon binding of a neurotransmitter
 can cause either depolarization or hyperpolarization of the dendrite, depending
 on the charge of the ion, and whether the resting concentration of the
 ion is higher intracellular or extracellular.
 If a critical threshold of depolarization is reached, the 
\begin_inset Formula $Na^{+}$
\end_inset

 ion channels will open, and an action potential 
\begin_inset Quotes eld
\end_inset

spike
\begin_inset Quotes erd
\end_inset

 is generated as described above.
\end_layout

\begin_layout Paragraph
Encoding of information in action potentials
\end_layout

\begin_layout Standard
The presence of a critical threshold suggests that it is not the 
\begin_inset Quotes eld
\end_inset

analogue
\begin_inset Quotes erd
\end_inset

 electric potential, but the 
\begin_inset Quotes eld
\end_inset

digital
\begin_inset Quotes erd
\end_inset

 spike that carries the information from one neuron to the next.
 For example, the strength muscles are innervated with is encoded in the
 number of action potentials per time delivered by the muscle neuron to
 the muscle fiber.
 However, some neurons involved in perception directly transmit information
 in the fluctuations of neurotransmitter released.
 This analogue mode of transmission allows more information to be transmitted
 per time.
 Sub-threshold emission of neurotransmitter also seems to modulate subsequent
 action potentials, allowing for a mixture of analogue and digital information
 transmission 
\begin_inset CommandInset citation
LatexCommand cite
key "DebanneRama2013"

\end_inset

.
 The machinery facilitating propagation and transmission of information
 in and between biological neurons is highly simplified in artificial neurons.
\end_layout

\begin_layout Paragraph
Artificial neurons as simple models of biological neurons
\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
artificial neuron signal processing
\end_layout

\end_inset

Signal processing of a real neuron is modelled in an artificial neuron as
 a mathematical function that has multiple input variables, computes a value
 according to the function formula and its parameters and outputs its computed
 value to multiple neurons, which use it as an input variable.
 Herein, the processes of neurotransmitter release, de- and hyperpolarization,
 and propagation of the action potential are abstracted away into discrete
 time steps.
\end_layout

\begin_layout Standard
Each artificial neuron's function is evaluated once per time step.
 Often, the 
\emph on
sigmoid
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sigmoid function
\end_layout

\end_inset


\emph on
 
\emph default
function is used to describe the output of an artificial neuron, the so-called
 
\emph on
activation
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
activation of an artificial neuron
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
o_{i}=\sigma(v_{i})=\frac{1}{1+\exp(v_{i})}
\]

\end_inset

where 
\begin_inset Formula $v_{i}\in\mathbb{R}$
\end_inset

 is the accumulated input to neuron 
\begin_inset Formula $i$
\end_inset

, and 
\begin_inset Formula $o_{i}=\sigma(v_{i})\in[0;1]$
\end_inset

 is the activation of neuron 
\begin_inset Formula $i$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier eine Figure mit dem Plot von 
\begin_inset Formula $\sigma(x)$
\end_inset

 einfügen.
\end_layout

\end_inset

The effect of an incoming axon onto a neuron, that is, the different types
 of receptors that can be present on the outside of a real dendrite, and
 the effected de- or hyperpolarization of the dendrite are abstracted away
 by using real-numbered weights.
 These are parameters to the mathematical function describing the accumulation
 and modulation of the vector of outputs of artificial neurons to form the
 single input value 
\begin_inset Formula $v$
\end_inset

 of an artificial neuron connected to them.
 Usually the following formula is used to describe the computation of the
 input 
\begin_inset Formula $v_{i}$
\end_inset

 of neuron 
\begin_inset Formula $i$
\end_inset

 from the outputs of its connected neurons 
\begin_inset Formula $\mathbf{c_{i}}$
\end_inset

:
\begin_inset Formula 
\[
v_{i}=-b_{i}-\sum_{j\in\mathbf{c_{i}}}o_{j}w_{ij}
\]

\end_inset

where 
\begin_inset Formula $b_{i}\in\mbox{\mathbb{R}}$
\end_inset

 is the so-called 
\emph on
bias
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
bias of an artificial neuron
\end_layout

\end_inset


\emph on
 
\emph default
of neuron 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $\mathbf{c_{i}}$
\end_inset

 is the vector of indices of its in-going connected neurons, 
\begin_inset Formula $o_{j}\in\mathbb{R}$
\end_inset

 is the activation of the connected neuron 
\begin_inset Formula $j$
\end_inset

, and 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

 is the weight of the connection going out of neuron 
\begin_inset Formula $j$
\end_inset

 and into neuron 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Paragraph
Learning
\end_layout

\begin_layout Standard
Nervous systems do not only process signals, but they also learn, that means
 that they adapt their signal processing over time.
 One reason for this is an organism's need for a change in behavior, as
 response to a changing environment.
\end_layout

\begin_layout Standard
In biological neuronal systems, this is possible by altering or exchanging
 the receptors on the surface of dendrites, or by connecting the axon terminals
 of a neuron to different neurons.
 Strengthening of the synaptic link (that occurs within minutes and remains
 after hours and days in mammals) is called long-term potentiation (LTP),
 while its weakening is called long-term depression (LTD).
 LTP is induced by associativity of connected neurons, that is, the 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Hier weitermachen mit Erklären und Zitieren der 
\begin_inset Quotes eld
\end_inset

Hebbian learning rule
\begin_inset Quotes erd
\end_inset

: die synapse zwischen zwei benachbarten neuronen wird gestärkt, wenn beide
 neuronen gleichzeitig aktiv sind.? Ein Einblick in molekulare Abläufe wäre
 wünschenswert.
\end_layout

\end_inset

.The cellular mechanisms controlling these processes, and their interplay
 in larger neuron ensembles are a field of active research
\begin_inset CommandInset citation
LatexCommand cite
key "BermudezFederico2007"

\end_inset

.
\end_layout

\begin_layout Standard
In artificial neuronal networks, and machine learning in general, there
 are two major types of learning: supervised and unsupervised learning.
 In supervised learning, for every input pattern in the data set there is
 defined an output pattern that the learner should compute from the input
 pattern.
 This means the algorithm should infer a function that maps from the space
 of input patterns to the space of output patterns.
 In unsupervised learning, there 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "zusammenfassung"
options "plain"

\end_inset


\end_layout

\end_body
\end_document

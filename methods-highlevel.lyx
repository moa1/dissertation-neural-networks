#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This is supposed to be a high-level description of neuronal networks, to
 be used in the thesis before the mathematical description of neuronal networks
 (currently 
\begin_inset Quotes eld
\end_inset

methods.lyx
\begin_inset Quotes erd
\end_inset

).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Insert sections and paragraphs into the following text and title them
 appropriately.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Artificial neuronal networks
\end_layout

\end_inset

Artificial neuronal networks are mathematical constructs, designed to imitate
 the signal processing capabilities of real neurons, found in nearly all
 animals.
 Like their biological counterparts, artificial neuronal networks consist
 of simpler building blocks, the neurons.
\end_layout

\begin_layout Paragraph
Neurons as basic signal processing units
\end_layout

\begin_layout Standard
The real, biological neurons are defined (according to the neuron doctrine
\begin_inset CommandInset citation
LatexCommand cite
key "BullockDouglas2005"

\end_inset

) as the smallest units whose state change may be called signal processing,
 so they are the basic signal processing units.
 They have multiple inputs (dendrites) and multiple outputs (axons)
\begin_inset CommandInset citation
LatexCommand cite
key "ByrneDafny1997"

\end_inset

.
 In most real neurons, the signal transmission and processing is facilitated
 by alternating small electric (action) potentials (along the axons) and
 chemical transmissions (at chemical synapses between axon and dendrite).
 The electric potential is transmitted along the dendrites of a neuron,
 and flows to the axon of the neuron, which can lead to the release of neurotran
smitters in the axon terminals.
 The neurotransmitters cause ion channels in the adjacent dendrites of other
 neurons to open, which changes their membrane potential.
\end_layout

\begin_layout Paragraph
Action potentials, their propagation, and chemical synapses 
\end_layout

\begin_layout Standard
The action potentials are realized by cells in the form of different ion
 concentrations inside and outside the cell.
 These ion gradients are maintained in the resting state by 
\begin_inset Formula $Na^{+}/K^{+}$
\end_inset

-ATPases that pump 3 
\begin_inset Formula $Na^{+}$
\end_inset

 ions out of and 2 
\begin_inset Formula $K^{+}$
\end_inset

 ions into the cell for every ATP molecule
\begin_inset CommandInset citation
LatexCommand cite
key "LodishZipursky2000"

\end_inset

.
 Because ions are charged, there is an electric potential between the outside
 and inside of the cell.
 (The resting potential is between -80mV and -40mV, depending on the type
 of neuron.) The electric potential becoming more positive is called depolarizati
on, and the opposite hyperpolarization.
\end_layout

\begin_layout Standard
The propagation of the action potentials along dendrites is realized by
 the opening and closing of ion channels.
 Once depolarization of an adjacent region of a neuron causes the electric
 potential between the inside and outside of a 
\begin_inset Formula $Na^{+}$
\end_inset

 ion channel to reach a critical value, the ion channel opens, causing further
 depolarization in adjacent regions of the neuron.
 This positive feedback loop continues until all 
\begin_inset Formula $Na^{+}$
\end_inset

 channels are open.
 At the peak of depolarization, 
\begin_inset Formula $K^{+}$
\end_inset

 ion channels open, causing hyperpolarization, and the potential returns
 to the resting  potential.
 This makes the action potential travel along the neuron.
 Once it has reached an axon terminal, it causes neurotransmitter release.
\end_layout

\begin_layout Standard
Neurotransmitters binding to receptors
\begin_inset Note Note
status open

\begin_layout Plain Layout
for example, AMPA and NMDA receptors
\end_layout

\end_inset

 present on the outside of the neuron's membrane cause ion channels to open,
 and the ions flow into or out of the cell to achieve equilibrium of ion
 concentration.
 The type of ion channel being opened upon binding of a neurotransmitter
 can cause either depolarization or hyperpolarization of the dendrite, depending
 on the charge of the ion, and whether the resting concentration of the
 ion is higher intracellular or extracellular.
 If a critical threshold of depolarization is reached, the 
\begin_inset Formula $Na^{+}$
\end_inset

 ion channels will open, and an action potential 
\begin_inset Quotes eld
\end_inset

spike
\begin_inset Quotes erd
\end_inset

 is generated as described above.
\end_layout

\begin_layout Paragraph
Encoding of information in action potentials
\end_layout

\begin_layout Standard
The presence of a critical threshold suggests that it is not the 
\begin_inset Quotes eld
\end_inset

analogue
\begin_inset Quotes erd
\end_inset

 electric potential, but the 
\begin_inset Quotes eld
\end_inset

digital
\begin_inset Quotes erd
\end_inset

 spike that carries the information from one neuron to the next.
 For example, the strength muscles are innervated with, is encoded in the
 number of action potentials per time delivered by the muscle neuron to
 the muscle fiber.
 However, some neurons involved in perception directly transmit information
 in the fluctuations of neurotransmitter released.
 This analogue mode of transmission allows more information to be transmitted
 per time.
 Sub-threshold emission of neurotransmitter also seems to modulate subsequent
 action potentials, allowing for a mixture of analogue and digital information
 transmission 
\begin_inset CommandInset citation
LatexCommand cite
key "DebanneRama2013"

\end_inset

.
 
\end_layout

\begin_layout Standard
Examples for neuronal networks that have been partly decoded are the eye
 (visual system) and the nose (olfactory system).
\end_layout

\begin_layout Paragraph
The eye as an example of a biological neuronal network
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: how does light on rhodopsin lead to glutamate release inhibition?
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: cite Kolb 2003 here.
\end_layout

\end_inset

In the eye, specialized cells, the rods and cones detect light
\begin_inset CommandInset citation
LatexCommand cite
key "Kolb2003"

\end_inset

.
 In these cells, light elicits a transformation of cis-rhodopsin to trans-rhodop
sin, which receives a G protein binding site.
 Transducin binds to the activated rhodopsin, and GDP acquires a phosphate
 group to form GTP.
 
\end_layout

\begin_layout Standard
Rods are more sensitive to dim light, while cones only react to bright light
 but can see colors.
 Both release the neurotransmitter glutamate continuously into the synaptic
 cleft, but when hit by light, suspend this emission for the duration of
 the light.
 The area that must be lit for a cell to emit neurotransmitter is called
 the 
\emph on
receptive field
\emph default
, and is just as large as the top of the photoreceptor for rods and cones.
 The glutamate binds to receptors on the cell outside of bipolar cells,
 and, depending on the type of bipolar cell, cause either an action potential
 to be generated when the photoreceptor is lit and the surrounding area
 is dark (
\emph on
ON 
\emph default
bipolar cell), or when the photoreceptor is dark against a bright background
 (
\emph on
OFF 
\emph default
bipolar cell).
 Horizontal cells integrate signals from surrounding cone cells, and feed
 their signal back to the cones, or directly to bipolar cells.
 This enhances contrast.
 The signal from several bipolar cells is fed into a ganglion cell, which
 therefore has a larger receptive field than its connected bipolar cells.
 ON bipolar cells only excite ON ganglion cells, and OFF bipolar cells excite
 only OFF ganglion cells.
 Finally, there are more than a million nerve fibers from ganglion cells
 to the visual cortex of the brain.
 Altogether, the basic cell types are, depending on the species, 1 to 4
 horizontal types of cell, 11 types of bipolar cells, 22 to 30 types of
 amacrine cells, and 20 types of ganglion cells.
 Among their known functions are brightness-dependent size regulation of
 the receptive field of amacrine cells, integration of a large number of
 rods to provide sight in little light, and an additional photoreceptor
 distinct from rods and cones.
\end_layout

\begin_layout Standard
The machinery facilitating propagation and transmission of information in
 and between biological neurons is highly simplified in artificial neurons.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Hier ein Bild einfügen, das links ein biologisches neuronales Netz zeigt,
 und rechts ein artifizielles.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Odor sensing in the olfactory system
\end_layout

\begin_layout Standard
The olfactory system of mammals and insects contains neurons that detect
 odor molecules called glomeruli.
 In humans, there are about 500 different types of glomeruli [1], but it
 is hypothesized that a human can perceive around 10,000 different odors.
 A single odorant consisting of a mixture of different odor molecules can
 excite one or more glomeruli.
 Each glomerulus is connected to one or more neurons in the olfactory cortex
 in the mammalian brain, or to one or more Kenyon Cells in insects.
 For example, in 
\emph on
Drosophila
\emph default
, about 9% of the glomeruli are excited by an odorant, and the connectivity
 between glomeruli and Kenyon Cells is between 6.5% and 12.5%.
 However, these numbers vary greatly between species.
 It is hypothesized that species with sparse connectivity have better odor
 perception of complex odor mixtures.
 On the other hand, species with dense connectivity should have better performan
ce in detecting low concentrations of simple odor mixtures.
 (TODO: confirm the last sentence.)
\end_layout

\begin_layout Paragraph
Artificial neurons as simple models of biological neurons
\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
artificial neuron signal processing
\end_layout

\end_inset

Signal processing of a real neuron is modelled in an artificial neuron as
 a mathematical function that has multiple input variables, computes a value
 according to the function formula and its parameters and outputs its computed
 value to multiple neurons, which use it as an input variable.
 Herein, the processes of neurotransmitter release, de- and hyperpolarization,
 and propagation of the action potential are abstracted away into discrete
 time steps.
\end_layout

\begin_layout Standard
Each artificial neuron's function is evaluated once per time step.
 Often, the 
\emph on
sigmoid
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
sigmoid function
\end_layout

\end_inset


\emph on
 
\emph default
function is used to describe the output of an artificial neuron, the so-called
 
\emph on
activation
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
activation of an artificial neuron
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
o_{i}=\sigma(v_{i})=\frac{1}{1+\exp(v_{i})}
\]

\end_inset

where 
\begin_inset Formula $v_{i}\in\mathbb{R}$
\end_inset

 is the accumulated input to neuron 
\begin_inset Formula $i$
\end_inset

, and 
\begin_inset Formula $o_{i}=\sigma(v_{i})\in[0;1]$
\end_inset

 is the activation of neuron 
\begin_inset Formula $i$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Hier eine Figure mit dem Plot von 
\begin_inset Formula $\sigma(x)$
\end_inset

 einfügen.
\end_layout

\end_inset

The effect of an incoming axon onto a neuron, that is, the different types
 of receptors that can be present on the outside of a real dendrite, and
 the effected de- or hyperpolarization of the dendrite are abstracted away
 by using real-numbered weights.
 These are parameters to the mathematical function describing the accumulation
 and modulation of the vector of outputs of artificial neurons to form the
 single input value 
\begin_inset Formula $v$
\end_inset

 of an artificial neuron connected to them.
 Usually the following formula is used to describe the computation of the
 input 
\begin_inset Formula $v_{i}$
\end_inset

 of neuron 
\begin_inset Formula $i$
\end_inset

 from the outputs of its connected neurons 
\begin_inset Formula $\mathbf{c_{i}}$
\end_inset

:
\begin_inset Formula 
\[
v_{i}=-b_{i}-\sum_{j\in\mathbf{c_{i}}}o_{j}w_{ij}
\]

\end_inset

where 
\begin_inset Formula $b_{i}\in\mbox{\mathbb{R}}$
\end_inset

 is the so-called 
\emph on
bias
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
bias of an artificial neuron
\end_layout

\end_inset


\emph on
 
\emph default
of neuron 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $\mathbf{c_{i}}$
\end_inset

 is the vector of indices of its in-going connected neurons, 
\begin_inset Formula $o_{j}\in\mathbb{R}$
\end_inset

 is the activation of the connected neuron 
\begin_inset Formula $j$
\end_inset

, and 
\begin_inset Formula $w_{ij}\in\mathbb{R}$
\end_inset

 is the weight of the connection going out of neuron 
\begin_inset Formula $j$
\end_inset

 and into neuron 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Paragraph
Learning
\end_layout

\begin_layout Standard
Nervous systems do not only process signals, but they also learn, that means
 that they adapt their signal processing over time.
 One reason for this is an organism's need for a change in behavior, as
 response to a changing environment.
\end_layout

\begin_layout Standard
In biological neuronal systems, this is possible by altering existing synapses
 (for example by exchanging the receptors on the surface of dendrites),
 or by connecting the axon terminals of a neuron to different neurons.
 There are several known cellular mechanisms for that, among them LTP, LTD,
 PTP
\begin_inset Note Note
status open

\begin_layout Plain Layout
post-tetanic potentiation
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "BermudezFederico2007"

\end_inset

.
 Strengthening of the synaptic link (that occurs within minutes and remains
 after hours and up to weeks in the hippocampus of mammals) is called long-term
 potentiation (LTP), while its weakening is called long-term depression
 (LTD).
 LTP is induced by associativity of connected neurons, that means, when
 a neuron contributes to the depolarization in a directly connected neuron,
 the efficiency of that connection will be strengthened
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe (after reading) cite 
\begin_inset CommandInset citation
LatexCommand cite
key "GalanGalizia2006"

\end_inset


\end_layout

\end_inset

.
 The molecular mechanisms responsible for this phenomenon are not yet completely
 understood.
 It is known that they differ between brain regions, and also types of synapses
 in the same brain region.
 This 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Hier weitermachen mit Erklären und Zitieren der 
\begin_inset Quotes eld
\end_inset

Hebbian learning rule
\begin_inset Quotes erd
\end_inset

: die synapse zwischen zwei benachbarten neuronen wird gestärkt, wenn beide
 neuronen gleichzeitig aktiv sind.? Ein Einblick in molekulare Abläufe wäre
 wünschenswert.
\end_layout

\end_inset

.The cellular mechanisms controlling these processes, and their interplay
 in larger neuron ensembles are a field of active research
\begin_inset CommandInset citation
LatexCommand cite
key "BermudezFederico2007"

\end_inset

.
\end_layout

\begin_layout Standard
Of note in the Hebbian learning rule is that it is local
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
locality of Hebbian learning rule
\end_layout

\end_inset

, which means that changes at a synapse only depend on the directly connected
 neurons, but not on other distantly-connected neurons.
\end_layout

\begin_layout Paragraph
Supervised and Unsupervised Machine Learning
\end_layout

\begin_layout Standard
In artificial neuronal networks, the goals of learning are defined by humans.
In machine learning in general, there are two major types of learning: supervised
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
supervised machine learning
\end_layout

\end_inset

 and unsupervised learning
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
unsupervised machine learning
\end_layout

\end_inset

.
 Both methods process data sets that are in matrix form: for example, in
 expression data, the rows usually denote different genes or transcripts,
 and each column represents an independently measured sample.
 (Note that in the general machine learning literature, usually the data
 matrix is transposed: the columns denote the features, and the rows the
 samples.) Samples usually are tissue, blood samples, or cell line, and differ
 in their biological background (e.g.
 cell type, gene knock-out or knock-in, cell cycle phase) or treatment (e.g.
 drugs applied).
\end_layout

\begin_layout Standard
In supervised learning, for every input pattern in the data set there is
 defined an output pattern that the learner should compute from the input
 pattern.
 Herein, both input and output pattern can be one- or multi-dimensional
 vectors.
 The goal of supervised learning is to infer a function that maps from the
 space of input patterns to the space of output patterns.
 The output patterns are also called 
\begin_inset Index idx
status open

\begin_layout Plain Layout
labels
\end_layout

\end_inset


\emph on
labels
\emph default
, and one says that  
\begin_inset Quotes eld
\end_inset

the input data is labeled
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
In unsupervised learning, there are only input data and the goal is to find
 interesting underlying patterns in them.
 Unsupervised learning algorithms usually compute a mapping from input space
 to output space, where both spaces can differ in their dimensionality.
 This can be used for example for dimensionality reduction, or data re-represent
ation and abstraction.
 Examples of supervised learning algorithms are (linear or logistic) regression,
 k-Nearest-Neighbor (k-NN), support vector machines (SVMs), backpropagation
 neuronal networks, Deep Belief Networks (DBNs).
 Examples for unsupervised learning algorithms are clustering, self-organising
 maps (SOMs), principal component analysis (PCA), Restricted Boltzmann Machine
 (RBM).
\end_layout

\begin_layout Paragraph
Semi-supervised Machine Learning
\end_layout

\begin_layout Standard
An intermediate form of these two machine learning modes is semi-supervised
 learning.
 In contrast to supervised machine learning, which has for every input pattern
 a corresponding target output pattern, semi-supervised learning does not
 need a target output pattern for every input pattern.
 However, in contrast to fully unsupervised machine learning, it does need
 some labeled input data sets.
 Supervised machine learning algorithms often try to find underlying structure
 in all input data sets and then use the known labels to assign probable
 labels to the found structure (or just the unlabeled input samples).
 There are two types of semi-supervised learning: transductive and inductive
 semi-supervised learning.
 The goal of transductive semi-supervised learning is to predict the class
 label of a pre-specified list of unlabeled input patterns, while the goal
 of inductive semi-supervised learning is to find a universal rule mapping
 from the space of input patterns to class labels, which could be applied
 to classify unknown, future input patterns.
 In case unknown, future input patterns are to be classified using transductive
 semi-supervised machine learning, the whole model may have to be re-evaluated.
\end_layout

\begin_layout Paragraph
Properties of semi-supervised learning
\end_layout

\begin_layout Standard
An advantage of semi-supervised learning over supervised learning is that
 it does not need labels for all input patterns, because labels are often
 time-consuming or costly to acquire.
 For example, the Gene Expression Omnibus data base (GEO) 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: citation
\end_layout

\end_inset

 contains 41,379 expression data sets that were uploaded between Jan 1st,
 2000, and August 31st, 2013.
 Many of these are potentially usable as unlabeled data sets.
 However, machine learning algorithms usually require samples to be 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
independetly and identically distributed
\end_layout

\end_inset

independetly and identically distributed (iid
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
iid
\end_layout

\end_inset

).
 In an ideal world, GEO samples could be assumed to be identically distributed
 within a data set.
 However, even within the same GEO data set there often is systematic variation
 between samples, called the 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
batch-effect
\end_layout

\end_inset


\emph on
batch-effect
\emph default
.
 This means if one wants to use samples from different GEO data sets simultaneou
sly, one either has to correct for a possible batch-effect manually, or
 use an algorithm that has some built-in mechanism to make such a correction.
\end_layout

\begin_layout Standard
One machine learning algorithm with such a built-in mechanism is
\emph on
 deep learning
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
deep learning
\end_layout

\end_inset

, as employed in 
\emph on
Deep Belief Networks (DBNs)
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
Deep Belief Network (DBN)
\end_layout

\end_inset

 that have been described by 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

.
 This machine-learning algorithm (which can be used unsupervisedly, supervisedly, as well as semi-supervisedly) was shown to be able to learn from images
 of objects or faces, where the objects or faces are in different lighting
 conditions or are viewed from different angles (
\begin_inset CommandInset citation
LatexCommand cite
key "HintonSalakhutdinov2006"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: cite one of the YouTube video learning papers by Salakhutdinov and
 Srivastava (iirc).
\end_layout

\end_inset

).
 Such results seem to imply that DBNs are able to abstract the images, which
 are given as vectors of pixels, into encodings of relevant features and
 compute a classifier on these abstract features.
\end_layout

\begin_layout Standard
Before we motivate using DBNs on expression data, we first introduce deep
 learning.
 Deep learning is a term used in conjunction with artificial neuronal networks.
 Those 
\begin_inset Quotes eld
\end_inset

deep artificial
\begin_inset space ~
\end_inset

neuronal networks
\begin_inset Quotes erd
\end_inset

 usually have several hidden layers.
\end_layout

\begin_layout Paragraph
Back-propagation
\end_layout

\begin_layout Standard
Before deep learning, artificial neuronal networks were usually trained
 using only back-propagation without regularizations.
 Back-propagation learns from labeled samples the weights and biases of an artificial neuronal network.
 (TODO: cite Rumelhart, D.
 E., Hinton, G.
 E., and Williams, R.
 J.
 (1988).
 Learning representations by back-propagating errors.
 Cognitive modeling, 5.) This supervised training procedure sets input activation
s in the input layer according to the input pattern to be learned, computes
 the activations of the hidden layers and the output layer, and then computes
 the error at each node in the output layer by subtracting the computed
 activation from the target activation as given by the desired output pattern
 for the particular input pattern that was fed into the network.
 The error for each node is then back-propagated in reverse input direction
 to the hidden layers and finally to the input layer.
 The purpose of this is to be able to adjust the weights of the artificial
 neuronal network such that when the current input pattern pair is presented
 to the network, its computed output pattern gets closer and closer to the
 desired output pattern.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
(TODO: hier vielleicht eine figure mit dem Daten-strom von input layer über
 hidden layers in output layers, und dann backprop-daten-strom in reverse
 direction)
\end_layout

\end_inset

However, back-propagation was not able to compute networks with more than
 very few hidden layers.
\end_layout

\begin_layout Paragraph
Autoassociator
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: vielleicht sollte die Beschreibung eines autoassociators hier kürzer
 sein, und diese lange Beschreibung stattdessen in den Methods-Teil, da
 sonst die Beschreibung von Deep learning danach sehr kurz wirkt.
\end_layout

\end_inset

An algorithm to train an artificial neuronal networks with more than one
 hidden layer was the autoassociator (TODO: add index entry).
 The algorithm is unsupervised and iteratively constructs deeper and deeper
 networks.
 Its essential idea is the construction of an encoder network and its anti-symme
tric counterpart, the decoder network.
 The encoder starts in the first iteration as a network that consists of
 the input layer and one hidden layer on top.
 The decoder network consists of the very same hidden layer and the output
 layer on top, which must have the same dimensions as the input layer.
\end_layout

\begin_layout Standard
When training this first iteration of the network, standard back-propagation
 is used to adapt the weights between input layer and hidden layer and between
 hidden layer and output layer so that the network reconstructs its input
 patterns as the output patterns (i.e.
 the values of the nodes in the output layer).
 The input patterns are taken from the training data set.
 Usually the number of nodes in the hidden layer is chosen to be smaller
 than the number of nodes in the input (and output) layer.
 In this way the autoassociator learns to reconstruct its input from a compresse
d representation.
 (TODO: hier ein Bild eines autoassociator networks).
 Once back-propagation does not improve the reconstruction error on the
 test set anymore, the second iteration starts.
\end_layout

\begin_layout Standard
In the second iteration, the encoder network is enlarged by an additional
 hidden layer put on top of its existing 1st hidden layer.
 The weights between the 1st and 2nd (new) hidden layer are initialized
 randomly.
 The biases of the nodes in the 2nd (new) hidden layer are also initialized
 (randomly or constantly zero).
 The decoder network is constructed on top of the new hidden 2nd layer by
 inserting weights initialized randomly, copying the 1st hidden layer to
 form the 3rd hidden layer, copying the weights between hidden layer 1 (from
 the first iteration) to the output layer, and copying the output layer
 (from the first iteration).
 Then back-propagation is used to train the autoassociator consisting of
 the new encoder network with the new decoder network on top.
 This establishes values for the values of the initialized weights and biases.
\end_layout

\begin_layout Standard
Because the network size increases iteratively from only 3 layers, to 5
 layers, to 7 layers, and so on, and only 2 weight layers are initialized
 randomly in each iteration, back-propagation is often able to find parameters
 for a network that is not stuck in a poor local optimum.
\end_layout

\begin_layout Paragraph
Autoassociator with classifier on top
\end_layout

\begin_layout Standard
The autoassociator can be used in a supervised fashion by first training
 its encoder and decoder networks up to sufficient depth, and then removing
 the decoder network and replacing it by a single output layer that has
 the dimension of the training output patterns.
 The weights between the last hidden layer of the encoder and the new output
 layer as well as the biases of the new output layer are initialized randomly
 (or to constantly zero), and trained using back-propagation.
\end_layout

\begin_layout Paragraph
Deep learning
\end_layout

\begin_layout Standard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep learning
\end_layout

\end_inset

Deep learning, as demonstrated by 
\begin_inset CommandInset citation
LatexCommand cite
key "HintonTeh2006"

\end_inset

 also overcame the limitation of only a few hidden layers.
 It is separated into a pre-training phase and a fine-tuning phase.
 The pre-training phase is unsupervised.
 In this phase, hidden layers are iteratively added on top of the input
 layer of the network, and the weights between layers are initialized and
 pre-trained like 
\emph on

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Restricted Boltzmann Machine
\end_layout

\end_inset

Restricted Boltzmann Machines
\emph default
 (
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
RBMs
\end_layout

\end_inset

RBMs).
 Without this step, fine-tuning a multiple-hidden-layer network usually
 gets stuck in a local error minimum.
 The pre-training algorithm is called
\emph on
 contrastive divergence
\emph default

\begin_inset Index idx
status open

\begin_layout Plain Layout
contrastive divergence
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
(As 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: cite Bengio and Delalleau, 2007, Justifying and Generalizing Contrastive
 Divergence.
 Wahrscheinlich sollte der ganze Satz hier in die Methods, nicht die Introductio
n.
\end_layout

\end_inset

showed, training using contrastive divergence (CD-1) and training using
 reconstruction error as error metric in an autoassociator are linked.
 They showed that the weight learning gradient used with the reconstruction
 error is a more biased approximator of the log-likelihood than the gradient
 computed by CD-1.)
\end_layout

\begin_layout Standard
In unsupervised pre-training, unlabeled training samples can be used in
 learning the weights between layers.
 While the pre-training phase is unsupervised, the fine-tuning phase can
 be unsupervised as well as supervised.
 After training, the multiple-hidden-layer-network forms a generative artificial
 neuronal network called a 
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Deep Belief Network
\end_layout

\end_inset


\emph on
Deep Belief Network 
\emph default
(
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
DBN
\end_layout

\end_inset

DBN).
\end_layout

\begin_layout Paragraph
Regularizations for Training Artificial Neuronal Networks
\end_layout

\begin_layout Standard
Several regularization methods have been developed over the years. They have in common that they artificially constrain the search space of weights and biases in order to let the model find better error minima or to prevent overfitting, that means that the neuronal network does less 
\begin_inset Quotes eld
\end_inset
learning by heart
\begin_inset Quotes erd
\end_inset
 and make its predictions more general.
\end_layout

\begin_layout Paragraph
L1 and L2 Weight Decay Regularization
\end_layout

\begin_layout Standard
L1 weight decay decreases the absolute value of each weight in each training iteration, in order to prevent large weights. This can be necessary because for some training samples, some weights tend to escape, i.e. become larger and larger in value. L1 weight decay multiplies all weights $w_ij$ with $c$, where $c$ is a constant meta-parameter between 0 and 1. (TODO: check this)
\end_layout

\begin_layout Paragraph
L2 weight decay is similar to L1 weight decay but penalizes large weights even more. It assigns the weights $w_ij := w_ij - c * \sqrt(w_ij*w_ij)$, where $c$ is a constant meta-parameter between 0 and 1. (TODO: check this)
\end_layout

\begin_layout Paragraph
Dropout Regularization
\end_layout

\begin_layout Paragraph
Dropout is a regularization method to make the nodes in the hidden layers, which can be seen as feature detectors, more independent from each other (TODO: cite Dropout: A Simple Way to Prevent Neural Networks from Overfitting). This is enforced by omitting (or 
\begin_inset Quotes eld
\end_inset
dropping
\begin_inset Quotes erd
\end_inset
) in a layer a random subset of nodes from computing the layers after that layer. Its meta-parameter is a single value
\begin_inset Formula $d$
\end_inset
 between 0 and 1 for each layer, the fraction of nodes whose input and output weights should temporarily be set to 0 during one learning iteration. This makes the incoming activations in the layer after the current layer smaller by the factor
\begin_inset Formula $d$
\end_inset
 on average. During prediction (or testing) time, no node is dropped out, and the value
 \begin_inset Formula $d$
\end_inset
 is set to 1, so that all nodes are used in calculating the output. To compensate for the larger average activation input in the next layer, all incoming weights are multiplied by the factor
 \begin_inset Formula $d$
\end_inset
.
The theory behind dropout is that for a network with
\begin_inset Formula $d$
\end_inset
 nodes, there are 
\begin_inset Formula $2^{n}$
\end_inset
 possible ways to drop out nodes and a network trained with dropout implicitly averages its output over all these 
\begin_inset Formula $2^{n}$
\end_inset
 networks with shared weights. Dropping out a random fraction of nodes prevents single nodes from co-adapting to the specific working of a large number of other nodes.
\end_layout

\begin_layout Paragraph
Regularization: Sparsity
\end_layout

\begin_layout Standard
Sparsity regularization is a method to make only a small fraction of nodes output an activation different from zero. This helps to generalize the output, and also makes neuronal networks more interpretable. It has at least one meta-parameter, $s$, the fraction of nodes that should be active within a neuronal network layer.
TODO: HIER WEITERMACHEN, INDEM ICH NACH PAPERS SUCHE UND ZITIERE, DIE SPARSITY BESCHREIBEN.
\end_layout

\begin_layout Paragraph
Early stopping
\end_layout

\begin_layout Standard
Early stopping is not really a regularization method, but nevertheless a method to prevent overfitting in supervised training of an artificial neuronal network. It is used by splitting the training data set into a real training data set and a validation data set, and using only the real training data set for adapting the weights and biases during learning. After each learning iteration, the validation data set is used to compute the output error of the current network. After a defined number of training iterations, the neuronal network that had the lowest output error on the validation data set is used as the final network to be used in making predictions.
\end_layout

\begin_layout Standard
This prevents the training procedure from overfitting to sampling error present in the training data set.
\end_layout

\begin_layout Paragraph
Motivations for using Deep Belief Networks on transcriptomic data
\end_layout

\begin_layout Standard
The motivations for using Deep Belief Networks on transcriptomic data come
 from those networks' successes when used on image data.
\end_layout

\begin_layout Paragraph
Highly correlated inputs
\end_layout

\begin_layout Standard
Both underlying distributions often have many correlated dimensions.
 For images, adjacent pixels often display the same object and have therefore
 correlated values.
 In face recognition for example, the faces are usually scaled and translated
 so that the centers of both eyes and mouth are aligned in different faces.
 There will be highly correlated pixels for areas of the image where the
 cheeks and lips usually are.
 If you use the pixels of the whole image as input to the neural network,
 the corresponding input nodes will be highly correlated as well.
\end_layout

\begin_layout Standard
For transcriptomic data, the correlations can be due to many genes being
 regulated by the same transcription factor.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: add some statistics of correlations, e.g.
 in the breast_cancer data set.
 for example, find a group of genes that are all correlated in all samples
 with a correlation coefficient larger than 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Plain Layout
TODO: add some citations for reasons of high correlation in expression data.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Deep Belief Networks find correlated nodes
\end_layout

\begin_layout Standard
Deep Belief Networks can group correlated input nodes by increasing their
 weights to a single hidden node, and decreasing the weights to all other
 hidden nodes.
 This is a form of abstraction (and dimensionality reduction), since in
 this way, the many correlated input nodes are grouped into one hidden node.
 The hidden node will only be active if many of its highly-weighted input
 nodes are active and only few of its negatively-weighted input nodes are
 active.
 Repeated application of this principle of abstraction allows the Deep Belief
 Network to form more and more abstract representations of its input.
\end_layout

\begin_layout Standard
In face recognition for example, an abstract representation might have a
 single value for the size of the lips.
 In transcriptomic data, a single node in an abstract representation might
 encode the activity of a gene module.
\end_layout

\begin_layout Paragraph
Previous Work: Gene Expression inference with deep learning
\end_layout

\begin_layout Standard
Very recently, 
\begin_inset Note Note
status open

\begin_layout Plain Layout
(TODO: insert citation of Yifei Chen, Xiaohui Xie, (Gene expression inference
 with deep learning) here)
\end_layout

\end_inset

 AUTHORS_YEAR published work on compressing expression data into fewer dimension
s on a large scale using deep learning.
 Input data were all genome-wide expression data from the GEO database of
 Affymetrix microarrays, which were partitioned into training, validation,
 and testing data sets.
 For each sample, the same subset of 943 "landmark" genes was chosen and
 all the other genes were predicted from the landmark genes.
\end_layout

\begin_layout Standard
Their artificial neuronal network architecture had between 1 and 3 hidden
 layers with either 3,000, 6,000, or 9,000 nodes.
 It had 943 input expression values (one for each landmark gene), and a
 total of 9,520 output expression values (one for each gene to be predicted).
\end_layout

\begin_layout Standard
In addition to the (non-linear) neuronal network, they tried linear regression
 with no regularization, l1-, and l2-regularization (to keep the weights
 between input layer and output layer small).
\end_layout

\begin_layout Standard
They also tried k-Nearest Neighbor (TODO: add index entry), where, during
 training, they determined the $k$ landmark genes with expression value
 closest to each target gene $i$ (let's call this set of genes $knn_i$)
 in the training data set, and during testing, they predicted the expression
 value of the target gene $i$ as the average of the gene's $knn_i$ expression
 values in the testing data set.
 The optimal $k$ (number of genes to average over) was chosen based on a
 validation data set.
\end_layout

\begin_layout Standard
The input values were quantile normalized expression values between 4 and
 15.
 The models were ranked according to the average prediction errors over
 all 9,520 target genes.
\end_layout

\begin_layout Standard
 k-Nearest Neighbor performed worst, with an average prediction error of
 0.5866.
 The three linear regression models performed about equally well, with average
 prediction errors of either 0.3784 or 0.3782, which means that regularization
 did not improve linear regression.
 The neuronal network-based average prediction errors were between 0.3421
 and 0.3204, with the network having 3 hidden layers of size 9000, and 10%
 dropout rate performing best.
 (TODO: have to explain dropout rate, or better all regularization methods
 before this in the introduction, maybe at the end of the paragraph "Artificial
 neurons as simple models of biological neurons") Because the input expression
 values were between 4 and 15, an average prediction error of 0.3204 implies
 an average error of about 3% on the GEO dataset.
\end_layout

\begin_layout Standard
In another dataset, AUTHORS_YEAR used the GEO dataset for training, the
 1000 Genomes data for validation (TODO: cite Lappalainen, T., Sammeth, M.,
 FriedlÂ¨ander, M.
 R., ACt Hoen, P., Monlong, J., Rivas, M.
 A., Gonz`alez-Porta, M., Kurbatova, N., Griebel, T., Ferreira, P.
 G., et al.
 (2013).
 Transcriptome and genome sequencing uncovers functional variation in humans.
 Nature, 501(7468), 506-511), and GTEx data for testing (TODO: cite Ardlie,
 K.
 G., Deluca, D.
 S., Segr`e, A.
 V., Sullivan, T.
 J., Young, T.
 R., Gelfand, E.
 T., Trowbridge, C.
 A., Maller, J.
 B., Tukiainen, T., Lek, M., et al.
 (2015).
 The genotype-tissue expression (gtex) pilot analysis: Multitissue gene
 regulation in humans.
 Science, 348(6235), 648-660).
\end_layout

\begin_layout Standard
Learning in this data set, where multiple input sources were combined, is
 therefore prone to the Batch effect.
 Nevertheless, the performance ranking of the methods is the same.
 KNN scored worst, with an average prediction error of 0.6520.
 Linear regression with L1-regularization had an average prediction error
 of 0.5667.
 Linear regression without regularization and L2-regularization had an average
 prediction error of 0.4702.
 The artificial neuronal networks all scored consistently better than KNN
 and linear regression, with the artificial neuronal network with 2 hidden
 layers of size 9000, and 25% dropout rate having the lowest prediction
 error of 0.4393 (which is equivalent to a relative error of 4%).
 On the validation data set of this data set, the average prediction error
 was 0.7467, which is a relative error of 6.8%.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
(TODO: in the Discussion, review the observation of ./Bioinformatics-2016-Chen-bi
oinformatics_btw074-supplementary.pdf, Figure S3, that there seem to be hub
 genes, that have outgoing connections to many output layer genes, with
 many of the hub genes having either positive or negative outgoing weights,
 but not both.)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "zusammenfassung"
options "plain"

\end_inset


\end_layout

\end_body
\end_document

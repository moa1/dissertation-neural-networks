#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Itemize
Das was meine Arbeit von anderen abhebt (und was meines Wissens nach neu
 ist), ist, dass ich direkt auf Expressionsdaten neuronale Netze trainiere,
 um eine Eigenschaft des Patienten vorherzusagen.
 Zuvor haben andere (z.B.
 Biganzoli 1998: 
\begin_inset Quotes eld
\end_inset

FEED FORWARD NEURAL NETWORKS FOR THE ANALYSIS OF CENSORED SURVIVAL DATA:
 A PARTIAL LOGISTIC REGRESSION APPROACH
\begin_inset Quotes erd
\end_inset

) auf Patienten-Metadaten gelernt.
\end_layout

\begin_deeper
\begin_layout Itemize
zwei References, die ich als Example zitieren könnte, die Eigenschaften
 von Cancer vorhersagen (allerdings nicht auf expressionsdaten) (die references
 sind aus 
\begin_inset Quotes eld
\end_inset

Sharaf Tsokos 2015 neural networks for modeling discrete survival time of
 censored data.pdf
\begin_inset Quotes erd
\end_inset

, die übrigens aus 4 input-variablen die survival time vorhersagen.
 Sie haben allerdings 69000 Patienten (siehe Figure 1).)
\end_layout

\begin_deeper
\begin_layout Itemize
D.-R.
 Chen, R.-F.
 Chang, W.-J.
 Kuo, M.-C.
 Chen, and Y.-L.
 Huang, “Diagnosis of breast tumors with sonographic texture analysis using
 wavelet transform and neural networks,” Ultra- sound in Medicine and Biology,
 vol.
 28, no.
 10, pp.
 1301–1310, 2002.
\end_layout

\begin_layout Itemize
F.
 Ercal, A.
 Chawla, W.
 V.
 Stoecker, H.-C.
 Lee, and R.
 H.
 Moss, “Neural network diagnosis of malignant melanoma from color images,”
 IEEE Transactions on Biomedical Engineering, vol.
 41, no.
 9, pp.
 837–845, 1994.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
we used feed-forward neural networks (aka multi-layer perceptron?) to predict
 cancer recurrence
\end_layout

\begin_layout Itemize
instances of those networks perform well for image classification
\end_layout

\begin_layout Itemize
does pre-training improve prediction accuracy?
\end_layout

\begin_layout Itemize
pre-training using autoencoder or RBM (breast_cancer_04)
\end_layout

\begin_layout Itemize
there does not seem to be a difference in accuracy on the testing set between
 the two tested pre-training methods (breast_cancer_04)
\end_layout

\begin_layout Itemize
we tried different normalization methods (breast_cancer_08): RMA and MAS5
 normalization, COMBAT batch effect correction, ZCA whitening.
\end_layout

\begin_deeper
\begin_layout Itemize
reconstruction error plots seem to depend on the normalization used
\end_layout

\begin_layout Itemize
insert the breast_cancer_11 experiment here (taking the log2 after MAS5
 normalization).
\end_layout

\end_deeper
\begin_layout Itemize
do the different normalization methods influence prediction accuracy on
 the testing set?
\end_layout

\begin_layout Itemize
we also tried networks with two hidden layers
\end_layout

\begin_layout Itemize
the networks with two hidden layers seem to not perform better than those
 with one hidden layer
\end_layout

\begin_layout Itemize
I should still try networks with a larger hidden layer 1 size (larger than
 the number of input neurons), and with larger hidden layer 2 size.
 (This is actually probably a not-so-good idea, since 
\end_layout

\begin_layout Itemize
we used an increasing number of unlabeled data in pre-training.
\end_layout

\begin_deeper
\begin_layout Itemize
does this improve testing set accuracy?
\end_layout

\end_deeper
\begin_layout Itemize
we used SVM and TSVM (transductive SVM, as implemented in SVMlight) to compare
 to the neural network classifier
\end_layout

\begin_deeper
\begin_layout Itemize
neural network classifiers without pre-training (and with pre-training)
 seem to perform better than SVM.
\end_layout

\begin_layout Itemize
Die Ergebnisse vom StatComp-Poster müssen hier rein.
\end_layout

\end_deeper
\begin_layout Itemize
reconstruction error during pre-training converged well for our cases, sometimes
 you have to wait a few iterations though (die 
\begin_inset Quotes eld
\end_inset

Höcker
\begin_inset Quotes erd
\end_inset

 im reconstruction error plot)
\end_layout

\begin_layout Itemize
we used validation set accuracy to select the neural network which is used
 to predict testing data recurrence status
\end_layout

\begin_layout Itemize
I should also try to do regression using the neural network to predict expected
 survival time at time of diagnosis
\end_layout

\begin_layout Itemize
insert the findings from breast_cancer_08.lyx (see label 
\begin_inset Quotes eld
\end_inset

sub:bb_ic-Reconstruction-error-plots
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Itemize
The test set accuracies depend on the normalization procedure of the microarray
 data
\end_layout

\begin_layout Itemize
A low reconstruction error rate does not imply a good accuracy on the test
 set: Although the neural nets using MAS5 normalized data have a lower reconstru
ction error than their RMA counterparts, the former have a lower accuracy
 than the latter.
\end_layout

\begin_layout Itemize
Das was ich in ~/uni/nnet/datasets/breast_cancer_10/README.lyx geschrieben
 habe.
\end_layout

\begin_layout Itemize
The accuracy of the classifier in the GSE25055-paper is 65% on the testing
 data.
 I should compare that to the accuracies that my classifiers achieve.
\end_layout

\begin_deeper
\begin_layout Itemize
also calculate and tabulate sensitivity, selectivity, Diagnostic likelihood
 ratios
\end_layout

\end_deeper
\begin_layout Itemize
When looking at the accuracies of the TSVM on the data sets with more and
 more unlabeled data, accuracy does not increase with more unlabeled data.
 (like the nnets).
 It's just that TSVM seems to be (or is it just random noise; TODO: make
 the paired T-Test or Wilcoxon-Rank-Test (see ~/uni/journal.txt) to check
 if the differences between accuracies of TSVM and NNet are just by chance
 or systematic.)
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Biganzoli 1998 neural networks for censored survival data.pdf
\begin_inset Quotes erd
\end_inset

: from that paper on page 3: 
\begin_inset Quotes eld
\end_inset

Feed forward ANNs, are strictly equivalent to non-linear multivariate regression
 methods.
\begin_inset Quotes erd
\end_inset

 (Or maybe reference 15 of that paper showed it earlier.)
\end_layout

\begin_layout Itemize
\begin_inset CommandInset label
LatexCommand label
name "inputs-to-weights-ratio"

\end_inset

Comparing my data sets and neuronal network configurations to the ones in
 handwritten digit classification:
\end_layout

\begin_deeper
\begin_layout Itemize
[A Fast Learning Algorithm for Deep Belief Nets; by Hinton, Osindero, Teh]
 write that they use a 3-hidden-layer network with about 
\begin_inset Formula $1.7*10^{6}$
\end_inset

 weights.
 (
\begin_inset Quotes eld
\end_inset

Section 6 shows the pattern recognition performance of a network with three
 hidden layers and about 1.7 million weights on the MNIST set of handwritten
 digits.
\begin_inset Quotes erd
\end_inset

) I believe they used 44000 training samples (
\begin_inset Quotes eld
\end_inset

The network shown in Figure 1 was trained on 44,000 of the training images
\begin_inset Quotes erd
\end_inset

) with 28*28 pixels each (their Figure 1: 
\begin_inset Quotes eld
\end_inset

28 x 28 pixel image
\begin_inset Quotes erd
\end_inset

).
 So altogether they have about 34.5 million 
\begin_inset Quotes eld
\end_inset

training numbers
\begin_inset Quotes erd
\end_inset

, and 1.7 million weights that have to be determined.
\end_layout

\begin_layout Itemize
In contrast to that I have (in data set breast_cancer_12, data set 6) 238
 training samples, each of which has 500 expression levels, i.e.
 238*500=119000 
\begin_inset Quotes eld
\end_inset

training numbers
\begin_inset Quotes erd
\end_inset

.
 The networks whose performances I compared for the statcomp poster are
 breast_cancer_12_aa - dv.
 They all (see 
\begin_inset Quotes eld
\end_inset

~/uni/nnet/configs/make-config-breast_cancer_12-01
\begin_inset Quotes erd
\end_inset

) have an architecture of 500 input nodes, 1000 hidden layer 1 nodes, 1000
 hidden layer 2 nodes, 2000 hidden layer 3 nodes, and 1 output layer node.
 This means there are 
\begin_inset Formula $500*1000+1000*1000+1000*2000+2000*1\approx3.5*10^{6}$
\end_inset

 weights to be learnt.
 How could this work? I'm surprised that I didn't notice that before.
\end_layout

\begin_layout Itemize
I could reduce the number of input (and subsequently hidden) nodes:
\begin_inset Newline newline
\end_inset

Let 
\begin_inset Formula $i$
\end_inset

 be the number of input nodes and 
\begin_inset Formula $h$
\end_inset

 be the number of hidden nodes.
 Assume that 
\begin_inset Formula $h=i/2$
\end_inset

, and 
\begin_inset Formula $o=1$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula 
\begin{eqnarray*}
\frac{i*238+i*h+h*h+h*o}{i*238} & = & =\frac{i*238+i*i/2+i/2*i/2+i/2*1}{i*238}\\
 & = & \frac{i*(238+i/2+i/4+1/2)}{i*238}\\
 & = & \frac{238+i/2+i/4+1/2}{238}
\end{eqnarray*}

\end_inset


\begin_inset Newline newline
\end_inset

This is linear in 
\begin_inset Formula $i$
\end_inset

, so it doesn't help.
\end_layout

\end_deeper
\begin_layout Itemize
But I could have all 22283 genes as inputs, and only a very small 
\begin_inset Formula $h$
\end_inset

.
 If I take 
\begin_inset Formula $i=22283$
\end_inset

, and there are 238 training samples (in fact I have only 20 labeled+100
 unlabeled training samples in data set breast_cancer_12), then I have 
\begin_inset Formula $22283*238=5303354$
\end_inset

 measured numbers.
 In 
\begin_inset Quotes eld
\end_inset

A Fast Learning Algorithm for Deep Belief Nets
\begin_inset Quotes erd
\end_inset

, they have 
\begin_inset Formula $\frac{44000*28*28}{28*28*500+500*500+500*2000+2000*10}=20.8$
\end_inset

 measured numbers per weight.
 Let's say I also wanted such a ratio.
 Then
\begin_inset Newline newline
\end_inset


\begin_inset Formula $r(h)=\frac{22283*238}{22283*h+h*h+h*h+h*o}$
\end_inset

; 
\begin_inset Formula $r(12)\approx19.8$
\end_inset


\begin_inset Newline newline
\end_inset

So I could use 
\begin_inset Formula $h=12$
\end_inset

.
\end_layout

\begin_layout Itemize
See 
\begin_inset Quotes eld
\end_inset

~/uni/nnet/datasets/breast_cancer_15/README.lyx
\begin_inset Quotes erd
\end_inset

 for the 
\begin_inset Formula $h$
\end_inset

 that I chose.
\end_layout

\end_deeper
\begin_layout Itemize
"~/uni/publication/zusammenfassung/applications/Bioinformatics-2016-Chen-bioinfo
rmatics_btw074.pdf" lernen auf der hidden layer representations of the training
 data a linear model to predict the expression value of the genes.
 (Siehe ihre Seite 7 links unten 
\begin_inset Quotes eld
\end_inset

To dissect the nonlinear contribution, we took a relatively simple approach
 by focusing on the representation (activations) from the last hidden layer.
 Each of the hidden unit in that layer can be viewed as a feature generated
 through some nonlinear transformation of the landmark genes.
 We then studied whether a linear regression based on these nonlinear features
 can achieve better performance than a linear regression based solely on
 the landmark genes.
\begin_inset Quotes erd
\end_inset

).
 Das ist so ähnlich wie mein Versuch, auf der hidden layer representation
 eine SVM zu lernen.
\end_layout

\begin_layout Itemize
(vielleicht nicht in die results, aber in die conclusion.
 oder vielleicht noch besser in der Überleitung von results in die discussion)
 In most applications of deep learning there is some algorithm involved
 to multiply the number of available training data sets.
 For example, in image classification the training images are usually translated
 by pixel or subpixel shifts, or by small non-linear deformations using
 a warped mesh.
 This has the effect that a local feature of an input training image (for
 example, a red pixel on green background) is present in different input
 pixels in the transformed training images.
 This allows producing a large number of similar training images from an
 (arguably alreadly relatively large) input training set.
 The neuronal network is thereby forced to learn the property of a feature
 regardless of its position in the image.
\begin_inset Newline newline
\end_inset

Having such a transformation for expression data would be very useful, not
 only for classification using neuronal networks, but also other machine
 learning algorithms.
 However, it is not at all clear what a pre-processing equivalent to the
 local image deformations could look like for mRNA abundance.
 Straightforward application of the principle would provide the neuronal
 network with input for a gene in the dimension of a maybe completely unrelated
 gene.
 (Note that adding some sort of noise onto the expression levels would be
 equivalent to adding noise to the image, which is not equivalent to shifting
 the image.) A possible approach could be to look for gene modules that consist
 of redundant genes, and permute their expression values among the redundacy
 group.
 This would require knowledge about gene modules in advance .
\end_layout

\end_body
\end_document

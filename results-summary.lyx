#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO: Maybe this list is not summary of results, but should be considered
 as items for the discussion part of the dissertation.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Das was ich über survival time prediction in ~/uni/nnet/datasets/breast_cancer_1
0/README.lyx geschrieben habe.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Das was meine Arbeit von anderen abhebt (und was meines Wissens nach neu
 ist), ist, dass ich direkt auf Expressionsdaten neuronale Netze trainiere,
 um eine Eigenschaft des Patienten vorherzusagen.
 Zuvor haben andere (z.B.
 Biganzoli 1998: 
\begin_inset Quotes eld
\end_inset

FEED FORWARD NEURAL NETWORKS FOR THE ANALYSIS OF CENSORED SURVIVAL DATA:
 A PARTIAL LOGISTIC REGRESSION APPROACH
\begin_inset Quotes erd
\end_inset

) auf Patienten-Metadaten gelernt.
\end_layout

\begin_deeper
\begin_layout Itemize
zwei References, die ich als Example zitieren könnte, die Eigenschaften
 von Cancer vorhersagen (allerdings nicht auf expressionsdaten) (die references
 sind aus 
\begin_inset Quotes eld
\end_inset

Sharaf Tsokos 2015 neural networks for modeling discrete survival time of
 censored data.pdf
\begin_inset Quotes erd
\end_inset

, die übrigens aus 4 input-variablen die survival time vorhersagen.
 Sie haben allerdings 69,000 Patienten (siehe Figure 1).)
\end_layout

\begin_deeper
\begin_layout Itemize
D.-R.
 Chen, R.-F.
 Chang, W.-J.
 Kuo, M.-C.
 Chen, and Y.-L.
 Huang, “Diagnosis of breast tumors with sonographic texture analysis using
 wavelet transform and neural networks,” Ultra- sound in Medicine and Biology,
 vol.
 28, no.
 10, pp.
 1301–1310, 2002.
\end_layout

\begin_layout Itemize
F.
 Ercal, A.
 Chawla, W.
 V.
 Stoecker, H.-C.
 Lee, and R.
 H.
 Moss, “Neural network diagnosis of malignant melanoma from color images,”
 IEEE Transactions on Biomedical Engineering, vol.
 41, no.
 9, pp.
 837–845, 1994.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
we used feed-forward neural networks (aka multi-layer perceptron?) to predict
 cancer recurrence
\end_layout

\begin_deeper
\begin_layout Itemize
instances of those networks perform well for image classification
\end_layout

\end_deeper
\begin_layout Itemize
does pre-training improve prediction accuracy?
\end_layout

\begin_deeper
\begin_layout Itemize
pre-training using autoencoder or RBM (breast_cancer_04)
\end_layout

\begin_layout Itemize
there does not seem to be a difference in accuracy on the testing set between
 the two tested pre-training methods (breast_cancer_04)
\end_layout

\begin_layout Itemize
reconstruction error during pre-training converged well for our cases, sometimes
 you have to wait a few iterations though (die 
\begin_inset Quotes eld
\end_inset

Höcker
\begin_inset Quotes erd
\end_inset

 im reconstruction error plot)
\end_layout

\begin_layout Itemize
we used an increasing number of unlabeled data in pre-training.
\end_layout

\begin_deeper
\begin_layout Itemize
does this improve testing set accuracy?
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
we tried different normalization methods (breast_cancer_08): RMA and MAS5
 normalization, COMBAT batch effect correction, ZCA whitening.
\end_layout

\begin_deeper
\begin_layout Itemize
reconstruction error plots seem to depend on the normalization used
\end_layout

\begin_deeper
\begin_layout Itemize
MAS5 normalized data seem to have lower reconstruction error than RMA normalized
 data.
\end_layout

\begin_layout Itemize
Additional COMBAT batch effect correction does not seem to alter the reconstruct
ion error of RMA or MAS5 normalized data, except for MAS5 normalization
 without ZCA whitening, where additional COMBAT does seem to lead to an
 increased reconstruction error
\end_layout

\end_deeper
\begin_layout Itemize
insert the breast_cancer_11 experiment here (taking the log2 after MAS5
 normalization).
\end_layout

\begin_layout Itemize
the different normalization methods influence test set prediction accuracy?
\end_layout

\begin_deeper
\begin_layout Itemize
Of all normalizations tested, RMA with no additional pre-processing yields
 the best accuracies in all data sets tested.
\end_layout

\begin_layout Itemize
A low reconstruction error rate does not imply a good accuracy on the test
 set: Although the neural nets using MAS5 normalized data have a lower reconstru
ction error than their RMA counterparts, the former have a lower accuracy
 than the latter.
\end_layout

\begin_layout Itemize
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
From notes/breast_cancer_11/breast_cancer_11.lyx
\end_layout

\end_inset

when doing subsequent Combat pre-processing, taking the log2 beforehand
 helps improve accuracy.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
we also tried networks with two and three hidden layers
\end_layout

\begin_deeper
\begin_layout Itemize
the networks with two hidden layers seem to not perform better than those
 with one hidden layer
\end_layout

\end_deeper
\begin_layout Itemize
we used SVM and TSVM (transductive SVM, as implemented in SVMlight) to compare
 to the neural network classifier
\end_layout

\begin_deeper
\begin_layout Itemize
neural network classifiers without pre-training (and with pre-training)
 seem to perform better than SVM.
\end_layout

\begin_layout Itemize
Die Ergebnisse vom StatComp-Poster müssen hier rein.
\end_layout

\end_deeper
\begin_layout Itemize
we used validation set accuracy to select the neural network which is used
 to predict testing data recurrence status
\end_layout

\begin_deeper
\begin_layout Itemize
we smoothed the training, validation, and test set accuracies
\end_layout

\end_deeper
\begin_layout Itemize
The accuracy of the classifier in the GSE25055-paper is 65% on the testing
 data.
 I should compare that to the accuracies that my classifiers achieve.
 (Of note should be however, that they did not optimize for accuracy, but
 for another measure.)
\end_layout

\begin_deeper
\begin_layout Itemize
also calculate and tabulate sensitivity, selectivity, Diagnostic likelihood
 ratios
\end_layout

\end_deeper
\begin_layout Itemize
When looking at the accuracies of the TSVM on the data sets with more and
 more unlabeled data, accuracy does not increase with more unlabeled data.
 (like the nnets).
 It's just that TSVM seems to be better than nnets.
 (Or is it just random noise; TODO: make the paired T-Test or Wilcoxon-Rank-Test
 (see ~/uni/journal.txt) to check if the differences between accuracies of
 TSVM and NNet are just by chance or systematic.)
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Biganzoli 1998 neural networks for censored survival data.pdf
\begin_inset Quotes erd
\end_inset

: from that paper on page 3: 
\begin_inset Quotes eld
\end_inset

Feed forward ANNs, are strictly equivalent to non-linear multivariate regression
 methods.
\begin_inset Quotes erd
\end_inset

 (Or maybe reference 15 of that paper showed it earlier.)
\end_layout

\begin_layout Itemize
\begin_inset CommandInset label
LatexCommand label
name "inputs-to-weights-ratio"

\end_inset

We tried different training parameters, different network architectures
 (with many free parameters relative to training data set size, and with
 few free parameters), different normalizations, different data set compositions.
 None gave a clear result that using additional unlabeled data during training
 yields higher accuracies on the testing data set.
\end_layout

\begin_layout Itemize
"~/uni/publication/zusammenfassung/applications/Bioinformatics-2016-Chen-bioinfo
rmatics_btw074.pdf" lernen auf der hidden layer representations of the training
 data a linear model to predict the expression value of the genes.
 (Siehe ihre Seite 7 links unten 
\begin_inset Quotes eld
\end_inset

To dissect the nonlinear contribution, we took a relatively simple approach
 by focusing on the representation (activations) from the last hidden layer.
 Each of the hidden unit in that layer can be viewed as a feature generated
 through some nonlinear transformation of the landmark genes.
 We then studied whether a linear regression based on these nonlinear features
 can achieve better performance than a linear regression based solely on
 the landmark genes.
\begin_inset Quotes erd
\end_inset

).
 Das ist so ähnlich wie mein Versuch, auf der hidden layer representation
 eine SVM zu lernen.
\end_layout

\begin_deeper
\begin_layout Itemize
they come to the conclusion that apparently the neuronal network has captured
 some non-linear dependencies within the raw data that would not be captured
 by linear regression.
 However, they do not believe that 
\begin_inset Quotes eld
\end_inset

adjusted 
\begin_inset Formula $R^{2}$
\end_inset


\begin_inset Quotes erd
\end_inset

 characterizes the capturing of the non-linearity well.
\end_layout

\end_deeper
\begin_layout Itemize
(vielleicht nicht in die results, aber in die conclusion.
 oder vielleicht noch besser in der Überleitung von results in die discussion)
 In most applications of deep learning there is some algorithm involved
 to multiply the number of available training data sets.
 For example, in image classification the training images are usually translated
 by pixel or subpixel shifts, or small non-linear deformations are applied
 using a warped mesh.
 This has the effect that a local feature of an input training image (for
 example, a red pixel on green background) is present in different input
 pixels in the transformed training images.
 This allows producing a large number of similar training images from an
 (arguably alreadly relatively large) input training set.
 The neuronal network is thereby forced to learn the property of a feature
 regardless of its position in the image.
\begin_inset Newline newline
\end_inset

Having such a transformation for expression data would be very useful, not
 only for classification using neuronal networks, but also other machine
 learning algorithms.
 However, it is not at all clear what a pre-processing equivalent to the
 local image deformations could look like for mRNA abundance.
 Straightforward application of the image deformation scheme would provide
 the neuronal network with input for a gene in the dimension of a maybe
 completely unrelated gene.
 (Note that adding some sort of noise onto the expression levels would be
 equivalent to adding noise to the image, which is not equivalent to shifting
 the image.) A possible approach could be to look for gene modules that consist
 of redundant genes, and permute their expression values among the redundacy
 group.
 This would require knowledge about gene modules in advance.
 Maybe I could increase the number of input samples by creating additional
 samples by composing them of random subsets of other input samples.
 E.g.
 take gene 1-1000 from sample 1, gene 1001-2001 from samples 2, and so on.
 Or maybe even better use gene modules as learned by an RBM (see the _hub_featur
es_ in Figure S3 of "~/zusammenfassung/applications/Bioinformatics-2016-Chen-bio
informatics_btw074-supplementary.pdf").
 The algorithm would work like this: Determine for each _measured_ input
 sample the gene hub activation (by training an RBM or unsupervised DBN
 on all input samples).
 This results in a vector of numbers, one vector for each input sample;
 each number stands for one gene hub activation.
 Permute the gene hub activations between the learned representations, but
 only use representations from samples that have the same class label.
 Due to the number of permutations this creates a large number of labeled
 training samples (each training sample is labeled like the measured samples
 used in the training sample generation).
 Use these generated training samples as input for a supervised DBN.
\end_layout

\begin_layout Itemize
Eine probability zu erhalten wie im Paper von Inka (
\begin_inset Quotes eld
\end_inset

Estimating classification probabilities in high-dimensional diagnostic studies,
 Bioinformatics, 27(18):2563-2570.
\begin_inset Quotes erd
\end_inset

), ist in neuronal networks sehr einfach, da der Classifier selbst die Konfidenz
 seiner Vorhersage als Zahl zwischen 0 und 1 ausgibt.
\end_layout

\begin_layout Itemize
in the Discussion, review the observation of ./Bioinformatics-2016-Chen-bioinform
atics_btw074-supplementary.pdf, Figure S3, that there seem to be hub genes,
 that have outgoing connections to many output layer genes, with many of
 the hub genes having either positive or negative outgoing weights, but
 not both.
\end_layout

\begin_deeper
\begin_layout Itemize
Our work differs from the Chen paper in that they use purely unsupervised
 learning, and we try semi-supervised learning.
\end_layout

\end_deeper
\begin_layout Itemize
In terms of 
\begin_inset Quotes eld
\end_inset

Semi-supervised learning survey
\begin_inset Quotes erd
\end_inset

 by Xiaojin Zhu, I'm learning an efficient coding of the domain from unlabeled
 data (see their chapter 8).
 They also notice that one has to constrain the class proportions (in my
 case 50% for class 0,50% for class 1).
\end_layout

\end_body
\end_document
